# simple_face_crop_webcam.py (ê°„ëµí•œ UI + ê°œì„ ëœ ì–¼êµ´ í¬ë¡­)
import cv2
import torch
import torch.nn as nn
from torchvision import transforms, models
import numpy as np
from PIL import Image
import time
import os
import math
from collections import deque

# MediaPipe ì„¤ì¹˜ í™•ì¸
try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
    print("âœ… MediaPipe ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    MEDIAPIPE_AVAILABLE = False
    print("âš ï¸ MediaPipe ë¯¸ì„¤ì¹˜")

# ------------------ ëª¨ë¸ í´ë˜ìŠ¤ë“¤ ------------------
class CNNEncoderV2(nn.Module):
    def __init__(self, output_dim=1280):
        super().__init__()
        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)
        self.features = mobilenet.features
        self.avgpool = nn.AdaptiveAvgPool2d((4, 4))
        self.fc = nn.Sequential(
            nn.Linear(1280 * 4 * 4, 2048),
            nn.BatchNorm1d(2048),
            nn.ReLU(inplace=True),
            nn.Dropout(0.4),
            nn.Linear(2048, output_dim),
            nn.BatchNorm1d(output_dim),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        B, T, C, H, W = x.shape
        x = x.view(B * T, C, H, W)
        x = self.features(x)
        x = self.avgpool(x)
        x = x.view(B * T, -1)
        x = self.fc(x)
        return x.view(B, T, -1)

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        seq_len = x.size(0)
        return x + self.pe[:seq_len, :].to(x.device)

class EngagementModelV2(nn.Module):
    def __init__(self, cnn_feat_dim=1280, fusion_feat_dim=5, d_model=256, nhead=8, num_layers=4):
        super().__init__()
        
        self.input_projection = nn.Sequential(
            nn.Linear(cnn_feat_dim, d_model),
            nn.LayerNorm(d_model),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1)
        )
        
        self.pos_encoder = PositionalEncoding(d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=0.15,
            activation='gelu',
            batch_first=True,
            norm_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)
        self.global_max_pool = nn.AdaptiveMaxPool1d(1)
        
        self.fc = nn.Sequential(
            nn.Linear(d_model * 2 + fusion_feat_dim, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.LayerNorm(128),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(128, 1)
        )

    def forward(self, cnn_feats, fusion_feats):
        x = self.input_projection(cnn_feats)
        x = x.transpose(0, 1)
        x = self.pos_encoder(x)
        x = x.transpose(0, 1)
        transformer_out = self.transformer_encoder(x)
        
        avg_pooled = self.global_avg_pool(transformer_out.transpose(1, 2)).squeeze(-1)
        max_pooled = self.global_max_pool(transformer_out.transpose(1, 2)).squeeze(-1)
        pooled = torch.cat([avg_pooled, max_pooled], dim=1)
        
        combined = torch.cat([pooled, fusion_feats], dim=1)
        return self.fc(combined)

# ------------------ ê°œì„ ëœ ì–¼êµ´ í¬ë¡­ í´ë˜ìŠ¤ ------------------
class ImprovedFaceCropper:
    """ê°œì„ ëœ MediaPipe ì–¼êµ´ í¬ë¡­ (ê±°ë¦¬ ë¬¸ì œ í•´ê²°)"""
    def __init__(self):
        if MEDIAPIPE_AVAILABLE:
            self.mp_face_detection = mp.solutions.face_detection
            # model_selection=1ë¡œ ì„¤ì • (ë” ë„“ì€ ë²”ìœ„, 5mê¹Œì§€)
            self.face_detection = self.mp_face_detection.FaceDetection(
                model_selection=1,  # âœ… 0 â†’ 1ë¡œ ë³€ê²½ (ë” ë„“ì€ íƒì§€ ë²”ìœ„)
                min_detection_confidence=0.3  # âœ… 0.5 â†’ 0.3ìœ¼ë¡œ ë‚®ì¶¤ (ë” ë¯¼ê°í•˜ê²Œ)
            )
            print("âœ… ê°œì„ ëœ MediaPipe FaceDetection ì´ˆê¸°í™”")
        else:
            self.face_detection = None
            print("âš ï¸ MediaPipe ì—†ì´ ì¤‘ì•™ í¬ë¡­ ì‚¬ìš©")
    
    def crop_face(self, frame, padding_ratio=0.5):  # âœ… íŒ¨ë”© 0.3 â†’ 0.5ë¡œ ì¦ê°€
        """ê°œì„ ëœ ì–¼êµ´ í¬ë¡­ (ê±°ë¦¬ ë¬¸ì œ í•´ê²°)"""
        if not MEDIAPIPE_AVAILABLE or self.face_detection is None:
            return self._adaptive_center_crop(frame), False
        
        h, w, _ = frame.shape
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # ì–¼êµ´ íƒì§€
        results = self.face_detection.process(rgb_frame)
        
        if results.detections:
            # ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ ì–¼êµ´ ì„ íƒ
            detection = max(results.detections, 
                          key=lambda x: x.score)  # âœ… size ëŒ€ì‹  confidence ê¸°ì¤€
            
            bbox = detection.location_data.relative_bounding_box
            
            # ì ˆëŒ€ ì¢Œí‘œ ë³€í™˜
            x = int(bbox.xmin * w)
            y = int(bbox.ymin * h)
            face_w = int(bbox.width * w)
            face_h = int(bbox.height * h)
            
            # âœ… ë” í° íŒ¨ë”© ì ìš© (ì–´ê¹¨ê¹Œì§€ í¬í•¨)
            padding_w = int(face_w * padding_ratio)
            padding_h = int(face_h * padding_ratio)
            
            # í¬ë¡­ ì˜ì—­ ê³„ì‚°
            x1 = max(0, x - padding_w)
            y1 = max(0, y - padding_h)
            x2 = min(w, x + face_w + padding_w)
            y2 = min(h, y + face_h + padding_h)
            
            # âœ… ì •ì‚¬ê°í˜• ë§Œë“¤ê¸° (ë” ê´€ëŒ€í•˜ê²Œ)
            crop_w = x2 - x1
            crop_h = y2 - y1
            target_size = max(crop_w, crop_h)  # ë” í° ìª½ì— ë§ì¶¤
            
            # ì¤‘ì•™ ì •ë ¬
            center_x = (x1 + x2) // 2
            center_y = (y1 + y2) // 2
            
            half_size = target_size // 2
            x1 = max(0, center_x - half_size)
            y1 = max(0, center_y - half_size)
            x2 = min(w, center_x + half_size)
            y2 = min(h, center_y + half_size)
            
            # ê²½ê³„ ì¡°ì •
            if x2 - x1 < target_size:
                if x1 == 0:
                    x2 = min(w, x1 + target_size)
                else:
                    x1 = max(0, x2 - target_size)
            
            if y2 - y1 < target_size:
                if y1 == 0:
                    y2 = min(h, y1 + target_size)
                else:
                    y1 = max(0, y2 - target_size)
            
            cropped_face = frame[y1:y2, x1:x2]
            
            if cropped_face.shape[0] > 0 and cropped_face.shape[1] > 0:
                cropped_face = cv2.resize(cropped_face, (224, 224))
                return cropped_face, True
        
        # ì–¼êµ´ íƒì§€ ì‹¤íŒ¨ ì‹œ ì ì‘í˜• ì¤‘ì•™ í¬ë¡­
        return self._adaptive_center_crop(frame), False
    
    def _adaptive_center_crop(self, frame):
        """ì ì‘í˜• ì¤‘ì•™ í¬ë¡­ (ìƒì²´ í¬í•¨)"""
        h, w, _ = frame.shape
        
        # âœ… ë” í° í¬ë¡­ ë¹„ìœ¨ (ìƒì²´ í¬í•¨)
        crop_ratio = 0.8  # í™”ë©´ì˜ 80% ì‚¬ìš©
        crop_size = int(min(h, w) * crop_ratio)
        
        # ì¤‘ì•™ì—ì„œ ì•½ê°„ ìœ„ìª½ìœ¼ë¡œ ì´ë™ (ë¨¸ë¦¬ê°€ ì¤‘ì•™ì— ì˜¤ë„ë¡)
        center_x = w // 2
        center_y = int(h * 0.4)  # âœ… ì¤‘ì•™ë³´ë‹¤ ìœ„ìª½
        
        half_size = crop_size // 2
        x1 = max(0, center_x - half_size)
        y1 = max(0, center_y - half_size)
        x2 = min(w, center_x + half_size)
        y2 = min(h, center_y + half_size)
        
        cropped = frame[y1:y2, x1:x2]
        return cv2.resize(cropped, (224, 224))

# ------------------ ê°„ëµí•œ UI ëª¨ë‹ˆí„° ------------------
class SimpleFocusMonitor:
    def __init__(self, model_path, device='cuda', threshold=0.7):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.threshold = threshold
        self.frame_buffer = deque(maxlen=30)
        
        # ì–¼êµ´ í¬ë¡­ ì´ˆê¸°í™”
        self.face_cropper = ImprovedFaceCropper()
        
        # ì „ì²˜ë¦¬
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
        
        # ëª¨ë¸ ë¡œë“œ
        self.load_model(model_path)
        
        # í†µê³„ (ê°„ë‹¨í•˜ê²Œ)
        self.recent_predictions = deque(maxlen=5)  # 5ê°œë§Œ ì‚¬ìš©
        
        print(f"ğŸš€ ê°„ëµí•œ ì§‘ì¤‘ë„ ëª¨ë‹ˆí„° ì´ˆê¸°í™” ì™„ë£Œ!")
        print(f"   - ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"   - ì„ê³„ê°’: {self.threshold}")
    
    def load_model(self, model_path):
        """Version 2 ëª¨ë¸ ë¡œë“œ"""
        print(f"ğŸ“‚ ëª¨ë¸ ë¡œë”© ì¤‘: {model_path}")
        
        self.cnn = CNNEncoderV2().to(self.device)
        self.model = EngagementModelV2(d_model=256, nhead=8, num_layers=4).to(self.device)
        
        checkpoint = torch.load(model_path, map_location=self.device)
        self.cnn.load_state_dict(checkpoint['cnn_state_dict'])
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        self.cnn.eval()
        self.model.eval()
        for param in self.cnn.parameters():
            param.requires_grad = False
        for param in self.model.parameters():
            param.requires_grad = False
            
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def preprocess_frame(self, frame):
        """ì–¼êµ´ í¬ë¡­ + ì „ì²˜ë¦¬"""
        # ê°œì„ ëœ ì–¼êµ´ í¬ë¡­
        cropped_face, face_detected = self.face_cropper.crop_face(frame)
        
        # BGR to RGB
        face_rgb = cv2.cvtColor(cropped_face, cv2.COLOR_BGR2RGB)
        
        # í…ì„œ ë³€í™˜
        tensor = self.transform(face_rgb)
        
        return tensor, face_detected
    
    def predict_engagement(self):
        """ì§‘ì¤‘ë„ ì˜ˆì¸¡"""
        if len(self.frame_buffer) < 30:
            return None, None
        
        # 30í”„ë ˆì„ ìŠ¤íƒ
        frames_data = list(self.frame_buffer)
        frames = torch.stack([data[0] for data in frames_data])
        frames = frames.unsqueeze(0).to(self.device)
        
        # ë”ë¯¸ fusion features
        fusion = torch.zeros(1, 5).to(self.device)
        
        # ì˜ˆì¸¡
        with torch.no_grad():
            cnn_features = self.cnn(frames)
            logits = self.model(cnn_features, fusion)
            probability = torch.sigmoid(logits).item()
            prediction = 1 if probability > self.threshold else 0
        
        return prediction, probability
    
    def draw_simple_overlay(self, frame, prediction, probability):
        """ê°„ëµí•œ UI ì˜¤ë²„ë ˆì´"""
        height, width = frame.shape[:2]
        
        # ìµœê·¼ ì§‘ì¤‘ë¥  ê³„ì‚°
        focus_rate = 0
        if len(self.recent_predictions) > 0:
            focus_rate = (len(self.recent_predictions) - sum(self.recent_predictions)) / len(self.recent_predictions)
        
        # âœ… ê°„ë‹¨í•œ ë°°ê²½ (ì‘ê²Œ)
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, 10), (300, 100), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        # âœ… ì§‘ì¤‘ ìƒíƒœ (í° ê¸€ì”¨)
        if prediction == 0:
            color = (0, 255, 0)
            status = "ì§‘ì¤‘í•¨"
        else:
            color = (0, 0, 255)
            status = "ì§‘ì¤‘ì•ˆí•¨"
        
        # âœ… ë©”ì¸ ì •ë³´ë§Œ í‘œì‹œ
        cv2.putText(frame, status, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3)
        cv2.putText(frame, f"{probability*100:.0f}%", (20, 75), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        
        # âœ… ì‘ì€ ì§‘ì¤‘ë„ ë°” (ìš°ì¸¡)
        bar_x = width - 40
        bar_y = 20
        bar_height = 120
        bar_width = 20
        
        # ë°” ë°°ê²½
        cv2.rectangle(frame, (bar_x, bar_y), (bar_x + bar_width, bar_y + bar_height), (50, 50, 50), -1)
        
        # ì§‘ì¤‘ë„ í‘œì‹œ
        if prediction == 0:  # ì§‘ì¤‘í•¨
            fill_color = (0, 255, 0)
            fill_height = int(bar_height * (1 - probability))  # í™•ë¥ ì´ ë‚®ì„ìˆ˜ë¡ ì§‘ì¤‘í•¨
        else:  # ì§‘ì¤‘ì•ˆí•¨
            fill_color = (0, 0, 255)
            fill_height = int(bar_height * probability)  # í™•ë¥ ì´ ë†’ì„ìˆ˜ë¡ ì§‘ì¤‘ì•ˆí•¨
        
        cv2.rectangle(frame, (bar_x, bar_y + bar_height - fill_height), 
                     (bar_x + bar_width, bar_y + bar_height), fill_color, -1)
        
        # ë°” í…Œë‘ë¦¬
        cv2.rectangle(frame, (bar_x, bar_y), (bar_x + bar_width, bar_y + bar_height), (255, 255, 255), 2)
        
        return frame
    
    def run(self, camera_id=0):
        """ê°„ëµí•œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§"""
        cap = cv2.VideoCapture(camera_id)
        
        if not cap.isOpened():
            print("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        cap.set(cv2.CAP_PROP_FPS, 30)
        
        print("ğŸ¥ ê°„ëµí•œ ì§‘ì¤‘ë„ ëª¨ë‹ˆí„°ë§ ì‹œì‘!")
        print("   - 'q' í‚¤ë¡œ ì¢…ë£Œ")
        print("   - 'r' í‚¤ë¡œ ë¦¬ì…‹")
        
        frame_count = 0
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                
                # ì–¼êµ´ í¬ë¡­ ë° ì „ì²˜ë¦¬
                processed_frame, face_detected = self.preprocess_frame(frame)
                self.frame_buffer.append((processed_frame, face_detected))
                
                # 3í”„ë ˆì„ë§ˆë‹¤ ì˜ˆì¸¡
                prediction, probability = None, None
                if frame_count % 3 == 0 and len(self.frame_buffer) == 30:
                    prediction, probability = self.predict_engagement()
                    
                    if prediction is not None:
                        self.recent_predictions.append(prediction)
                        status = "ì§‘ì¤‘í•¨" if prediction == 0 else "ì§‘ì¤‘ì•ˆí•¨"
                        print(f"Frame {frame_count}: {status} ({probability*100:.0f}%)")
                
                # ì´ì „ ê²°ê³¼ ìœ ì§€
                if hasattr(self, '_last_result'):
                    display_pred, display_prob = self._last_result
                else:
                    display_pred, display_prob = prediction, probability
                
                if prediction is not None:
                    self._last_result = (prediction, probability)
                    display_pred, display_prob = prediction, probability
                
                # âœ… ê°„ëµí•œ ì˜¤ë²„ë ˆì´ ê·¸ë¦¬ê¸°
                if display_pred is not None:
                    frame = self.draw_simple_overlay(frame, display_pred, display_prob)
                
                cv2.imshow('Focus Monitor', frame)
                
                # í‚¤ ì…ë ¥
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('r'):
                    self.recent_predictions.clear()
                    print("ğŸ“Š ë¦¬ì…‹ ì™„ë£Œ")
                
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ì¤‘ë‹¨ë¨")
        
        finally:
            cap.release()
            cv2.destroyAllWindows()

# ------------------ ë©”ì¸ ì‹¤í–‰ ------------------
def main():
    print("ğŸš€ ê°„ëµí•œ ì–¼êµ´ í¬ë¡­ ì§‘ì¤‘ë„ í…ŒìŠ¤íŠ¸")
    print("="*40)
    
    model_paths = [
        "./log/v2/best_model_v2.pt",
        "./log/best_model2.pt",
    ]
    
    model_path = None
    for path in model_paths:
        if os.path.exists(path):
            model_path = path
            break
    
    if model_path is None:
        print("âŒ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“‚ ëª¨ë¸: {model_path}")
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    threshold = 0.7
    
    try:
        monitor = SimpleFocusMonitor(
            model_path=model_path,
            device=device,
            threshold=threshold
        )
        
        monitor.run(camera_id=0)
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
