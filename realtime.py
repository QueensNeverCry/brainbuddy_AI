# real_time_webcam_test.py (ì‹¤ì‹œê°„ ì›¹ìº  ì§‘ì¤‘ë„ ëª¨ë‹ˆí„°ë§)
import cv2
import torch
import torch.nn as nn
from torchvision import transforms, models
import numpy as np
from PIL import Image
import time
import os
import math
from collections import deque
import threading
import queue

# ------------------ ëª¨ë¸ í´ë˜ìŠ¤ë“¤ (ê¸°ì¡´ê³¼ ë™ì¼) ------------------
class CNNEncoderV1(nn.Module):
    def __init__(self, output_dim=1280):
        super().__init__()
        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)
        self.features = mobilenet.features
        self.avgpool = nn.AdaptiveAvgPool2d((4, 4))
        self.fc = nn.Sequential(
            nn.Linear(1280 * 4 * 4, 2048),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(2048, output_dim),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        B, T, C, H, W = x.shape
        x = x.view(B * T, C, H, W)
        x = self.features(x)
        x = self.avgpool(x)
        x = x.view(B * T, -1)
        x = self.fc(x)
        return x.view(B, T, -1)

class CNNEncoderV2(nn.Module):
    def __init__(self, output_dim=1280):
        super().__init__()
        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)
        self.features = mobilenet.features
        self.avgpool = nn.AdaptiveAvgPool2d((4, 4))
        self.fc = nn.Sequential(
            nn.Linear(1280 * 4 * 4, 2048),
            nn.BatchNorm1d(2048),
            nn.ReLU(inplace=True),
            nn.Dropout(0.4),
            nn.Linear(2048, output_dim),
            nn.BatchNorm1d(output_dim),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        B, T, C, H, W = x.shape
        x = x.view(B * T, C, H, W)
        x = self.features(x)
        x = self.avgpool(x)
        x = x.view(B * T, -1)
        x = self.fc(x)
        return x.view(B, T, -1)

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        seq_len = x.size(0)
        return x + self.pe[:seq_len, :].to(x.device)

class EngagementModelV1(nn.Module):
    def __init__(self, cnn_feat_dim=1280, fusion_feat_dim=5, d_model=128, nhead=8, num_layers=3):
        super().__init__()
        
        self.input_projection = nn.Linear(cnn_feat_dim, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=0.1,
            activation='relu',
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        
        self.fc = nn.Sequential(
            nn.Linear(d_model + fusion_feat_dim, 64),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(64, 1)
        )

    def forward(self, cnn_feats, fusion_feats):
        x = self.input_projection(cnn_feats)
        x = x.transpose(0, 1)
        x = self.pos_encoder(x)
        x = x.transpose(0, 1)
        transformer_out = self.transformer_encoder(x)
        pooled = self.global_pool(transformer_out.transpose(1, 2)).squeeze(-1)
        combined = torch.cat([pooled, fusion_feats], dim=1)
        return self.fc(combined)

class EngagementModelV2(nn.Module):
    def __init__(self, cnn_feat_dim=1280, fusion_feat_dim=5, d_model=256, nhead=8, num_layers=4):
        super().__init__()
        
        self.input_projection = nn.Sequential(
            nn.Linear(cnn_feat_dim, d_model),
            nn.LayerNorm(d_model),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1)
        )
        
        self.pos_encoder = PositionalEncoding(d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=0.15,
            activation='gelu',
            batch_first=True,
            norm_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)
        self.global_max_pool = nn.AdaptiveMaxPool1d(1)
        
        self.fc = nn.Sequential(
            nn.Linear(d_model * 2 + fusion_feat_dim, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.LayerNorm(128),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(128, 1)
        )

    def forward(self, cnn_feats, fusion_feats):
        x = self.input_projection(cnn_feats)
        x = x.transpose(0, 1)
        x = self.pos_encoder(x)
        x = x.transpose(0, 1)
        transformer_out = self.transformer_encoder(x)
        
        avg_pooled = self.global_avg_pool(transformer_out.transpose(1, 2)).squeeze(-1)
        max_pooled = self.global_max_pool(transformer_out.transpose(1, 2)).squeeze(-1)
        pooled = torch.cat([avg_pooled, max_pooled], dim=1)
        
        combined = torch.cat([pooled, fusion_feats], dim=1)
        return self.fc(combined)

class TransformerEnsembleModel(nn.Module):
    def __init__(self, cnn_v1, model_v1, cnn_v2, model_v2, ensemble_method='learned'):
        super().__init__()
        self.cnn_v1 = cnn_v1
        self.model_v1 = model_v1
        self.cnn_v2 = cnn_v2
        self.model_v2 = model_v2
        self.ensemble_method = ensemble_method
        
        if ensemble_method == 'weighted':
            self.register_buffer('weights', torch.tensor([0.3, 0.7]))
        elif ensemble_method == 'learned':
            self.ensemble_weights = nn.Parameter(torch.tensor([0.3, 0.7]))
            self.ensemble_fc = nn.Sequential(
                nn.Linear(2, 16),
                nn.ReLU(inplace=True),
                nn.Dropout(0.2),
                nn.Linear(16, 1)
            )

    def forward(self, videos, fusion_feats):
        feats_v1 = self.cnn_v1(videos)
        feats_v2 = self.cnn_v2(videos)
        
        logits_v1 = self.model_v1(feats_v1, fusion_feats)
        logits_v2 = self.model_v2(feats_v2, fusion_feats)
        
        if self.ensemble_method == 'weighted':
            return self.weights[0] * logits_v1 + self.weights[1] * logits_v2
        elif self.ensemble_method == 'learned':
            prob_v1 = torch.sigmoid(logits_v1)
            prob_v2 = torch.sigmoid(logits_v2)
            normalized_weights = torch.softmax(self.ensemble_weights, dim=0)
            weighted_v1 = prob_v1 * normalized_weights[0]
            weighted_v2 = prob_v2 * normalized_weights[1]
            combined_input = torch.cat([weighted_v1, weighted_v2], dim=1)
            return self.ensemble_fc(combined_input)

# ------------------ ì‹¤ì‹œê°„ ì›¹ìº  í´ë˜ìŠ¤ ------------------
class RealTimeEngagementMonitor:
    def __init__(self, model_path, device='cuda', threshold=0.7):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.threshold = threshold
        self.frame_buffer = deque(maxlen=30)  # 30í”„ë ˆì„ ë²„í¼
        self.result_queue = queue.Queue(maxsize=10)
        
        # ë³€í™˜ ì„¤ì •
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
        
        # ëª¨ë¸ ë¡œë“œ
        self.load_model(model_path)
        
        # ê²°ê³¼ í†µê³„
        self.recent_predictions = deque(maxlen=10)  # ìµœê·¼ 10ê°œ ì˜ˆì¸¡ í‰ê· 
        self.total_frames = 0
        self.focused_frames = 0
        
        print(f"ğŸš€ ì‹¤ì‹œê°„ ì§‘ì¤‘ë„ ëª¨ë‹ˆí„° ì´ˆê¸°í™” ì™„ë£Œ!")
        print(f"   - ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"   - ì„ê³„ê°’: {self.threshold}")
        print(f"   - ëª¨ë¸ ë¡œë“œ: ì„±ê³µ")
    
    def load_model(self, model_path):
        """ì•™ìƒë¸” ëª¨ë¸ ë¡œë“œ"""
        print(f"ğŸ“‚ ëª¨ë¸ ë¡œë”© ì¤‘: {model_path}")
        
        # ê°œë³„ ëª¨ë¸ë“¤ ì´ˆê¸°í™”
        cnn_v1 = CNNEncoderV1().to(self.device)
        model_v1 = EngagementModelV1(d_model=128, nhead=8, num_layers=3).to(self.device)
        cnn_v2 = CNNEncoderV2().to(self.device)
        model_v2 = EngagementModelV2(d_model=256, nhead=8, num_layers=4).to(self.device)
        
        # ê°œë³„ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
        v1_checkpoint = torch.load("./log/best_model2.pt", map_location=self.device)
        cnn_v1.load_state_dict(v1_checkpoint['cnn_state_dict'])
        model_v1.load_state_dict(v1_checkpoint['model_state_dict'])
        
        v2_checkpoint = torch.load("./log/v2/best_model_v2.pt", map_location=self.device)
        cnn_v2.load_state_dict(v2_checkpoint['cnn_state_dict'])
        model_v2.load_state_dict(v2_checkpoint['model_state_dict'])
        
        # ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ë° ë¡œë“œ
        self.model = TransformerEnsembleModel(
            cnn_v1, model_v1, cnn_v2, model_v2, 
            ensemble_method='learned'
        ).to(self.device)
        
        # ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ë¡œë“œ
        ensemble_checkpoint = torch.load(model_path, map_location=self.device)
        self.model.load_state_dict(ensemble_checkpoint['ensemble_state_dict'])
        
        # ëª¨ë¸ì„ evaluation ëª¨ë“œë¡œ ì„¤ì •
        self.model.eval()
        for param in self.model.parameters():
            param.requires_grad = False
            
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        if 'accuracy' in ensemble_checkpoint:
            accuracy = ensemble_checkpoint['accuracy']
            print(f"âœ… ì•™ìƒë¸” ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì •í™•ë„: {accuracy:.1%})")
    
    def preprocess_frame(self, frame):
        """í”„ë ˆì„ ì „ì²˜ë¦¬"""
        # BGR to RGB
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        # PIL Imageë¡œ ë³€í™˜
        pil_image = Image.fromarray(frame_rgb)
        # ë³€í™˜ ì ìš©
        tensor = self.transform(pil_image)
        return tensor
    
    def predict_engagement(self):
        """í˜„ì¬ í”„ë ˆì„ ë²„í¼ë¡œ ì§‘ì¤‘ë„ ì˜ˆì¸¡"""
        if len(self.frame_buffer) < 30:
            return None, None
        
        # 30í”„ë ˆì„ì„ í…ì„œë¡œ ë³€í™˜
        frames = torch.stack(list(self.frame_buffer))  # (30, 3, 224, 224)
        frames = frames.unsqueeze(0).to(self.device)   # (1, 30, 3, 224, 224)
        
        # ë”ë¯¸ fusion features (ì‹¤ì‹œê°„ì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        fusion = torch.zeros(1, 5).to(self.device)
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        with torch.no_grad():
            logits = self.model(frames, fusion)
            probability = torch.sigmoid(logits).item()
            prediction = 1 if probability > self.threshold else 0
        
        return prediction, probability
    
    def update_statistics(self, prediction):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        if prediction is not None:
            self.recent_predictions.append(prediction)
            self.total_frames += 1
            if prediction == 0:  # ì§‘ì¤‘í•¨
                self.focused_frames += 1
    
    def get_current_stats(self):
        """í˜„ì¬ í†µê³„ ë°˜í™˜"""
        if len(self.recent_predictions) == 0:
            return 0, 0, 0
        
        recent_focus_rate = (len(self.recent_predictions) - sum(self.recent_predictions)) / len(self.recent_predictions)
        overall_focus_rate = self.focused_frames / max(self.total_frames, 1)
        current_prediction = self.recent_predictions[-1] if self.recent_predictions else 0
        
        return recent_focus_rate, overall_focus_rate, current_prediction
    
    def draw_overlay(self, frame, prediction, probability):
        """ê²°ê³¼ë¥¼ í”„ë ˆì„ì— ì˜¤ë²„ë ˆì´"""
        height, width = frame.shape[:2]
        
        # í˜„ì¬ í†µê³„ ê°€ì ¸ì˜¤ê¸°
        recent_focus, overall_focus, current_pred = self.get_current_stats()
        
        # ë°°ê²½ ì‚¬ê°í˜• ê·¸ë¦¬ê¸°
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, 10), (400, 150), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        # ì§‘ì¤‘ ìƒíƒœì— ë”°ë¥¸ ìƒ‰ìƒ ì„¤ì •
        if prediction == 0:  # ì§‘ì¤‘í•¨
            color = (0, 255, 0)  # ì´ˆë¡ìƒ‰
            status = "FOCUSED"
            status_ko = "ì§‘ì¤‘í•¨"
        else:  # ì§‘ì¤‘í•˜ì§€ ì•ŠìŒ
            color = (0, 0, 255)  # ë¹¨ê°„ìƒ‰
            status = "UNFOCUSED"
            status_ko = "ì§‘ì¤‘í•˜ì§€ì•ŠìŒ"
        
        # í…ìŠ¤íŠ¸ ì •ë³´ í‘œì‹œ
        cv2.putText(frame, f"Status: {status}", (20, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
        cv2.putText(frame, f"Korean: {status_ko}", (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        cv2.putText(frame, f"Probability: {probability:.3f}", (20, 85), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        cv2.putText(frame, f"Recent Focus: {recent_focus:.1%}", (20, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        cv2.putText(frame, f"Overall Focus: {overall_focus:.1%}", (20, 135), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # ìš°ì¸¡ì— ì§‘ì¤‘ë„ ë°” ê·¸ë¦¬ê¸°
        bar_x = width - 60
        bar_y = 50
        bar_height = 200
        bar_width = 30
        
        # ë°°ê²½ ë°”
        cv2.rectangle(frame, (bar_x, bar_y), (bar_x + bar_width, bar_y + bar_height), (100, 100, 100), -1)
        
        # ì§‘ì¤‘ë„ ë°” (ìµœê·¼ ì§‘ì¤‘ë„ ê¸°ì¤€)
        fill_height = int(bar_height * recent_focus)
        cv2.rectangle(frame, (bar_x, bar_y + bar_height - fill_height), 
                     (bar_x + bar_width, bar_y + bar_height), (0, 255, 0), -1)
        
        # ë°” í…Œë‘ë¦¬
        cv2.rectangle(frame, (bar_x, bar_y), (bar_x + bar_width, bar_y + bar_height), (255, 255, 255), 2)
        cv2.putText(frame, "Focus", (bar_x - 10, bar_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        return frame
    
    def run(self, camera_id=0, save_video=False):
        """ì‹¤ì‹œê°„ ì›¹ìº  ëª¨ë‹ˆí„°ë§ ì‹¤í–‰"""
        cap = cv2.VideoCapture(camera_id)
        
        if not cap.isOpened():
            print(f"âŒ ì¹´ë©”ë¼ {camera_id}ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì¹´ë©”ë¼ ì„¤ì •
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        cap.set(cv2.CAP_PROP_FPS, 30)
        
        # ë¹„ë””ì˜¤ ì €ì¥ ì„¤ì • (ì„ íƒì‚¬í•­)
        if save_video:
            fourcc = cv2.VideoWriter_fourcc(*'XVID')
            out = cv2.VideoWriter('engagement_monitor.avi', fourcc, 20.0, (640, 480))
        
        print("ğŸ¥ ì‹¤ì‹œê°„ ì§‘ì¤‘ë„ ëª¨ë‹ˆí„°ë§ ì‹œì‘!")
        print("   - 'q' í‚¤ë¥¼ ëˆŒëŸ¬ ì¢…ë£Œ")
        print("   - 'r' í‚¤ë¥¼ ëˆŒëŸ¬ í†µê³„ ë¦¬ì…‹")
        print("   - 'p' í‚¤ë¥¼ ëˆŒëŸ¬ ì˜ˆì¸¡ ì¼ì‹œì •ì§€/ì¬ê°œ")
        
        frame_count = 0
        prediction_active = True
        last_prediction_time = time.time()
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                
                # í”„ë ˆì„ì„ ë²„í¼ì— ì¶”ê°€
                processed_frame = self.preprocess_frame(frame)
                self.frame_buffer.append(processed_frame)
                
                # 3í”„ë ˆì„ë§ˆë‹¤ ì˜ˆì¸¡ ìˆ˜í–‰ (ì†ë„ ìµœì í™”)
                prediction, probability = None, None
                if prediction_active and frame_count % 3 == 0 and len(self.frame_buffer) == 30:
                    current_time = time.time()
                    prediction, probability = self.predict_engagement()
                    prediction_time = time.time() - current_time
                    
                    if prediction is not None:
                        self.update_statistics(prediction)
                        last_prediction_time = current_time
                        print(f"Frame {frame_count}: {'ì§‘ì¤‘í•¨' if prediction == 0 else 'ì§‘ì¤‘í•˜ì§€ì•ŠìŒ'} "
                              f"(í™•ë¥ : {probability:.3f}, ì²˜ë¦¬ì‹œê°„: {prediction_time:.3f}ì´ˆ)")
                
                # ë§ˆì§€ë§‰ ì˜ˆì¸¡ ê²°ê³¼ë¡œ ì˜¤ë²„ë ˆì´ (ì˜ˆì¸¡ì´ ì—†ìœ¼ë©´ ì´ì „ ê²°ê³¼ ì‚¬ìš©)
                if hasattr(self, '_last_prediction'):
                    display_pred, display_prob = self._last_prediction
                else:
                    display_pred, display_prob = prediction, probability
                
                if prediction is not None:
                    self._last_prediction = (prediction, probability)
                    display_pred, display_prob = prediction, probability
                
                # ì˜¤ë²„ë ˆì´ ê·¸ë¦¬ê¸°
                if display_pred is not None:
                    frame = self.draw_overlay(frame, display_pred, display_prob)
                
                # ì˜ˆì¸¡ ìƒíƒœ í‘œì‹œ
                status_text = "ACTIVE" if prediction_active else "PAUSED"
                cv2.putText(frame, f"Prediction: {status_text}", (10, frame.shape[0] - 20), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                
                # í™”ë©´ì— í‘œì‹œ
                cv2.imshow('Real-time Engagement Monitor', frame)
                
                # ë¹„ë””ì˜¤ ì €ì¥
                if save_video:
                    out.write(frame)
                
                # í‚¤ ì…ë ¥ ì²˜ë¦¬
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('r'):
                    # í†µê³„ ë¦¬ì…‹
                    self.recent_predictions.clear()
                    self.total_frames = 0
                    self.focused_frames = 0
                    print("ğŸ“Š í†µê³„ê°€ ë¦¬ì…‹ë˜ì—ˆìŠµë‹ˆë‹¤.")
                elif key == ord('p'):
                    # ì˜ˆì¸¡ ì¼ì‹œì •ì§€/ì¬ê°œ
                    prediction_active = not prediction_active
                    print(f"ğŸ”„ ì˜ˆì¸¡ {'ì¬ê°œ' if prediction_active else 'ì¼ì‹œì •ì§€'}")
                
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        finally:
            # ìµœì¢… í†µê³„ ì¶œë ¥
            recent_focus, overall_focus, _ = self.get_current_stats()
            print(f"\nğŸ“Š ìµœì¢… í†µê³„:")
            print(f"   - ì´ ì²˜ë¦¬ í”„ë ˆì„: {self.total_frames}")
            print(f"   - ì „ì²´ ì§‘ì¤‘ë¥ : {overall_focus:.1%}")
            print(f"   - ìµœê·¼ ì§‘ì¤‘ë¥ : {recent_focus:.1%}")
            
            # ì •ë¦¬
            cap.release()
            if save_video:
                out.release()
            cv2.destroyAllWindows()

# ------------------ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ------------------
def main():
    print("ğŸš€ ì‹¤ì‹œê°„ ì›¹ìº  ì§‘ì¤‘ë„ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*50)
    
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì • (ìµœì‹  ì•™ìƒë¸” ëª¨ë¸ ì‚¬ìš©)
    model_paths = [
        "./log/ensemble/best_speed_ensemble.pt",          # ì†ë„ ìµœì í™” ì•™ìƒë¸”
        "./log/ensemble/best_weighted_ensemble.pt",       # ê°€ì¤‘ ì•™ìƒë¸”
        "./log/ensemble/best_transformer_ensemble.pt",    # Transformer ì•™ìƒë¸”
    ]
    
    # ì‚¬ìš©í•  ëª¨ë¸ ì°¾ê¸°
    model_path = None
    for path in model_paths:
        if os.path.exists(path):
            model_path = path
            break
    
    if model_path is None:
        print("âŒ ì•™ìƒë¸” ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ ê²½ë¡œë“¤ì„ í™•ì¸í•˜ì„¸ìš”:")
        for path in model_paths:
            print(f"   - {path}")
        return
    
    print(f"ğŸ“‚ ì‚¬ìš©í•  ëª¨ë¸: {model_path}")
    
    # ì„¤ì • ì˜µì…˜
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    threshold = 0.7  # ì„ê³„ê°’ (Version 2 í…ŒìŠ¤íŠ¸ì—ì„œ ìµœì ê°’)
    camera_id = 0    # ì›¹ìº  ID (ë³´í†µ 0ì´ ê¸°ë³¸ ì›¹ìº )
    save_video = False  # ë¹„ë””ì˜¤ ì €ì¥ ì—¬ë¶€
    
    try:
        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„° ì´ˆê¸°í™”
        monitor = RealTimeEngagementMonitor(
            model_path=model_path,
            device=device,
            threshold=threshold
        )
        
        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        monitor.run(camera_id=camera_id, save_video=save_video)
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
