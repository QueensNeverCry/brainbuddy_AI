# ============================================================
# ğŸ“Œ Pipeline Overview (Flowchart Style) â€” Step 3: CLIP Embedding + Linear Probe
#
#  ROOT_DIRS (ì´ë¯¸ì§€ í´ë”ë“¤)        labels_zeroshot.csv
#             â”‚                             â”‚
#             â–¼                             â–¼
#   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#   â”‚ 1) ì„ë² ë”© ë¡œë“œ/ìƒì„±â”‚          â”‚ 2) ë¼ë²¨ ì¤€ë¹„ (ê·¹ë‹¨ ë¶„ìœ„ìˆ˜)â”‚
#   â”‚   - embeddings.npz â”‚          â”‚   - margin ìƒ/í•˜ìœ„ q%    â”‚
#   â”‚   - ì—†ìœ¼ë©´ ìƒˆë¡œ ê³„ì‚°â”‚          â”‚   - pos=1 / neg=0        â”‚
#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#              â”‚ X, ids                          â”‚ df_lab
#              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#                             â–¼
#                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#                 â”‚ 3) X,y ë¼ë²¨ ì •ë ¬          â”‚  <-- align_Xy(X, ids, df_lab)
#                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#                               â”‚ X_all, y_all, folders_all
#                               â–¼
#                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#                 â”‚ 4) ê·¸ë£¹ ê¸°ë°˜ Train/Val ë¶„í• â”‚
#                 â”‚   - GroupShuffleSplit      â”‚
#                 â”‚   - extract_group()        â”‚
#                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#                        Xtr,ytr â”‚ Xva,yva
#                               â–¼
#                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#                 â”‚ 5) Logistic Regression í•™ìŠµâ”‚
#                 â”‚   - class_weight="balanced"â”‚
#                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#                               â”‚ clf
#                               â–¼
#                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#                 â”‚ 6) ê²€ì¦                   â”‚
#                 â”‚   - classification_report â”‚
#                 â”‚   - ROC-AUC               â”‚
#                 â”‚   - threshold íŠœë‹        â”‚
#                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#                               â”‚ th, recall, precision
#                               â–¼
#                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#                 â”‚ 7) ì €ì¥                   â”‚
#                 â”‚   - linear_probe.npz      â”‚
#                 â”‚   - probe_meta.json       â”‚
#                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# Key Params:
#   MODEL_NAME, PRETRAINED, Q_EXTREME, VAL_RATIO,
#   TARGET_RECALL, RANDOM_SEED
#
# Outputs:
#   embeddings.npz (ì¬ì‚¬ìš©)
#   linear_probe.npz (W, b, classes, meta)
#   probe_meta.json (threshold, counts, config)
# ============================================================

import os, json
import numpy as np
import pandas as pd
from PIL import Image
from tqdm import tqdm

import torch
import open_clip
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.model_selection import GroupShuffleSplit

# ====== ê²½ë¡œ ê³ ì • ======
ROOT_DIRS = [
    r"C:/Users/user/Downloads/126.eye/0801/t/o/og/TS/TS/all_image_30/134_face_crop",
    r"C:/Users/user/Downloads/126.eye/0801/t/o/og/TS/TS/all_image_30/135_face_crop",
    r"C:/Users/user/Downloads/126.eye/0801/t/o/og/TS/TS/all_image_30/136_face_crop",
    r"C:/Users/user/Downloads/126.eye/01-1.data/Training/01.data/TS/001/T1/image_30_face_crop",
    r"C:/Users/user/Downloads/126.eye/01-1.data/Training/01.data/TS/002/T1/image_30_face_crop",
    r"C:/Users/user/Downloads/126.eye/01-1.data/Training/01.data/TS/003/T1/image_30_face_crop",
]
ZEROSHOT_CSV = r"C:\Users\user\Desktop\brainbuddy_AI\labels_zeroshot.csv"
EMB_PATH = "embeddings.npz"

# ====== ì„ë² ë”©/ëª¨ë¸ ì„¤ì • ======
MODEL_NAME = "ViT-B-32"       # CLIP ë°±ë³¸
PRETRAINED = "openai"         # ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
IMG_EXTS = (".png",".jpg",".jpeg",".bmp",".webp")
MAX_FRAMES = 30
BATCH_SIZE = 32

# ====== í•™ìŠµ íŒŒë¼ë¯¸í„° ======
Q_EXTREME = 0.20      # margin ìƒ/í•˜ìœ„ 20%ì”© ê·¹ë‹¨ ìƒ˜í”Œë§Œ ì‚¬ìš©
VAL_RATIO = 0.2       # validation ë¹„ìœ¨
TARGET_RECALL = 0.90  # recall ìš°ì„  threshold íŠœë‹ ëª©í‘œ
RANDOM_SEED = 42

# ===================== ìœ í‹¸ í•¨ìˆ˜ë“¤ =====================

def list_seq_folders(roots):
    """ROOT_DIRS í•˜ìœ„ì—ì„œ ì´ë¯¸ì§€ í¬í•¨ëœ ëª¨ë“  í´ë” ìˆ˜ì§‘"""
    seqs = []
    for rd in roots:
        rd = os.path.abspath(rd)
        if not os.path.exists(rd): continue
        for cur, _, files in os.walk(rd):
            if any(f.lower().endswith(IMG_EXTS) for f in files):
                seqs.append(cur)
    return sorted(set(seqs))

def list_images(folder):
    """í´ë” ë‚´ ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸"""
    try:
        files = sorted(f for f in os.listdir(folder) if f.lower().endswith(IMG_EXTS))
        return [os.path.join(folder, f) for f in files]
    except:
        return []

def even_sample(items, k):
    """ë¦¬ìŠ¤íŠ¸ì—ì„œ ê· ë“± ê°„ê²©ìœ¼ë¡œ kê°œ ìƒ˜í”Œë§"""
    if k<=0 or len(items)<=k: return items
    idx = np.linspace(0, len(items)-1, k, dtype=int)
    return [items[i] for i in idx]

def load_model():
    """CLIP ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ê¸° ë¡œë“œ"""
    model, _, preprocess = open_clip.create_model_and_transforms(
        MODEL_NAME, pretrained=PRETRAINED, device=DEVICE
    )
    model.eval()
    return model, preprocess

@torch.no_grad()
def embed_folder(model, preprocess, folder):
    """í´ë” ë‚´ ì´ë¯¸ì§€ë“¤ â†’ CLIP ì„ë² ë”© í‰ê· """
    imgs = list_images(folder)
    if not imgs: return None
    sel = even_sample(imgs, MAX_FRAMES)
    xs = []
    for p in sel:
        try:
            xs.append(preprocess(Image.open(p).convert("RGB")))
        except:
            pass
    if not xs: return None
    X = torch.stack(xs,0).to(DEVICE)

    embs = []
    for i in range(0, len(X), BATCH_SIZE):
        v = model.encode_image(X[i:i+BATCH_SIZE])
        v = v / v.norm(dim=-1, keepdim=True)
        embs.append(v)
    v = torch.cat(embs,0).mean(0).cpu().numpy()  # í‰ê·  pooling
    return v

def build_embeddings(folders, save_npz=EMB_PATH):
    """í´ë” ì „ì²´ ì„ë² ë”© ìƒì„± í›„ npz ì €ì¥"""
    print("[Build] embeddings...")
    model, preprocess = load_model()
    X, ids = [], []
    for fd in tqdm(folders, desc="Embedding folders"):
        v = embed_folder(model, preprocess, fd)
        if v is None: continue
        X.append(v); ids.append(fd)
    X = np.stack(X,0)
    np.savez(save_npz, X=X, ids=np.array(ids, dtype=object),
             MODEL_NAME=MODEL_NAME, PRETRAINED=PRETRAINED)
    print(f"[Saved] {save_npz}  (N={len(ids)}, D={X.shape[1]})")
    return X, ids

def load_embeddings(npz_path=EMB_PATH):
    """ê¸°ì¡´ ì„ë² ë”© ë¡œë“œ, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±"""
    if not os.path.exists(npz_path):
        folders = list_seq_folders(ROOT_DIRS)
        return build_embeddings(folders, npz_path)
    data = np.load(npz_path, allow_pickle=True)
    X = data["X"]; ids = data["ids"]
    print(f"[Load] embeddings: N={len(ids)}, D={X.shape[1]})")
    return X, ids

# ê·¸ë£¹ ì¶”ì¶œ: '.../<group>/<sequence_folder>' êµ¬ì¡°ì—ì„œ groupì„ ìƒìœ„ í´ë”ëª…ìœ¼ë¡œ
def extract_group(folder_path: str) -> str:
    """í´ë” ê²½ë¡œì—ì„œ ê·¸ë£¹ëª… ì¶”ì¶œ (ì‚¬ëŒ/ì„¸ì…˜ ë‹¨ìœ„ ëˆ„ìˆ˜ ë°©ì§€ìš©)"""
    p = os.path.normpath(folder_path)
    parts = p.split(os.sep)
    if len(parts) >= 2:
        return parts[-2]
    return parts[-1]

def split_train_val_grouped(X, y, folders, val_ratio=0.2, seed=42):
    """GroupShuffleSplitìœ¼ë¡œ ê·¸ë£¹ ë‹¨ìœ„ ë¶„í•  â†’ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€"""
    groups = np.array([extract_group(f) for f in folders])
    gss = GroupShuffleSplit(n_splits=1, test_size=val_ratio, random_state=seed)
    tr_idx, va_idx = next(gss.split(X, y, groups))
    return (X[tr_idx], y[tr_idx], X[va_idx], y[va_idx],
            [folders[i] for i in tr_idx], [folders[i] for i in va_idx])

# ===================== ë¼ë²¨ ì¤€ë¹„ =====================

def prepare_labels_extreme(df, q=Q_EXTREME):
    """
    ê·¹ë‹¨ ë¶„ìœ„ìˆ˜ ìƒ˜í”Œë§Œ ì‚¬ìš©í•´ ë¼ë²¨ ìƒì„±:
    - margin ìƒìœ„ (1-q) ë¶„ìœ„ìˆ˜ ì´ìƒ â†’ pos(1)
    - margin í•˜ìœ„ q ë¶„ìœ„ìˆ˜ ì´í•˜  â†’ neg(0)
    """
    need = ["folder","margin","p_focused"]
    df = df.dropna(subset=need).copy()

    low_thr  = float(np.quantile(df["margin"], q))
    high_thr = float(np.quantile(df["margin"], 1.0 - q))
    print(f"[Quantiles-extreme] low_thr={low_thr:.6f}, high_thr={high_thr:.6f}")

    neg_df = df[df["margin"] <= low_thr].copy()
    pos_df = df[df["margin"] >= high_thr].copy()

    neg_df["y"] = 0
    pos_df["y"] = 1

    # pos/neg ìˆ˜ë¥¼ ë™ì¼í•˜ê²Œ ë§ì¶¤
    n = min(len(pos_df), len(neg_df))
    pos_df = pos_df.sample(n=n, random_state=RANDOM_SEED)
    neg_df = neg_df.sample(n=n, random_state=RANDOM_SEED)

    both = pd.concat([pos_df, neg_df], ignore_index=True).drop_duplicates(subset=["folder"])
    print(f"[Class sizes (extreme)] pos={len(pos_df)}, neg={len(neg_df)}")
    return both

def align_Xy(X, ids, df_lab):
    """ì„ë² ë”© ë°°ì—´ Xì™€ ë¼ë²¨ DataFrameì„ align"""
    id2idx = {ids[i]: i for i in range(len(ids))}
    keep = [r.folder in id2idx for r in df_lab.itertuples()]
    df_lab = df_lab[keep].copy()
    idx = [id2idx[r.folder] for r in df_lab.itertuples()]
    X_sel = X[idx]
    y_sel = df_lab["y"].astype(int).values
    folders_sel = df_lab["folder"].tolist()
    return X_sel, y_sel, folders_sel

# ===================== íŠœë‹/í•™ìŠµ =====================

def tune_threshold_for_recall(y_true, p_prob, target_recall=0.90):
    """validationì—ì„œ recall â‰¥ ëª©í‘œë¥¼ ë§Œì¡±í•˜ëŠ” threshold íƒìƒ‰"""
    grid = np.linspace(0.1, 0.9, 161)
    best = None
    for th in grid:
        pred = (p_prob >= th).astype(int)
        tp = ((pred==1)&(y_true==1)).sum()
        fp = ((pred==1)&(y_true==0)).sum()
        fn = ((pred==0)&(y_true==1)).sum()
        recall = tp / max(tp+fn,1)
        precision = tp / max(tp+fp,1)
        score = (recall >= target_recall, precision, recall)
        if best is None or score > best[0]:
            best = (score, th, recall, precision)
    _, th, r, p = best
    return float(th), float(r), float(p)

# ===================== ë©”ì¸ =====================

def main():
    # (0) ì„ë² ë”© ë¡œë“œ/ìƒì„±
    X, ids = load_embeddings(EMB_PATH)

    # (1) ë¼ë²¨ ì¤€ë¹„ (ê·¹ë‹¨ ë¶„ìœ„ìˆ˜ ê¸°ë°˜)
    if not os.path.exists(ZEROSHOT_CSV):
        raise FileNotFoundError(f"CSV not found: {ZEROSHOT_CSV}")
    df = pd.read_csv(ZEROSHOT_CSV)

    df_lab = prepare_labels_extreme(df, q=Q_EXTREME)
    X_all, y_all, folders_all = align_Xy(X, ids, df_lab)
    print(f"[Dataset aligned] N={len(y_all)}, pos={(y_all==1).sum()}, neg={(y_all==0).sum()}")

    # (2) ê·¸ë£¹ ê¸°ë°˜ train/val ë¶„ë¦¬ (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)
    Xtr, ytr, Xva, yva, ftr, fva = split_train_val_grouped(
        X_all, y_all, folders_all, val_ratio=VAL_RATIO, seed=RANDOM_SEED
    )
    print("[Groups] train unique groups:", len(set(map(extract_group, ftr))),
          "val unique groups:", len(set(map(extract_group, fva))))

    # (3) ë¡œì§€ìŠ¤í‹± íšŒê·€ í•™ìŠµ
    clf = LogisticRegression(max_iter=2000, class_weight="balanced")
    clf.fit(Xtr, ytr)

    # (4) ê²€ì¦ ì„±ëŠ¥ í‰ê°€
    pred = clf.predict(Xva)
    prob = clf.predict_proba(Xva)[:,1]
    print("=== Validation Report ===")
    print(classification_report(yva, pred, digits=4))
    try:
        auc = roc_auc_score(yva, prob)
        print("ROC-AUC:", round(auc,4))
    except:
        pass

    # threshold íŠœë‹ (ëª©í‘œ recall ìš°ì„ )
    th, r, p = tune_threshold_for_recall(yva, prob, target_recall=TARGET_RECALL)
    print(f"[Tuned] threshold={th:.3f}  (recallâ‰ˆ{r:.3f}, precisionâ‰ˆ{p:.3f})")

    # (5) ëª¨ë¸ ì €ì¥
    np.savez("linear_probe.npz",
             W=clf.coef_, b=clf.intercept_, classes_=clf.classes_,
             MODEL_NAME=MODEL_NAME, PRETRAINED=PRETRAINED)
    with open("probe_meta.json","w",encoding="utf-8") as f:
        json.dump({
            "threshold": float(th),
            "embedding_dim": int(X.shape[1]),
            "train_size": int(len(ytr)),
            "val_size": int(len(yva)),
            "pos_count": int((y_all==1).sum()),
            "neg_count": int((y_all==0).sum()),
            "model": MODEL_NAME, "pretrained": PRETRAINED
        }, f, ensure_ascii=False, indent=2)
    print("[Saved] linear_probe.npz, probe_meta.json")

if __name__ == "__main__":
    main()
