# ============================================================
# ğŸ“Œ Pipeline Overview (Flowchart Style) â€” Step 1: Zero-shot Labeling
#
#   ROOT_DIRS (ì´ë¯¸ì§€ í¬í•¨ëœ ëª¨ë“  í´ë”)
#                    â”‚
#                    â–¼
#        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#        â”‚ 1) í´ë” ìˆ˜ì§‘             â”‚  <-- í•˜ìœ„ í´ë” ìˆœíšŒí•˜ë©° ì´ë¯¸ì§€ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
#        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#                      â”‚ folder
#                      â–¼
#        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#        â”‚ 2) ê· ë“± ìƒ˜í”Œë§           â”‚  <-- ê° í´ë”ì—ì„œ ìµœëŒ€ MAX_FRAMES ê³ ë¥´ê²Œ ì„ íƒ
#        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#                      â”‚ paths[k]
#                      â–¼
#        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#        â”‚ 3) CLIP ì„ë² ë”©           â”‚  <-- encode_image â†’ L2 normalize
#        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#                      â”‚ image_embs
#                      â–¼
#        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#        â”‚ 4) í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì„ë² ë”©â”‚  <-- POS_PROMPTS / NEG_PROMPTS â†’ encode_text
#        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#                      â”‚ pos_txt, neg_txt
#                      â–¼
#        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#        â”‚ 5) p_focused & margin    â”‚  <-- softmax ê¸°ë°˜ í™•ë¥ , pos-neg ì°¨ì´
#        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#                      â”‚ p, m
#                      â–¼
#        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#        â”‚ 6) threshold íŒì •        â”‚  <-- p >= THRESHOLD â†’ predicted_label
#        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#                      â”‚ row append
#                      â–¼
#        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#        â”‚ 7) CSV ì €ì¥              â”‚  <-- labels_zeroshot.csv ì¶œë ¥
#        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# Key Params:
#   MODEL_NAME, PRETRAINED, DEVICE
#   MIN_FRAMES, MAX_FRAMES
#   THRESHOLD
#
# Output:
#   labels_zeroshot.csv
#   (folder, p_focused, margin, predicted_label, â€¦)
# ============================================================
import os, math, time
from datetime import datetime
from typing import List
import numpy as np
import pandas as pd
from PIL import Image
from tqdm import tqdm

import torch
import open_clip

# ====== ê²½ë¡œ ì„¤ì •: ì§€ì •ëœ í´ë”ë“¤ ======
ROOT_DIRS = [
    r"C:/Users/user/Downloads/126.eye/0801/t/o/og/TS/TS/all_image_30/134_face_crop",
    r"C:/Users/user/Downloads/126.eye/0801/t/o/og/TS/TS/all_image_30/135_face_crop",
    r"C:/Users/user/Downloads/126.eye/0801/t/o/og/TS/TS/all_image_30/136_face_crop",
    r"C:/Users/user/Downloads/126.eye/01-1.data/Training/01.data/TS/001/T1/image_30_face_crop",
    r"C:/Users/user/Downloads/126.eye/01-1.data/Training/01.data/TS/002/T1/image_30_face_crop",
    r"C:/Users/user/Downloads/126.eye/01-1.data/Training/01.data/TS/003/T1/image_30_face_crop",
]

# ====== í•˜ì´í¼íŒŒë¼ë¯¸í„° / ì˜µì…˜ ======
MODEL_NAME = "ViT-B-32"       # CLIP ëª¨ë¸ êµ¬ì¡°
PRETRAINED = "openai"         # ì‚¬ì „í•™ìŠµëœ ê°€ì¤‘ì¹˜
MIN_FRAMES = 8                # ìµœì†Œ ì´ë¯¸ì§€ ìˆ˜ (ì´ ë¯¸ë§Œì´ë©´ ì œì™¸)
MAX_FRAMES = 30               # ê· ë“± ìƒ˜í”Œë§í•  ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜
BATCH_SIZE = 32               # ì¸ì½”ë”© ì‹œ ë°°ì¹˜ í¬ê¸°
THRESHOLD = 0.5               # focused íŒì • ì„ê³„ê°’
OUT_CSV = "labels_zeroshot.csv"  # ì¶œë ¥ CSV ê²½ë¡œ

# ====== ì œë¡œìƒ· ë¶„ë¥˜ í”„ë¡¬í”„íŠ¸ ======
POS_PROMPTS = [
    "ì´ ì‚¬ëŒì€ í™”ë©´ì— ì§‘ì¤‘í•˜ê³  ìˆë‹¤.",
    "The person is focusing on the screen.",
    "ì‹œì„ ì´ ëª©í‘œë¥¼ í–¥í•˜ê³  ìˆë‹¤.",
    "ê³ ê°œê°€ ì •ë©´ì´ë©° ì£¼ì˜ê°€ ííŠ¸ëŸ¬ì§€ì§€ ì•Šì•˜ë‹¤.",
]
NEG_PROMPTS = [
    "ì´ ì‚¬ëŒì€ ì‚°ë§Œí•˜ê±°ë‚˜ ë”´ì§“ì„ í•˜ê³  ìˆë‹¤.",
    "The person is distracted or unfocused.",
    "ì‹œì„ ì´ ë‹¤ë¥¸ ê³³ì„ ë³¸ë‹¤.",
    "ê³ ê°œê°€ ì˜†ìœ¼ë¡œ ëŒì•„ê°€ ìˆê±°ë‚˜ ë©í•´ ë³´ì¸ë‹¤.",
]

IMG_EXTS = (".png", ".jpg", ".jpeg", ".bmp", ".webp")


# ====== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ ======
def list_sequence_folders(roots: List[str]) -> List[str]:
    """ROOT_DIRS í•˜ìœ„ì—ì„œ ì´ë¯¸ì§€ê°€ ìˆëŠ” í´ë”(ì‹œí€€ìŠ¤ ë‹¨ìœ„)ë¥¼ ìˆ˜ì§‘"""
    seqs = []
    for rd in roots:
        rd = os.path.abspath(rd)
        if not os.path.exists(rd):
            continue
        for cur, dirs, files in os.walk(rd):
            if any(f.lower().endswith(IMG_EXTS) for f in files):
                seqs.append(cur)
    return sorted(set(seqs))  # ì¤‘ë³µ ì œê±° í›„ ì •ë ¬


def list_images(folder: str) -> List[str]:
    """í´ë” ë‚´ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    try:
        files = sorted(
            f for f in os.listdir(folder)
            if f.lower().endswith(IMG_EXTS)
        )
        return [os.path.join(folder, f) for f in files]
    except Exception:
        return []


def even_sample(items: List[str], k: int) -> List[str]:
    """ë¦¬ìŠ¤íŠ¸ì—ì„œ kê°œë¥¼ ê· ë“± ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§"""
    if k <= 0 or len(items) <= k:
        return items
    idxs = np.linspace(0, len(items)-1, k, dtype=int)
    return [items[i] for i in idxs]


def load_model(model_name: str, pretrained: str, device: str):
    """CLIP ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸°, í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    model, _, preprocess = open_clip.create_model_and_transforms(
        model_name, pretrained=pretrained, device=device
    )
    model.eval()
    tokenizer = open_clip.get_tokenizer(model_name)
    return model, tokenizer, preprocess


# ====== ì„ë² ë”© ê´€ë ¨ ======
@torch.no_grad()
def encode_texts(model, tokenizer, prompts: List[str], device: str) -> torch.Tensor:
    """í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì¸ì½”ë”© í›„ ì •ê·œí™”"""
    toks = tokenizer(prompts)
    t = model.encode_text(toks.to(device))
    return t / t.norm(dim=-1, keepdim=True)


@torch.no_grad()
def encode_images(model, preprocess, paths: List[str], device: str, batch_size: int = 32) -> torch.Tensor:
    """ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ â†’ ì„ë² ë”© ë²¡í„°"""
    xs = []
    for p in paths:
        try:
            img = Image.open(p).convert("RGB")
            xs.append(preprocess(img))
        except Exception:
            # ì†ìƒëœ íŒŒì¼ì€ ìŠ¤í‚µ
            pass
    if not xs:
        return torch.empty(0, model.visual.output_dim if hasattr(model, "visual") else 512)
    X = torch.stack(xs, 0).to(device)

    embs = []
    for i in range(0, len(X), batch_size):
        chunk = X[i:i+batch_size]
        v = model.encode_image(chunk)
        v = v / v.norm(dim=-1, keepdim=True)
        embs.append(v)
    return torch.cat(embs, 0)  # (n_valid, d)


def prob_and_margin(image_embs: torch.Tensor, pos_txt: torch.Tensor, neg_txt: torch.Tensor):
    """ì´ë¯¸ì§€ ì„ë² ë”© vs ê¸ì •/ë¶€ì • í”„ë¡¬í”„íŠ¸ ìœ ì‚¬ë„ â†’ í™•ë¥  ë° margin ê³„ì‚°"""
    if image_embs.numel() == 0:
        return None, None
    pos_sim = (image_embs @ pos_txt.T).mean().item()
    neg_sim = (image_embs @ neg_txt.T).mean().item()
    # softmax ìœ ì‚¬ ë°©ì‹ìœ¼ë¡œ í™•ë¥ í™”
    exp_pos, exp_neg = math.exp(pos_sim), math.exp(neg_sim)
    p_focused = exp_pos / (exp_pos + exp_neg + 1e-8)
    margin = pos_sim - neg_sim
    return float(p_focused), float(margin)


# ====== ë©”ì¸ í”„ë¡œì„¸ìŠ¤ ======
def main():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"[Device] {device}")

    # ëª¨ë¸/í† í¬ë‚˜ì´ì €/ì „ì²˜ë¦¬ ë¡œë“œ
    model, tokenizer, preprocess = load_model(MODEL_NAME, PRETRAINED, device)

    # ê¸ì •/ë¶€ì • í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©
    pos_txt = encode_texts(model, tokenizer, POS_PROMPTS, device)
    neg_txt = encode_texts(model, tokenizer, NEG_PROMPTS, device)

    # ROOT_DIRSì—ì„œ ì‹œí€€ìŠ¤ í´ë” ìˆ˜ì§‘
    seq_folders = list_sequence_folders(ROOT_DIRS)
    if not seq_folders:
        print("[!] ìœ íš¨í•œ ì´ë¯¸ì§€ í´ë”ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    rows = []
    t0 = time.time()

    # ê° í´ë”ë³„ ì²˜ë¦¬
    for folder in tqdm(seq_folders, desc="Zero-shot labeling"):
        imgs = list_images(folder)
        if len(imgs) < MIN_FRAMES:
            continue

        # ê· ë“± ìƒ˜í”Œë§
        sel = even_sample(imgs, MAX_FRAMES) if MAX_FRAMES > 0 else imgs

        # ì´ë¯¸ì§€ ì„ë² ë”© ì¶”ì¶œ
        embs = encode_images(model, preprocess, sel, device, BATCH_SIZE)
        if embs.numel() == 0:
            continue

        # í™•ë¥  ë° margin ê³„ì‚°
        p, m = prob_and_margin(embs, pos_txt, neg_txt)
        if p is None:
            continue

        # ì˜ˆì¸¡ ë¼ë²¨ ê²°ì • (1=focused, 0=unfocused)
        pred = 1 if p >= THRESHOLD else 0

        rows.append({
            "unit": "folder",
            "path": folder,                  # ëŒ€í‘œ í´ë” ê²½ë¡œ
            "folder": folder,
            "n_frames_agg": int(embs.shape[0]),
            "p_focused": p,
            "margin": m,
            "predicted_label": int(pred),
            "engine": f"{MODEL_NAME}/{PRETRAINED}",
            "timestamp": datetime.now().isoformat(timespec="seconds"),
        })

    # ê²°ê³¼ ì €ì¥
    if not rows:
        print("[!] ê²°ê³¼ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤. MIN_FRAMES/í™•ì¥ì/ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return

    df = pd.DataFrame(rows)
    df.to_csv(OUT_CSV, index=False, encoding="utf-8-sig")
    print(f"\nSaved -> {OUT_CSV} (rows={len(df)})")
    print(f"Elapsed: {time.time()-t0:.1f}s")


if __name__ == "__main__":
    main()
