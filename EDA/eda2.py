import os
import pickle
import cv2
import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from torch.utils.data import DataLoader
from torchvision import transforms, models
from PIL import Image
import pandas as pd
import seaborn as sns
from tqdm import tqdm

# ------------------ train6.pyì—ì„œ ê°€ì ¸ì˜¨ Dataset + folder_path ë°˜í™˜ ------------------
class VideoFolderDataset(torch.utils.data.Dataset):
    def __init__(self, data_list, transform=None):
        self.data_list = []
        self.transform = transform or transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])

        for folder_path, label in data_list:
            if os.path.isdir(folder_path):
                img_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                if len(img_files) >= 30:
                    self.data_list.append((folder_path, label))

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        folder_path, label = self.data_list[idx]
        img_files = sorted([f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])[:30]
        frames = []
        for f in img_files:
            img_path = os.path.join(folder_path, f)
            img = cv2.imread(img_path)
            if img is None:
                continue
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            img_pil = Image.fromarray(img)
            frames.append(self.transform(img_pil))
        video = torch.stack(frames)  # (T, 3, 224, 224)

        fusion_path = os.path.join(folder_path, "fusion_features.pkl")
        if os.path.exists(fusion_path):
            with open(fusion_path, 'rb') as f:
                fusion = torch.tensor(pickle.load(f), dtype=torch.float32)
        else:
            fusion = torch.zeros(5)

        # Bì•ˆ: í´ë” ê²½ë¡œê¹Œì§€ í•¨ê»˜ ë°˜í™˜
        return video, fusion, torch.tensor(label, dtype=torch.float32), folder_path

# ------------------ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ ------------------
def load_data(pkl_files):
    all_data = []
    for pkl_path in pkl_files:
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f)
            all_data.extend(data)
    return all_data

# ------------------ íë¦¼/ë°ê¸° ê³„ì‚° ------------------
def calculate_blur_brightness(img_rgb):
    gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)
    blur = cv2.Laplacian(gray, cv2.CV_64F).var()
    brightness = np.mean(gray)
    return blur, brightness

# ------------------ CNN Encoder (ë°°ì¹˜ ìœ ì§€) ------------------
class CNNEncoder(torch.nn.Module):
    def __init__(self, output_dim=128):
        super().__init__()
        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)
        self.features = mobilenet.features
        self.avgpool = torch.nn.AdaptiveAvgPool2d((4, 4))
        self.fc = torch.nn.Sequential(
            torch.nn.Linear(1280 * 4 * 4, output_dim),
            torch.nn.ReLU()
        )

    def forward(self, x):  # x: (B, T, 3, 224, 224)
        B, T, C, H, W = x.shape
        x = x.view(B * T, C, H, W)
        x = self.features(x)
        x = self.avgpool(x)
        x = x.view(B * T, -1)
        x = self.fc(x)                 # (B*T, D)
        x = x.view(B, T, -1).mean(1)   # (B, D)  â† ì‹œí€€ìŠ¤ í‰ê· ë§Œ
        return x

# ------------------ EDA ë©”ì¸ ------------------
if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    pkl_files = [
        "C:/KSEB/brainbuddy_AI/preprocess2/pickle_labels/train/20_01.pkl",
        "C:/KSEB/brainbuddy_AI/preprocess2/pickle_labels/train/20_03.pkl"
    ]
    data_list = load_data(pkl_files)
    dataset = VideoFolderDataset(data_list, transform=transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]))

    loader = DataLoader(
        dataset,
        batch_size=16,                  # ì—¬ìœ  ë˜ë©´ 4â†’8ë¡œ ì ì§„ í™•ëŒ€
        shuffle=False,
        num_workers=8,
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=4
    )

    cnn = CNNEncoder().to(device).eval()
    torch.backends.cudnn.benchmark = True

    records = []
    features = []
    labels = []

    # ìƒ˜í”Œ ë‹¨ìœ„ ì§„í–‰ë¥ ì„ ìœ„í•´ total=len(dataset)ë¡œ pbar êµ¬ì„±
    with tqdm(total=len(dataset), desc="EDA ì§„í–‰ì¤‘", unit="sample") as pbar:
        for videos, fusions, labels_batch, folders in loader:
            # ì„ íƒ: í”„ë ˆì„ ë‹¤ìš´ìƒ˜í”Œ (ì†ë„ â†‘)
            # videos = videos[:, ::3]

            videos = videos.to(device, non_blocking=True)  # (B, T, 3, 224, 224)

            # EDAëŠ” ì¶”ë¡ ëª¨ë“œ + (GPUë©´) AMP ê¶Œì¥
            if device.type == "cuda":
                with torch.inference_mode(), torch.cuda.amp.autocast():
                    feats_batch = cnn(videos)  # (B, D)
            else:
                with torch.inference_mode():
                    feats_batch = cnn(videos)

            B = videos.size(0)
            for b in range(B):
                folder_path = folders[b]
                label_int   = int(labels_batch[b].item())
                fusion_np   = fusions[b].numpy().flatten()
                feat_np     = feats_batch[b].detach().cpu().numpy()

                # íë¦¼/ë°ê¸°: ì²« í”„ë ˆì„ í•œ ì¥ìœ¼ë¡œ ì¸¡ì •
                first_frame = sorted([f for f in os.listdir(folder_path)
                                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))])[0]
                img = cv2.imread(os.path.join(folder_path, first_frame))
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                blur, brightness = calculate_blur_brightness(img)

                labels.append(label_int)
                features.append(feat_np)
                records.append({
                    "folder": folder_path,
                    "label": label_int,
                    "blur": blur,
                    "brightness": brightness,
                    **{f"fusion_{i}": fusion_np[i] for i in range(len(fusion_np))}
                })

            pbar.update(B)  # ìƒ˜í”Œ ë‹¨ìœ„ë¡œ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸

    df = pd.DataFrame(records)

    # ====== fusion ë¶„ì„/ì‹œê°í™” & CSV ì €ì¥ ======
    import os
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import StandardScaler

    os.makedirs("eda_outputs", exist_ok=True)

    # 1) ì›ì‹œ ë ˆì½”ë“œ ì €ì¥ (ìƒ˜í”Œ ë‹¨ìœ„)
    df.to_csv("eda_outputs/samples_with_fusion.csv", index=False, encoding="utf-8-sig")

    # 2) fusion í†µê³„ (ë¼ë²¨ë³„): mean/std/max/min + CSV ì €ì¥
    fusion_cols = [c for c in df.columns if c.startswith("fusion_")]
    fusion_stats = df.groupby("label")[fusion_cols].agg(["mean", "std", "max", "min"])

    # ë©€í‹°ì¸ë±ìŠ¤ ì»¬ëŸ¼ í‰íƒ„í™” í›„ ì €ì¥
    fusion_stats_flat = fusion_stats.copy()
    fusion_stats_flat.columns = [f"{col}_{stat}" for col, stat in fusion_stats.columns]
    fusion_stats_flat.to_csv("eda_outputs/fusion_stats_by_label.csv", encoding="utf-8-sig")

    print("Fusion Feature Stats by Label:\n", fusion_stats)

    # 3) fusion ì°¨ì› ê°„ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ (ì „ì²´)
    plt.figure(figsize=(6, 5))
    corr = df[fusion_cols].corr()
    sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1)
    plt.title("Fusion Correlation (All Samples)")
    plt.tight_layout()
    plt.savefig("eda_outputs/fusion_corr_all.png", dpi=200)
    plt.close()

    # (ì„ íƒ) ë¼ë²¨ë³„ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
    for lab, sub in df.groupby("label"):
        if len(sub) < 3:  # í‘œë³¸ì´ ë„ˆë¬´ ì ìœ¼ë©´ ìŠ¤í‚µ
            continue
        plt.figure(figsize=(6,5))
        sns.heatmap(sub[fusion_cols].corr(), annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1)
        plt.title(f"Fusion Correlation (Label={lab})")
        plt.tight_layout()
        plt.savefig(f"eda_outputs/fusion_corr_label_{lab}.png", dpi=200)
        plt.close()

    # 4) ë¼ë²¨ë³„ fusion ë¶„í¬(ë°”ì´ì˜¬ë¦°/ë°•ìŠ¤)
    for c in fusion_cols:
        plt.figure(figsize=(7,5))
        sns.violinplot(data=df, x="label", y=c, inner="box", cut=0)
        plt.title(f"{c} Distribution by Label")
        plt.tight_layout()
        plt.savefig(f"eda_outputs/{c}_violin_by_label.png", dpi=200)
        plt.close()

    # 5) ë¼ë²¨ë³„ í‰ê· Â±í‘œì¤€í¸ì°¨ (ì—ëŸ¬ë°”)
    fusion_means = df.groupby("label")[fusion_cols].mean().reset_index()
    fusion_stds  = df.groupby("label")[fusion_cols].std().reset_index()

    # ë§‰ëŒ€ + ì—ëŸ¬ë°”(ê° fusion ì°¨ì›ë³„ë¡œ í•œ ê·¸ë¦¼)
    for c in fusion_cols:
        plt.figure(figsize=(7,5))
        means = fusion_means[["label", c]].set_index("label")[c]
        errs  = fusion_stds[["label", c]].set_index("label")[c]
        means.plot(kind="bar", yerr=errs, capsize=4, alpha=0.8)
        plt.ylabel(c)
        plt.title(f"{c} Mean Â± STD by Label")
        plt.tight_layout()
        plt.savefig(f"eda_outputs/{c}_mean_std_by_label.png", dpi=200)
        plt.close()

    # 6) PCAë¡œ fusion ë²¡í„° 2D íˆ¬ì˜ (ìƒ‰: ë¼ë²¨)
    X = df[fusion_cols].values
    y = df["label"].values
    X_std = StandardScaler().fit_transform(X)

    pca = PCA(n_components=2, random_state=42)
    X_pca = pca.fit_transform(X_std)

    plt.figure(figsize=(7,6))
    sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=y, palette="coolwarm", alpha=0.35)
    plt.title(f"Fusion PCA (2D)  â€¢ VarExp= PC1 {pca.explained_variance_ratio_[0]:.2f}, PC2 {pca.explained_variance_ratio_[1]:.2f}")
    plt.xlabel("PC1")
    plt.ylabel("PC2")
    plt.tight_layout()
    plt.savefig("eda_outputs/fusion_pca_2d.png", dpi=200)
    plt.close()

    #7) (ì˜µì…˜) t-SNEë¡œ fusion ë²¡í„° íˆ¬ì˜ (ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¬ë©´ ìƒ˜í”Œë§)
    from sklearn.manifold import TSNE
    tsne = TSNE(n_components=2, random_state=42, perplexity=20, max_iter=500)
    X_tsne = tsne.fit_transform(X_std)
    plt.figure(figsize=(7,6))
    sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=y, palette="coolwarm", alpha=0.35)
    plt.title("Fusion t-SNE (2D)")
    plt.tight_layout()
    plt.savefig("eda_outputs/fusion_tsne_2d.png", dpi=200)
    plt.close()

    print("âœ… Fusion ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:",
        "\n- eda_outputs/samples_with_fusion.csv",
        "\n- eda_outputs/fusion_stats_by_label.csv",
        "\n- eda_outputs/fusion_corr_all.png (ë° ë¼ë²¨ë³„ íŒŒì¼)",
        "\n- eda_outputs/<fusion_i>_violin_by_label.png",
        "\n- eda_outputs/<fusion_i>_mean_std_by_label.png",
        "\n- eda_outputs/fusion_pca_2d.png")

    # ğŸ“Š íë¦¼/ë°ê¸° ì‹œê°í™” (ë¼ë²¨ë³„ ìƒ‰ìƒ)
    plt.figure(figsize=(8, 6))
    sns.scatterplot(data=df, x="brightness", y="blur", hue="label", palette="coolwarm",alpha=0.3)
    plt.title("Brightness vs Blur (by Label)")
    plt.savefig("brightness_vs_blur.png")
    plt.close()

    # ğŸ“Š Fusion feature í†µê³„
    fusion_cols = [c for c in df.columns if c.startswith("fusion_")]
    fusion_stats = df.groupby("label")[fusion_cols].agg(["mean", "std", "max", "min"])
    print("Fusion Feature Stats by Label:\n", fusion_stats)

    # ğŸ“Š t-SNE ì‹œê°í™” (ë¹ ë¥´ê²Œ í•˜ë ¤ë©´ ì°¨ì›ì¶•ì†Œë‚˜ ìƒ˜í”Œë§ ê³ ë ¤)
    features_arr = np.array(features)
    features_arr = StandardScaler().fit_transform(features_arr)
    tsne = TSNE(n_components=2, random_state=42, max_iter=500, perplexity=20)  # ê¸°ë³¸ë³´ë‹¤ ê°€ë³ê²Œ
    tsne_feats = tsne.fit_transform(features_arr)

    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=tsne_feats[:, 0], y=tsne_feats[:, 1], hue=labels, palette="coolwarm",alpha=0.3)
    plt.title("t-SNE of CNN Features")
    plt.savefig("tsne_cnn_features.png")
    plt.close()

    print("âœ… EDA ì™„ë£Œ: brightness_vs_blur.png, tsne_cnn_features.png ì €ì¥ë¨")
