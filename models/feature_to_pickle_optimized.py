import os
import cv2
import torch
import pickle
from tqdm import tqdm
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
import mediapipe as mp
from models.cnn_encoder import CNNEncoder

# â”€â”€â”€ ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

# â”€â”€â”€ MediaPipe ì–¼êµ´ ê²€ì¶œê¸° ì´ˆê¸°í™” (í•œ ë²ˆë§Œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
mp_detector = mp.solutions.face_detection.FaceDetection(
    model_selection=0,
    min_detection_confidence=0.5
)

# â”€â”€â”€ ì´ë¯¸ì§€ ì „ì²˜ë¦¬(transform) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# â”€â”€â”€ í†µê³„ ì¹´ìš´í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
skip_path_count = 0
skip_frame_count = 0
load_fail_count = 0

# â”€â”€â”€ DataLoaderìš© None ì œê±° í•¨ìˆ˜(ëª¨ë“ˆ ìµœìƒë‹¨ì— ì •ì˜) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def collate_remove_none(batch):
    """Noneì¸ í•­ëª©ì„ ê±¸ëŸ¬ë‚¸ í›„ (frames, label) í˜ì–´ë§Œ ë‚¨ê¹€"""
    return [item for item in batch if item is not None]

# â”€â”€â”€ ì–¼êµ´ í¬ë¡­ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def crop_face(img_bgr, fallback_to_full=True):
    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
    h, w, _ = img_rgb.shape
    results = mp_detector.process(img_rgb)
    if results.detections:
        best = max(
            results.detections,
            key=lambda d: (
                d.location_data.relative_bounding_box.width *
                d.location_data.relative_bounding_box.height
            )
        )
        bbox = best.location_data.relative_bounding_box
        x1 = max(int(bbox.xmin * w), 0)
        y1 = max(int(bbox.ymin * h), 0)
        x2 = min(x1 + int(bbox.width * w), w)
        y2 = min(y1 + int(bbox.height * h), h)
        return img_rgb[y1:y2, x1:x2]
    return img_rgb if fallback_to_full else None

# â”€â”€â”€ Dataset ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class FrameFolderDataset(Dataset):
    def __init__(self, dataset_link, T=100):
        self.dataset_link = dataset_link
        self.T = T

    def __len__(self):
        return len(self.dataset_link)

    def __getitem__(self, idx):
        global skip_path_count, skip_frame_count, load_fail_count
        folder, label = self.dataset_link[idx]
        if not os.path.exists(folder):
            skip_path_count += 1
            return None
        files = sorted(
            f for f in os.listdir(folder)
            if f.lower().endswith(('.jpg', '.jpeg', '.png'))
        )
        if len(files) < self.T:
            skip_frame_count += 1
            return None

        tensors = []
        for fname in files[:self.T]:
            img = cv2.imread(os.path.join(folder, fname))
            if img is None:
                load_fail_count += 1
                return None
            face = crop_face(img)
            tensors.append(transform(face))
        return torch.stack(tensors), torch.tensor(label, dtype=torch.float32)

# â”€â”€â”€ ë©”ì¸ ê¸°ëŠ¥ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save_features_as_pkl(
    dataset_link,
    save_path,
    device=None,
    T=100,
    batch_size=4,
    num_workers=4
):
    global skip_path_count, skip_frame_count, load_fail_count

    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    os.makedirs(os.path.dirname(save_path), exist_ok=True)

    model = CNNEncoder().to(device)
    if device.type == 'cuda':
        model.half()
    model.eval()

    dataset = FrameFolderDataset(dataset_link, T)
    loader = DataLoader(
        dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=collate_remove_none   # ì—¬ê¸°ì„œ ëŒë‹¤ ëŒ€ì‹  ìµœìƒë‹¨ í•¨ìˆ˜ ì‚¬ìš©
    )

    all_features = []
    all_labels = []

    for batch in tqdm(loader, desc="ğŸ“¦ Feature ì¶”ì¶œ ì¤‘"):
        frames, labels = zip(*batch)
        frames = torch.stack(frames)
        if device.type == 'cuda':
            frames = frames.half()
        frames = frames.to(device, non_blocking=True)

        with torch.no_grad():
            feats = model(frames)
        feats = feats.cpu()

        all_features.extend(feats)
        all_labels.extend(labels)

    with open(save_path, "wb") as f:
        pickle.dump({"features": all_features, "labels": all_labels}, f)

    print(f"[âœ… ì €ì¥ ì™„ë£Œ] {save_path} | ì´ ìƒ˜í”Œ: {len(all_features)}")
    print("\nğŸ“Š ì²˜ë¦¬ í†µê³„:")
    print(f"  [ê²½ë¡œ ì—†ìŒ] {skip_path_count}")
    print(f"  [í”„ë ˆì„ ë¶€ì¡±] {skip_frame_count}")
    print(f"  [ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨] {load_fail_count}")
    print(f"  [ì •ìƒ ì¶”ì¶œ ì™„ë£Œ] {len(all_features)}")

# â”€â”€â”€ ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ ì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import sys
    pkl_path  = sys.argv[1] if len(sys.argv) > 1 else "preprocess2/pickle_labels/valid/20_01.pkl"
    save_path = sys.argv[2] if len(sys.argv) > 2 else "cnn_features/features/valid_20_01.pkl"

    with open(pkl_path, "rb") as f:
        dataset_link = pickle.load(f)
    save_features_as_pkl(dataset_link, save_path=save_path)
