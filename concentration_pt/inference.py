import torch
import cv2
import numpy as np
from collections import deque
import time
import argparse
import os

from models.pytorch_concentration import create_model
from utils.face_detector import FaceDetector
from utils.attention_features import AttentionFeatureExtractor

class PyTorchConcentrationInference:
    """PyTorch ëª¨ë¸ ê¸°ë°˜ ì‹¤ì‹œê°„ ì§‘ì¤‘ë„ ë¶„ì„"""
    
    def __init__(self, model_path, model_type='lstm'):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_type = model_type
        
        # ëª¨ë¸ ë¡œë“œ
        self.load_model(model_path)
        
        # ì–¼êµ´ ê²€ì¶œ ë° íŠ¹ì§• ì¶”ì¶œê¸°
        self.face_detector = FaceDetector()
        self.feature_extractor = AttentionFeatureExtractor()
        
        # 30í”„ë ˆì„ ë²„í¼
        self.frame_buffer = deque(maxlen=30)
        self.analysis_results = []
        
        # UI ì„¤ì •
        self.cls_name = {0: 'Not Focused', 1: 'Focused'}
        self.cls_color = {0: (0, 0, 255), 1: (0, 255, 0)}
        
        # ì§ì‚¬ê°í˜• ì§‘ì¤‘ ì˜ì—­
        self.attention_zone_width = 720
        self.attention_zone_height = 180
        
        print(f"âœ… PyTorch ì§‘ì¤‘ë„ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"ëª¨ë¸: {model_type}")
        print(f"ë””ë°”ì´ìŠ¤: {self.device}")
    
    def load_model(self, model_path):
        """PyTorch ëª¨ë¸ ë¡œë“œ"""
        print(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ: {model_path}")
        
        checkpoint = torch.load(model_path, map_location=self.device)
        
        # ëª¨ë¸ ìƒì„±
        self.model = create_model(self.model_type, input_dim=31)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (F1: {checkpoint.get('val_f1', 'N/A'):.4f})")
    
    def extract_frame_features(self, frame):
        """í”„ë ˆì„ì—ì„œ íŠ¹ì§• ì¶”ì¶œ"""
        # ì–¼êµ´ ê²€ì¶œ
        face_box = self.face_detector.detect_face(frame)
        
        # íŠ¹ì§• ì¶”ì¶œ (26ì°¨ì› + 5ì°¨ì› attention = 31ì°¨ì›)
        features, attention_features = self.feature_extractor.extract_features(frame, face_box)
        
        # íŠ¹ì§• ê²°í•©
        combined_features = np.concatenate([
            features,
            [
                attention_features['central_focus'],
                attention_features['gaze_fixation'],
                attention_features['head_stability'],
                attention_features['face_orientation'],
                attention_features['attention_score']
            ]
        ])
        
        return combined_features, face_box, attention_features
    
    def predict_concentration(self):
        """30í”„ë ˆì„ìœ¼ë¡œ ì§‘ì¤‘ë„ ì˜ˆì¸¡"""
        if len(self.frame_buffer) < 30:
            return None, 0.0
        
        # 30í”„ë ˆì„ì„ í…ì„œë¡œ ë³€í™˜
        sequence = torch.FloatTensor(list(self.frame_buffer)).unsqueeze(0)  # [1, 30, 31]
        sequence = sequence.to(self.device)
        
        # ëª¨ë¸ ì˜ˆì¸¡
        with torch.no_grad():
            output = self.model(sequence)
            confidence = output.item()
            prediction = 1 if confidence > 0.5 else 0
        
        return prediction, confidence
    
    def run_realtime_analysis(self):
        """ì‹¤ì‹œê°„ ë¶„ì„ ì‹¤í–‰"""
        cap = cv2.VideoCapture(0)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        
        if not cap.isOpened():
            print("âŒ ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        frame_count = 0
        analysis_count = 0
        start_time = time.time()
        
        print("ğŸš€ PyTorch ì‹¤ì‹œê°„ ì§‘ì¤‘ë„ ë¶„ì„ ì‹œì‘")
        print("30í”„ë ˆì„ì´ ìŒ“ì´ë©´ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        print("ESC í‚¤ë¡œ ì¢…ë£Œ")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            # í”„ë ˆì„ë³„ íŠ¹ì§• ì¶”ì¶œ
            features, face_box, attention_features = self.extract_frame_features(frame)
            self.frame_buffer.append(features)
            
            # 30í”„ë ˆì„ë§ˆë‹¤ ë¶„ì„
            if frame_count % 30 == 0 and len(self.frame_buffer) == 30:
                analysis_count += 1
                prediction, confidence = self.predict_concentration()
                
                if prediction is not None:
                    # ê²°ê³¼ ì €ì¥
                    result = {
                        'frame': frame_count,
                        'analysis': analysis_count,
                        'prediction': prediction,
                        'confidence': confidence,
                        'timestamp': time.time()
                    }
                    self.analysis_results.append(result)
                    
                    # ê²°ê³¼ ì¶œë ¥
                    status = "ğŸ¯ ì§‘ì¤‘" if prediction == 1 else "âŒ ë¹„ì§‘ì¤‘"
                    print(f"[ë¶„ì„ {analysis_count:3d}] {status} | í™•ì‹ ë„: {confidence:.3f} | í”„ë ˆì„: {frame_count}")
            
            # UI ê·¸ë¦¬ê¸°
            self.draw_ui(frame, face_box, attention_features)
            
            # FPS í‘œì‹œ
            elapsed = time.time() - start_time
            fps = frame_count / elapsed if elapsed > 0 else 0
            cv2.putText(frame, f"FPS: {fps:.1f}", (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            
            # ë²„í¼ ìƒíƒœ
            buffer_status = f"Buffer: {len(self.frame_buffer)}/30"
            cv2.putText(frame, buffer_status, (10, 70), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)
            
            # ë¶„ì„ íšŸìˆ˜
            cv2.putText(frame, f"Analysis: {analysis_count}", (10, 110), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 255), 2)
            
            cv2.imshow("PyTorch Concentration Analysis", frame)
            
            if cv2.waitKey(1) & 0xFF == 27:  # ESC
                break
        
        # ê²°ê³¼ ìš”ì•½
        self.print_summary(start_time)
        cap.release()
        cv2.destroyAllWindows()
    
    def draw_ui(self, frame, face_box, attention_features):
        """UI ê·¸ë¦¬ê¸°"""
        # ìµœê·¼ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
        if self.analysis_results:
            recent_result = self.analysis_results[-1]
            prediction = recent_result['prediction']
            confidence = recent_result['confidence']
            
            color = self.cls_color[prediction]
            status_text = "FOCUSED" if prediction == 1 else "NOT FOCUSED"
            
            cv2.putText(frame, f"Status: {status_text}", (10, 160), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)
            cv2.putText(frame, f"Confidence: {confidence:.3f}", (10, 200), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
        
        # ì–¼êµ´ ë°•ìŠ¤
        if face_box is not None:
            x, y, w, h = face_box
            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)
        
        # ì§‘ì¤‘ ì˜ì—­ (ì§ì‚¬ê°í˜•)
        center_x, center_y = frame.shape[1] // 2, frame.shape[0] // 2
        rect_left = center_x - self.attention_zone_width // 2
        rect_right = center_x + self.attention_zone_width // 2
        rect_top = center_y - self.attention_zone_height // 2
        rect_bottom = center_y + self.attention_zone_height // 2
        
        cv2.rectangle(frame, (rect_left, rect_top), (rect_right, rect_bottom), (255, 255, 255), 2)
        
        # ëª¨ë¸ ì •ë³´
        cv2.putText(frame, f"Model: PyTorch {self.model_type.upper()}", (10, frame.shape[0] - 50), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        cv2.putText(frame, f"Device: {self.device}", (10, frame.shape[0] - 20), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (128, 128, 128), 1)
    
    def print_summary(self, start_time):
        """ê²°ê³¼ ìš”ì•½"""
        total_time = time.time() - start_time
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š PyTorch ì‹¤ì‹œê°„ ë¶„ì„ ê²°ê³¼")
        print(f"{'='*60}")
        print(f"ëª¨ë¸: {self.model_type}")
        print(f"ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.1f}ì´ˆ")
        print(f"ì´ ë¶„ì„ íšŸìˆ˜: {len(self.analysis_results)}ë²ˆ")
        
        if self.analysis_results:
            focus_count = sum(1 for r in self.analysis_results if r['prediction'] == 1)
            focus_ratio = focus_count / len(self.analysis_results) * 100
            
            print(f"ì§‘ì¤‘ íŒì •: {focus_count}/{len(self.analysis_results)}ë²ˆ ({focus_ratio:.1f}%)")
            
            avg_confidence = np.mean([r['confidence'] for r in self.analysis_results])
            print(f"í‰ê·  í™•ì‹ ë„: {avg_confidence:.3f}")
        
        print(f"{'='*60}")

def main():
    parser = argparse.ArgumentParser(description='PyTorch ì§‘ì¤‘ë„ ëª¨ë¸ ì‹¤ì‹œê°„ ì¶”ë¡ ')
    parser.add_argument('--model', type=str, required=True,
                       help='PyTorch ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (.pt)')
    parser.add_argument('--model_type', type=str, default='lstm',
                       choices=['lstm', 'transformer', 'cnn1d'],
                       help='ëª¨ë¸ íƒ€ì…')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.model):
        print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.model}")
        return
    
    # ì¶”ë¡ ê¸° ìƒì„± ë° ì‹¤í–‰
    inference = PyTorchConcentrationInference(args.model, args.model_type)
    
    try:
        inference.run_realtime_analysis()
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
