import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, random_split
import numpy as np
from tqdm import tqdm
import os
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import argparse

from models.pytorch_concentration import create_model

class ConcentrationTrainer:
    """ì§‘ì¤‘ë„ ëª¨ë¸ í•™ìŠµê¸°"""
    
    def __init__(self, model_type='lstm', lr=0.001, batch_size=32, epochs=100):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_type = model_type
        self.lr = lr
        self.batch_size = batch_size
        self.epochs = epochs
        
        print(f"ğŸ¯ í•™ìŠµ ì„¤ì •")
        print(f"  ëª¨ë¸: {model_type}")
        print(f"  ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"  í•™ìŠµë¥ : {lr}")
        print(f"  ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"  ì—í­: {epochs}")
        
    def load_dataset(self, dataset_path):
        """ë°ì´í„°ì…‹ ë¡œë“œ"""
        print(f"ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë“œ: {dataset_path}")
        
        dataset = torch.load(dataset_path)
        sequences = dataset['sequences']
        labels = dataset['labels']
        
        print(f"  ì‹œí€€ìŠ¤ ìˆ˜: {len(sequences)}")
        print(f"  íŠ¹ì§• ì°¨ì›: {dataset['feature_dim']}")
        print(f"  ì‹œí€€ìŠ¤ ê¸¸ì´: {dataset['sequence_length']}")
        
        # í…ì„œë¡œ ë³€í™˜
        X = torch.stack(sequences)  # [N, 30, 31]
        y = torch.FloatTensor(labels).unsqueeze(1)  # [N, 1]
        
        # ë°ì´í„°ì…‹ ë¶„í•  (8:1:1)
        total_size = len(sequences)
        train_size = int(0.8 * total_size)
        val_size = int(0.1 * total_size)
        test_size = total_size - train_size - val_size
        
        # ëœë¤ ë¶„í• 
        indices = torch.randperm(total_size)
        train_indices = indices[:train_size]
        val_indices = indices[train_size:train_size + val_size]
        test_indices = indices[train_size + val_size:]
        
        # ë¶„í• ëœ ë°ì´í„°
        train_X, train_y = X[train_indices], y[train_indices]
        val_X, val_y = X[val_indices], y[val_indices]
        test_X, test_y = X[test_indices], y[test_indices]
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        train_dataset = TensorDataset(train_X, train_y)
        val_dataset = TensorDataset(val_X, val_y)
        test_dataset = TensorDataset(test_X, test_y)
        
        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        self.val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)
        self.test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)
        
        print(f"  í•™ìŠµ ë°ì´í„°: {len(train_dataset)}")
        print(f"  ê²€ì¦ ë°ì´í„°: {len(val_dataset)}")
        print(f"  í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_dataset)}")
        
        # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
        train_focus_ratio = train_y.mean().item() * 100
        print(f"  í•™ìŠµ ì§‘ì¤‘ ë¹„ìœ¨: {train_focus_ratio:.1f}%")
        
        return dataset['feature_dim']
    
    def create_model(self, input_dim):
        """ëª¨ë¸ ìƒì„±"""
        self.model = create_model(self.model_type, input_dim=input_dim)
        self.model = self.model.to(self.device)
        
        # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
        total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"ğŸ§  ëª¨ë¸ íŒŒë¼ë¯¸í„°: {total_params:,}ê°œ")
        
        # ì†ì‹¤í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €
        self.criterion = nn.BCELoss()
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-4)
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=self.epochs)
        
    def train_epoch(self):
        """í•œ ì—í­ í•™ìŠµ"""
        self.model.train()
        total_loss = 0
        
        for batch_X, batch_y in tqdm(self.train_loader, desc="Training"):
            batch_X = batch_X.to(self.device)
            batch_y = batch_y.to(self.device)
            
            self.optimizer.zero_grad()
            outputs = self.model(batch_X)
            loss = self.criterion(outputs, batch_y)
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
        
        return total_loss / len(self.train_loader)
    
    def validate_epoch(self):
        """í•œ ì—í­ ê²€ì¦"""
        self.model.eval()
        total_loss = 0
        all_preds = []
        all_probs = []
        all_labels = []
        
        with torch.no_grad():
            for batch_X, batch_y in tqdm(self.val_loader, desc="Validation"):
                batch_X = batch_X.to(self.device)
                batch_y = batch_y.to(self.device)
                
                outputs = self.model(batch_X)
                loss = self.criterion(outputs, batch_y)
                total_loss += loss.item()
                
                # ì˜ˆì¸¡ ìˆ˜ì§‘
                probs = outputs.cpu().numpy()
                preds = (probs > 0.5).astype(int)
                labels = batch_y.cpu().numpy()
                
                all_probs.extend(probs.flatten())
                all_preds.extend(preds.flatten())
                all_labels.extend(labels.flatten())
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = {
            'loss': total_loss / len(self.val_loader),
            'accuracy': accuracy_score(all_labels, all_preds),
            'precision': precision_score(all_labels, all_preds),
            'recall': recall_score(all_labels, all_preds),
            'f1': f1_score(all_labels, all_preds),
            'auc': roc_auc_score(all_labels, all_probs)
        }
        
        return metrics
    
    def train(self):
        """ì „ì²´ í•™ìŠµ ê³¼ì •"""
        print(f"\nğŸš€ í•™ìŠµ ì‹œì‘")
        
        train_losses = []
        val_metrics = []
        best_f1 = 0
        patience = 15
        patience_counter = 0
        
        for epoch in range(self.epochs):
            # í•™ìŠµ
            train_loss = self.train_epoch()
            train_losses.append(train_loss)
            
            # ê²€ì¦
            val_metrics_epoch = self.validate_epoch()
            val_metrics.append(val_metrics_epoch)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            self.scheduler.step()
            
            # ë¡œê¹…
            print(f"\nEpoch {epoch+1}/{self.epochs}")
            print(f"  Train Loss: {train_loss:.4f}")
            print(f"  Val Loss: {val_metrics_epoch['loss']:.4f}")
            print(f"  Val Acc: {val_metrics_epoch['accuracy']:.4f}")
            print(f"  Val F1: {val_metrics_epoch['f1']:.4f}")
            print(f"  Val AUC: {val_metrics_epoch['auc']:.4f}")
            print(f"  LR: {self.scheduler.get_last_lr()[0]:.6f}")
            
            # ëª¨ë¸ ì €ì¥
            current_f1 = val_metrics_epoch['f1']
            if current_f1 > best_f1:
                best_f1 = current_f1
                patience_counter = 0
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                os.makedirs('./checkpoints', exist_ok=True)
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'val_f1': current_f1,
                    'val_metrics': val_metrics_epoch,
                    'model_type': self.model_type
                }, f'./checkpoints/best_{self.model_type}_concentration.pt')
                
                print(f"  âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ (F1: {current_f1:.4f})")
            else:
                patience_counter += 1
            
            # Early Stopping
            if patience_counter >= patience:
                print(f"  â¹ï¸ Early stopping (patience: {patience})")
                break
        
        # í•™ìŠµ ê³¡ì„  ì €ì¥
        self.plot_training_curves(train_losses, val_metrics)
        
        print(f"\nâœ… í•™ìŠµ ì™„ë£Œ!")
        print(f"ìµœê³  F1 ì ìˆ˜: {best_f1:.4f}")
        
        return best_f1
    
    def plot_training_curves(self, train_losses, val_metrics):
        """í•™ìŠµ ê³¡ì„  ì‹œê°í™”"""
        epochs = range(1, len(train_losses) + 1)
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Loss
        axes[0, 0].plot(epochs, train_losses, 'b-', label='Train Loss')
        val_losses = [m['loss'] for m in val_metrics]
        axes[0, 0].plot(epochs, val_losses, 'r-', label='Val Loss')
        axes[0, 0].set_title('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # Accuracy
        val_accs = [m['accuracy'] for m in val_metrics]
        axes[0, 1].plot(epochs, val_accs, 'g-', label='Val Accuracy')
        axes[0, 1].set_title('Accuracy')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # F1 Score
        val_f1s = [m['f1'] for m in val_metrics]
        axes[1, 0].plot(epochs, val_f1s, 'm-', label='Val F1')
        axes[1, 0].set_title('F1 Score')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
        
        # AUC
        val_aucs = [m['auc'] for m in val_metrics]
        axes[1, 1].plot(epochs, val_aucs, 'c-', label='Val AUC')
        axes[1, 1].set_title('AUC')
        axes[1, 1].legend()
        axes[1, 1].grid(True)
        
        plt.tight_layout()
        plt.savefig(f'./checkpoints/{self.model_type}_training_curves.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“Š í•™ìŠµ ê³¡ì„  ì €ì¥: ./checkpoints/{self.model_type}_training_curves.png")

def main():
    parser = argparse.ArgumentParser(description='ì§‘ì¤‘ë„ PyTorch ëª¨ë¸ í•™ìŠµ')
    parser.add_argument('--model', type=str, default='lstm', 
                       choices=['lstm', 'transformer', 'cnn1d'],
                       help='ëª¨ë¸ íƒ€ì…')
    parser.add_argument('--data', type=str, default='./data/concentration_sequences.pt',
                       help='ë°ì´í„°ì…‹ ê²½ë¡œ')
    parser.add_argument('--lr', type=float, default=0.001, help='í•™ìŠµë¥ ')
    parser.add_argument('--batch_size', type=int, default=32, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--epochs', type=int, default=100, help='ì—í­ ìˆ˜')
    
    args = parser.parse_args()
    
    print("ğŸ§  PyTorch ì§‘ì¤‘ë„ ëª¨ë¸ í•™ìŠµ")
    print(f"ëª¨ë¸: {args.model}")
    
    # ë°ì´í„°ì…‹ í™•ì¸
    if not os.path.exists(args.data):
        print(f"âŒ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.data}")
        print("ë¨¼ì € convert_ml_to_pytorch.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
        return
    
    # í•™ìŠµê¸° ìƒì„±
    trainer = ConcentrationTrainer(
        model_type=args.model,
        lr=args.lr,
        batch_size=args.batch_size,
        epochs=args.epochs
    )
    
    # ë°ì´í„° ë¡œë“œ
    input_dim = trainer.load_dataset(args.data)
    
    # ëª¨ë¸ ìƒì„±
    trainer.create_model(input_dim)
    
    # í•™ìŠµ ì‹¤í–‰
    best_f1 = trainer.train()
    
    print(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ!")
    print(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸: ./checkpoints/best_{args.model}_concentration.pt")
    print(f"ìµœê³  F1 ì ìˆ˜: {best_f1:.4f}")

if __name__ == "__main__":
    main()
