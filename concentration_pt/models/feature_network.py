import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class FeatureEmbeddingNetwork(nn.Module):
    """31ì°¨ì› íŠ¹ì§•ì„ ì„ë² ë”©í•˜ëŠ” ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, input_dim=31, embed_dim=128, dropout=0.1):
        super(FeatureEmbeddingNetwork, self).__init__()
        
        # íŠ¹ì§•ë³„ ê°€ì¤‘ì¹˜ (26ì°¨ì› ê¸°ë³¸ íŠ¹ì§• + 5ì°¨ì› attention íŠ¹ì§•)
        self.basic_feature_weight = nn.Parameter(torch.ones(26))
        self.attention_feature_weight = nn.Parameter(torch.ones(5))
        
        # ì„ë² ë”© ë ˆì´ì–´ë“¤
        self.embedding_layers = nn.ModuleList([
            nn.Linear(input_dim, embed_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim * 2, embed_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim, embed_dim)
        ])
        
        # ì •ê·œí™”
        self.layer_norm = nn.LayerNorm(embed_dim)
        
    def forward(self, x):
        """
        Args:
            x: [batch_size, sequence_length, 31] ë˜ëŠ” [batch_size, 31]
        Returns:
            embedded_features: [batch_size, sequence_length, embed_dim] ë˜ëŠ” [batch_size, embed_dim]
        """
        # íŠ¹ì§• ê°€ì¤‘ì¹˜ ì ìš©
        if len(x.shape) == 3:  # ì‹œí€€ìŠ¤ ë°ì´í„°
            batch_size, seq_len, _ = x.shape
            weights = torch.cat([self.basic_feature_weight, self.attention_feature_weight])
            x = x * weights.unsqueeze(0).unsqueeze(0)  # [1, 1, 31]
        else:  # ë‹¨ì¼ í”„ë ˆì„
            weights = torch.cat([self.basic_feature_weight, self.attention_feature_weight])
            x = x * weights.unsqueeze(0)  # [1, 31]
        
        # ìˆœì°¨ì  ì„ë² ë”©
        for layer in self.embedding_layers:
            x = layer(x)
        
        # ì •ê·œí™”
        x = self.layer_norm(x)
        
        return x


class AdaptiveFeatureEncoder(nn.Module):
    """ì ì‘í˜• íŠ¹ì§• ì¸ì½”ë”"""
    
    def __init__(self, input_dim=31, hidden_dim=256, output_dim=128):
        super(AdaptiveFeatureEncoder, self).__init__()
        
        # íŠ¹ì§• íƒ€ì…ë³„ ì¸ì½”ë”
        self.pose_encoder = nn.Linear(3, hidden_dim // 4)      # [0:3] ë¨¸ë¦¬ í¬ì¦ˆ
        self.gaze_encoder = nn.Linear(2, hidden_dim // 4)      # [4:6] ì‹œì„ 
        self.eye_encoder = nn.Linear(6, hidden_dim // 4)       # [6:12] ëˆˆ ê´€ë ¨
        self.stability_encoder = nn.Linear(16, hidden_dim // 4) # [13:29] ì•ˆì •ì„± ë“±
        self.attention_encoder = nn.Linear(5, hidden_dim // 4)  # [26:31] attention íŠ¹ì§•
        
        # ìœµí•© ë„¤íŠ¸ì›Œí¬
        self.fusion_network = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim // 4, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, output_dim),
            nn.ReLU()
        )
        
    def forward(self, x):
        """íŠ¹ì§•ë³„ ì¸ì½”ë”© í›„ ìœµí•©"""
        # íŠ¹ì§• ë¶„í• 
        pose_feat = x[..., 0:3]         # ë¨¸ë¦¬ í¬ì¦ˆ
        gaze_feat = x[..., 4:6]         # ì‹œì„  (ê±°ë¦¬ ì œì™¸)
        eye_feat = x[..., 6:12]         # ëˆˆ ê´€ë ¨
        stability_feat = x[..., 12:26]  # ì•ˆì •ì„± ê´€ë ¨
        attention_feat = x[..., 26:31]  # attention íŠ¹ì§•
        
        # ê°ê° ì¸ì½”ë”©
        pose_encoded = self.pose_encoder(pose_feat)
        gaze_encoded = self.gaze_encoder(gaze_feat)
        eye_encoded = self.eye_encoder(eye_feat)
        stability_encoded = self.stability_encoder(stability_feat)
        attention_encoded = self.attention_encoder(attention_feat)
        
        # íŠ¹ì§• ìœµí•©
        fused = torch.cat([
            pose_encoded, gaze_encoded, eye_encoded, 
            stability_encoded, attention_encoded
        ], dim=-1)
        
        # ìµœì¢… ì¸ì½”ë”©
        output = self.fusion_network(fused)
        
        return output


class TemporalFeatureExtractor(nn.Module):
    """ì‹œê³„ì—´ íŠ¹ì§• ì¶”ì¶œê¸°"""
    
    def __init__(self, input_dim=31, hidden_dim=128, num_layers=2):
        super(TemporalFeatureExtractor, self).__init__()
        
        # 1D CNNìœ¼ë¡œ ì§€ì—­ì  íŒ¨í„´ ì¶”ì¶œ
        self.local_conv = nn.Sequential(
            nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1),
            nn.ReLU()
        )
        
        # LSTMìœ¼ë¡œ ì „ì—­ì  íŒ¨í„´ ì¶”ì¶œ
        self.global_lstm = nn.LSTM(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.2 if num_layers > 1 else 0
        )
        
        # ì‹œê°„ ê°€ì¤‘ì¹˜ (ìµœê·¼ í”„ë ˆì„ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
        self.temporal_weights = nn.Parameter(torch.linspace(0.1, 1.0, 30))
        
    def forward(self, x):
        """
        Args:
            x: [batch_size, sequence_length, input_dim] = [B, 30, 31]
        Returns:
            temporal_features: [batch_size, hidden_dim]
        """
        batch_size, seq_len, input_dim = x.shape
        
        # 1D CNN ì ìš© (ì°¨ì› ë³€í™˜ í•„ìš”)
        x_transposed = x.transpose(1, 2)  # [B, 31, 30]
        local_features = self.local_conv(x_transposed)  # [B, hidden_dim, 30]
        local_features = local_features.transpose(1, 2)  # [B, 30, hidden_dim]
        
        # LSTM ì ìš©
        lstm_out, _ = self.global_lstm(local_features)  # [B, 30, hidden_dim]
        
        # ì‹œê°„ ê°€ì¤‘ í‰ê· 
        weights = self.temporal_weights.unsqueeze(0).unsqueeze(-1)  # [1, 30, 1]
        weighted_features = lstm_out * weights
        temporal_features = torch.sum(weighted_features, dim=1)  # [B, hidden_dim]
        
        return temporal_features


class MultiScaleFeatureExtractor(nn.Module):
    """ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§• ì¶”ì¶œê¸°"""
    
    def __init__(self, input_dim=31, base_dim=64):
        super(MultiScaleFeatureExtractor, self).__init__()
        
        # ë‹¤ì–‘í•œ ì»¤ë„ í¬ê¸°ì˜ 1D CNN
        self.conv_3 = nn.Conv1d(input_dim, base_dim, kernel_size=3, padding=1)
        self.conv_5 = nn.Conv1d(input_dim, base_dim, kernel_size=5, padding=2)
        self.conv_7 = nn.Conv1d(input_dim, base_dim, kernel_size=7, padding=3)
        
        # ê¸€ë¡œë²Œ í’€ë§
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        
        # íŠ¹ì§• ìœµí•©
        self.fusion = nn.Sequential(
            nn.Linear(base_dim * 3, base_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(base_dim * 2, base_dim),
            nn.ReLU()
        )
        
    def forward(self, x):
        """ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§• ì¶”ì¶œ"""
        # x: [B, 30, 31] -> [B, 31, 30]
        x = x.transpose(1, 2)
        
        # ê°ê¸° ë‹¤ë¥¸ ìŠ¤ì¼€ì¼ ì ìš©
        feat_3 = F.relu(self.conv_3(x))  # [B, base_dim, 30]
        feat_5 = F.relu(self.conv_5(x))  # [B, base_dim, 30]
        feat_7 = F.relu(self.conv_7(x))  # [B, base_dim, 30]
        
        # ê¸€ë¡œë²Œ í’€ë§
        feat_3 = self.global_pool(feat_3).squeeze(-1)  # [B, base_dim]
        feat_5 = self.global_pool(feat_5).squeeze(-1)  # [B, base_dim]
        feat_7 = self.global_pool(feat_7).squeeze(-1)  # [B, base_dim]
        
        # ìœµí•©
        fused = torch.cat([feat_3, feat_5, feat_7], dim=-1)  # [B, base_dim*3]
        output = self.fusion(fused)  # [B, base_dim]
        
        return output


class AttentionFeatureProcessor(nn.Module):
    """Attention íŠ¹ì§• ì „ìš© ì²˜ë¦¬ê¸°"""
    
    def __init__(self, attention_dim=5, output_dim=32):
        super(AttentionFeatureProcessor, self).__init__()
        
        # attention íŠ¹ì§•ë³„ ê°€ì¤‘ì¹˜
        self.attention_weights = nn.Parameter(torch.tensor([
            1.2,  # central_focus (ì¤‘ìš”)
            1.1,  # gaze_fixation (ì¤‘ìš”) 
            0.9,  # head_stability
            0.8,  # face_orientation
            1.3   # attention_score (ê°€ì¥ ì¤‘ìš”)
        ]))
        
        # ì²˜ë¦¬ ë„¤íŠ¸ì›Œí¬
        self.processor = nn.Sequential(
            nn.Linear(attention_dim, output_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(output_dim * 2, output_dim),
            nn.Sigmoid()  # 0~1 ë²”ìœ„ ìœ ì§€
        )
        
    def forward(self, attention_features):
        """
        Args:
            attention_features: [..., 5] attention íŠ¹ì§•
        Returns:
            processed: [..., output_dim] ì²˜ë¦¬ëœ íŠ¹ì§•
        """
        # ê°€ì¤‘ì¹˜ ì ìš©
        weighted = attention_features * self.attention_weights
        
        # ì²˜ë¦¬
        processed = self.processor(weighted)
        
        return processed


# í†µí•© íŠ¹ì§• ë„¤íŠ¸ì›Œí¬
class ComprehensiveFeatureNetwork(nn.Module):
    """í†µí•© íŠ¹ì§• ì²˜ë¦¬ ë„¤íŠ¸ì›Œí¬"""
    
    def __init__(self, input_dim=31, output_dim=256):
        super(ComprehensiveFeatureNetwork, self).__init__()
        
        # ì„œë¸Œ ë„¤íŠ¸ì›Œí¬ë“¤
        self.feature_embedding = FeatureEmbeddingNetwork(input_dim, 64)
        self.adaptive_encoder = AdaptiveFeatureEncoder(input_dim, 128, 64)
        self.attention_processor = AttentionFeatureProcessor(5, 32)
        self.multiscale_extractor = MultiScaleFeatureExtractor(input_dim, 64)
        
        # ìµœì¢… ìœµí•©
        self.final_fusion = nn.Sequential(
            nn.Linear(64 + 64 + 32 + 64, output_dim),  # ì´ 224ì°¨ì› ì…ë ¥
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(output_dim, output_dim),
            nn.ReLU()
        )
        
    def forward(self, x):
        """ì¢…í•©ì  íŠ¹ì§• ì²˜ë¦¬"""
        # ê° ì„œë¸Œë„¤íŠ¸ì›Œí¬ ì ìš©
        embedded = self.feature_embedding(x)  # [64]
        adaptive = self.adaptive_encoder(x)   # [64]
        
        # attention íŠ¹ì§•ë§Œ ë¶„ë¦¬ ì²˜ë¦¬
        attention_feat = x[..., 26:31]
        attention_processed = self.attention_processor(attention_feat)  # [32]
        
        # ì‹œí€€ìŠ¤ê°€ ìˆëŠ” ê²½ìš°ë§Œ multiscale ì ìš©
        if len(x.shape) == 3:  # [B, seq, dim]
            multiscale = self.multiscale_extractor(x)  # [64]
        else:
            # ë‹¨ì¼ í”„ë ˆì„ì¸ ê²½ìš° ë”ë¯¸ í…ì„œ
            multiscale = torch.zeros(x.shape[0], 64, device=x.device)
        
        # ëª¨ë“  íŠ¹ì§• ìœµí•©
        fused = torch.cat([embedded, adaptive, attention_processed, multiscale], dim=-1)
        output = self.final_fusion(fused)
        
        return output


def test_feature_networks():
    """íŠ¹ì§• ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª íŠ¹ì§• ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    batch_size = 4
    seq_len = 30
    input_dim = 31
    
    # ì‹œí€€ìŠ¤ ë°ì´í„°
    sequence_data = torch.randn(batch_size, seq_len, input_dim)
    
    # ë‹¨ì¼ í”„ë ˆì„ ë°ì´í„°
    single_frame = torch.randn(batch_size, input_dim)
    
    # 1. íŠ¹ì§• ì„ë² ë”© í…ŒìŠ¤íŠ¸
    embedding_net = FeatureEmbeddingNetwork()
    embedded_seq = embedding_net(sequence_data)
    embedded_single = embedding_net(single_frame)
    
    print(f"âœ… íŠ¹ì§• ì„ë² ë”©: {sequence_data.shape} -> {embedded_seq.shape}")
    print(f"âœ… ë‹¨ì¼ ì„ë² ë”©: {single_frame.shape} -> {embedded_single.shape}")
    
    # 2. ì ì‘í˜• ì¸ì½”ë” í…ŒìŠ¤íŠ¸
    adaptive_net = AdaptiveFeatureEncoder()
    adaptive_out = adaptive_net(sequence_data)
    print(f"âœ… ì ì‘í˜• ì¸ì½”ë”: {sequence_data.shape} -> {adaptive_out.shape}")
    
    # 3. ì‹œê³„ì—´ ì¶”ì¶œê¸° í…ŒìŠ¤íŠ¸
    temporal_net = TemporalFeatureExtractor()
    temporal_out = temporal_net(sequence_data)
    print(f"âœ… ì‹œê³„ì—´ ì¶”ì¶œ: {sequence_data.shape} -> {temporal_out.shape}")
    
    # 4. í†µí•© ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸
    comprehensive_net = ComprehensiveFeatureNetwork()
    comprehensive_out = comprehensive_net(sequence_data)
    print(f"âœ… í†µí•© ë„¤íŠ¸ì›Œí¬: {sequence_data.shape} -> {comprehensive_out.shape}")
    
    print("ğŸ‰ ëª¨ë“  íŠ¹ì§• ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    test_feature_networks()
