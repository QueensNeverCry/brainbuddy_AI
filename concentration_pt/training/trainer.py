import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
import time
import os
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

class ConcentrationModelTrainer:
    """ì§‘ì¤‘ë„ ëª¨ë¸ ì „ë¬¸ í•™ìŠµê¸°"""
    
    def __init__(self, model: nn.Module, device: str = 'auto'):
        self.model = model
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device == 'auto' else torch.device(device)
        self.model.to(self.device)
        
        # í•™ìŠµ ê¸°ë¡
        self.train_losses = []
        self.val_losses = []
        self.val_metrics = []
        self.learning_rates = []
        
        # Early stopping
        self.best_val_score = 0.0
        self.patience_counter = 0
        
        print(f"ğŸ¯ í•™ìŠµê¸° ì´ˆê¸°í™” ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {self.device})")
    
    def setup_training(self, learning_rate: float = 0.001, weight_decay: float = 1e-4, 
                      scheduler_type: str = 'cosine'):
        """í•™ìŠµ ì„¤ì •"""
        # ì†ì‹¤í•¨ìˆ˜
        self.criterion = nn.BCELoss()
        
        # ì˜µí‹°ë§ˆì´ì €
        self.optimizer = optim.AdamW(
            self.model.parameters(), 
            lr=learning_rate, 
            weight_decay=weight_decay
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬
        if scheduler_type == 'cosine':
            self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
                self.optimizer, T_0=10, T_mult=2
            )
        elif scheduler_type == 'step':
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer, step_size=20, gamma=0.5
            )
        elif scheduler_type == 'plateau':
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer, mode='max', factor=0.5, patience=5
            )
        else:
            self.scheduler = None
        
        print(f"âš™ï¸ í•™ìŠµ ì„¤ì • ì™„ë£Œ (LR: {learning_rate}, Scheduler: {scheduler_type})")
    
    def train_epoch(self, train_loader: DataLoader, epoch: int) -> float:
        """í•œ ì—í­ í•™ìŠµ"""
        self.model.train()
        total_loss = 0.0
        num_batches = len(train_loader)
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1} [Train]")
        
        for batch_idx, (sequences, labels) in enumerate(progress_bar):
            sequences = sequences.to(self.device)
            labels = labels.to(self.device)
            
            # Forward pass
            self.optimizer.zero_grad()
            outputs = self.model(sequences)
            loss = self.criterion(outputs, labels)
            
            # Backward pass
            loss.backward()
            
            # Gradient clipping (ì„ íƒì )
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # ì†ì‹¤ ê¸°ë¡
            batch_loss = loss.item()
            total_loss += batch_loss
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            progress_bar.set_postfix({
                'loss': f'{batch_loss:.4f}',
                'avg_loss': f'{total_loss/(batch_idx+1):.4f}'
            })
        
        avg_loss = total_loss / num_batches
        return avg_loss
    
    def validate_epoch(self, val_loader: DataLoader, epoch: int) -> Dict[str, float]:
        """í•œ ì—í­ ê²€ì¦"""
        self.model.eval()
        total_loss = 0.0
        all_predictions = []
        all_probabilities = []
        all_labels = []
        
        progress_bar = tqdm(val_loader, desc=f"Epoch {epoch+1} [Val]")
        
        with torch.no_grad():
            for sequences, labels in progress_bar:
                sequences = sequences.to(self.device)
                labels = labels.to(self.device)
                
                outputs = self.model(sequences)
                loss = self.criterion(outputs, labels)
                
                total_loss += loss.item()
                
                # ì˜ˆì¸¡ ê²°ê³¼ ìˆ˜ì§‘
                probabilities = outputs.cpu().numpy()
                predictions = (probabilities > 0.5).astype(int)
                labels_np = labels.cpu().numpy()
                
                all_probabilities.extend(probabilities.flatten())
                all_predictions.extend(predictions.flatten())
                all_labels.extend(labels_np.flatten())
                
                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
        
        metrics = {
            'loss': total_loss / len(val_loader),
            'accuracy': accuracy_score(all_labels, all_predictions),
            'precision': precision_score(all_labels, all_predictions, zero_division=0),
            'recall': recall_score(all_labels, all_predictions, zero_division=0),
            'f1': f1_score(all_labels, all_predictions, zero_division=0),
            'auc': roc_auc_score(all_labels, all_probabilities) if len(set(all_labels)) > 1 else 0.5
        }
        
        return metrics
    
    def train(self, train_loader: DataLoader, val_loader: DataLoader, 
              epochs: int = 100, patience: int = 15, save_dir: str = './checkpoints') -> Dict:
        """ì „ì²´ í•™ìŠµ ê³¼ì •"""
        
        print(f"ğŸš€ í•™ìŠµ ì‹œì‘ (ì—í­: {epochs}, ì¸ë‚´ì‹¬: {patience})")
        print(f"ğŸ“Š í•™ìŠµ ë°ì´í„°: {len(train_loader.dataset)}ê°œ")
        print(f"ğŸ“Š ê²€ì¦ ë°ì´í„°: {len(val_loader.dataset)}ê°œ")
        
        os.makedirs(save_dir, exist_ok=True)
        
        start_time = time.time()
        
        for epoch in range(epochs):
            epoch_start = time.time()
            
            # í•™ìŠµ
            train_loss = self.train_epoch(train_loader, epoch)
            self.train_losses.append(train_loss)
            
            # ê²€ì¦
            val_metrics = self.validate_epoch(val_loader, epoch)
            self.val_losses.append(val_metrics['loss'])
            self.val_metrics.append(val_metrics)
            
            # í•™ìŠµë¥  ê¸°ë¡
            current_lr = self.optimizer.param_groups[0]['lr']
            self.learning_rates.append(current_lr)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            if self.scheduler is not None:
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_metrics['f1'])
                else:
                    self.scheduler.step()
            
            # ì—í­ ì‹œê°„ ê³„ì‚°
            epoch_time = time.time() - epoch_start
            
            # ë¡œê·¸ ì¶œë ¥
            print(f"\n{'='*60}")
            print(f"Epoch {epoch+1}/{epochs} ({epoch_time:.1f}s)")
            print(f"{'='*60}")
            print(f"Train Loss: {train_loss:.4f}")
            print(f"Val Loss:   {val_metrics['loss']:.4f}")
            print(f"Val Acc:    {val_metrics['accuracy']:.4f}")
            print(f"Val Prec:   {val_metrics['precision']:.4f}")
            print(f"Val Rec:    {val_metrics['recall']:.4f}")
            print(f"Val F1:     {val_metrics['f1']:.4f}")
            print(f"Val AUC:    {val_metrics['auc']:.4f}")
            print(f"Learning Rate: {current_lr:.6f}")
            
            # ëª¨ë¸ ì €ì¥ (F1 score ê¸°ì¤€)
            current_score = val_metrics['f1']
            if current_score > self.best_val_score:
                self.best_val_score = current_score
                self.patience_counter = 0
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                model_name = self.model.__class__.__name__.lower()
                save_path = os.path.join(save_dir, f'best_{model_name}_concentration.pt')
                
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
                    'val_f1': current_score,
                    'val_metrics': val_metrics,
                    'train_losses': self.train_losses,
                    'val_losses': self.val_losses,
                    'val_metrics': self.val_metrics,
                    'model_type': model_name
                }, save_path)
                
                print(f"âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {save_path}")
                print(f"ğŸ† ìµœê³  F1 ì ìˆ˜: {current_score:.4f}")
            else:
                self.patience_counter += 1
                print(f"â³ Early stopping ì¹´ìš´í„°: {self.patience_counter}/{patience}")
            
            # Early stopping í™•ì¸
            if self.patience_counter >= patience:
                print(f"â¹ï¸ Early stopping ë°œë™ (patience: {patience})")
                break
            
            print(f"{'='*60}\n")
        
        # í•™ìŠµ ì™„ë£Œ
        total_time = time.time() - start_time
        print(f"ğŸ‰ í•™ìŠµ ì™„ë£Œ! (ì´ ì‹œê°„: {total_time/60:.1f}ë¶„)")
        print(f"ğŸ† ìµœê³  F1 ì ìˆ˜: {self.best_val_score:.4f}")
        
        return {
            'best_f1': self.best_val_score,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'val_metrics': self.val_metrics,
            'total_time': total_time
        }
    
    def plot_training_history(self, save_path: str = None):
        """í•™ìŠµ ê³¼ì • ì‹œê°í™”"""
        if not self.val_metrics:
            print("âŒ í•™ìŠµ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        epochs = range(1, len(self.train_losses) + 1)
        
        # Loss ê³¡ì„ 
        axes[0, 0].plot(epochs, self.train_losses, 'b-', label='Train Loss', linewidth=2)
        axes[0, 0].plot(epochs, self.val_losses, 'r-', label='Val Loss', linewidth=2)
        axes[0, 0].set_title('Loss Curves', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # Accuracy
        val_accuracies = [m['accuracy'] for m in self.val_metrics]
        axes[0, 1].plot(epochs, val_accuracies, 'g-', linewidth=2)
        axes[0, 1].set_title('Validation Accuracy', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Accuracy')
        axes[0, 1].grid(True, alpha=0.3)
        
        # F1 Score
        val_f1s = [m['f1'] for m in self.val_metrics]
        axes[0, 2].plot(epochs, val_f1s, 'm-', linewidth=2)
        axes[0, 2].set_title('Validation F1 Score', fontsize=14, fontweight='bold')
        axes[0, 2].set_xlabel('Epoch')
        axes[0, 2].set_ylabel('F1 Score')
        axes[0, 2].grid(True, alpha=0.3)
        
        # Precision & Recall
        val_precisions = [m['precision'] for m in self.val_metrics]
        val_recalls = [m['recall'] for m in self.val_metrics]
        axes[1, 0].plot(epochs, val_precisions, 'c-', label='Precision', linewidth=2)
        axes[1, 0].plot(epochs, val_recalls, 'y-', label='Recall', linewidth=2)
        axes[1, 0].set_title('Precision & Recall', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Score')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # AUC
        val_aucs = [m['auc'] for m in self.val_metrics]
        axes[1, 1].plot(epochs, val_aucs, 'orange', linewidth=2)
        axes[1, 1].set_title('Validation AUC', fontsize=14, fontweight='bold')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('AUC')
        axes[1, 1].grid(True, alpha=0.3)
        
        # Learning Rate
        axes[1, 2].plot(epochs, self.learning_rates, 'purple', linewidth=2)
        axes[1, 2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')
        axes[1, 2].set_xlabel('Epoch')
        axes[1, 2].set_ylabel('Learning Rate')
        axes[1, 2].set_yscale('log')
        axes[1, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š í•™ìŠµ ê³¡ì„  ì €ì¥: {save_path}")
        
        plt.show()
    
    def evaluate_model(self, test_loader: DataLoader, save_dir: str = './results'):
        """ëª¨ë¸ ìµœì¢… í‰ê°€"""
        self.model.eval()
        all_predictions = []
        all_probabilities = []
        all_labels = []
        
        print("ğŸ” ìµœì¢… ëª¨ë¸ í‰ê°€ ì¤‘...")
        
        with torch.no_grad():
            for sequences, labels in tqdm(test_loader, desc="Testing"):
                sequences = sequences.to(self.device)
                outputs = self.model(sequences)
                
                probabilities = outputs.cpu().numpy()
                predictions = (probabilities > 0.5).astype(int)
                labels_np = labels.cpu().numpy()
                
                all_probabilities.extend(probabilities.flatten())
                all_predictions.extend(predictions.flatten())
                all_labels.extend(labels_np.flatten())
        
        # í‰ê°€ ë©”íŠ¸ë¦­
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
        
        test_metrics = {
            'accuracy': accuracy_score(all_labels, all_predictions),
            'precision': precision_score(all_labels, all_predictions, zero_division=0),
            'recall': recall_score(all_labels, all_predictions, zero_division=0),
            'f1': f1_score(all_labels, all_predictions, zero_division=0),
            'auc': roc_auc_score(all_labels, all_probabilities)
        }
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\n{'='*50}")
        print(f"ğŸ“Š ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        print(f"{'='*50}")
        for metric, value in test_metrics.items():
            print(f"{metric.upper():>10}: {value:.4f}")
        print(f"{'='*50}")
        
        # Confusion Matrix ì‹œê°í™”
        os.makedirs(save_dir, exist_ok=True)
        self._plot_confusion_matrix(all_labels, all_predictions, 
                                   os.path.join(save_dir, 'confusion_matrix.png'))
        
        # Classification Report
        report = classification_report(all_labels, all_predictions, 
                                     target_names=['Not Focused', 'Focused'])
        print(f"\nğŸ“‹ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n{report}")
        
        return test_metrics
    
    def _plot_confusion_matrix(self, y_true, y_pred, save_path: str):
        """í˜¼ë™ í–‰ë ¬ ì‹œê°í™”"""
        cm = confusion_matrix(y_true, y_pred)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['Not Focused', 'Focused'],
                   yticklabels=['Not Focused', 'Focused'])
        plt.title('Confusion Matrix', fontsize=16, fontweight='bold')
        plt.xlabel('Predicted', fontsize=12)
        plt.ylabel('Actual', fontsize=12)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“Š í˜¼ë™ í–‰ë ¬ ì €ì¥: {save_path}")
