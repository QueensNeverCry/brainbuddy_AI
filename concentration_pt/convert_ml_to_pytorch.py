import torch
import torch.nn as nn
import numpy as np
import pickle
import cv2
import os
from collections import deque
import math
from tqdm import tqdm
import sys
sys.path.append('.')

from utils.face_detector import FaceDetector
from utils.attention_features import AttentionFeatureExtractor

class MLToPyTorchConverter:
    """ML ëª¨ë¸ì„ PyTorchë¡œ ë³€í™˜í•˜ê³  ë°ì´í„° ìƒì„±"""
    
    def __init__(self, ml_model_path):
        """
        Args:
            ml_model_path: ê¸°ì¡´ .pkl ëª¨ë¸ ê²½ë¡œ
        """
        self.ml_model_path = ml_model_path
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ê¸°ì¡´ ML ëª¨ë¸ ë¡œë“œ
        self.load_ml_model()
        
        # ì–¼êµ´ ê²€ì¶œ ë° íŠ¹ì§• ì¶”ì¶œê¸°
        self.face_detector = FaceDetector()
        self.feature_extractor = AttentionFeatureExtractor()
        
        print(f"âœ… MLâ†’PyTorch ë³€í™˜ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"ë””ë°”ì´ìŠ¤: {self.device}")
    
    def load_ml_model(self):
        """ê¸°ì¡´ ML ëª¨ë¸ ë¡œë“œ (ë‹¤ì–‘í•œ ë°©ë²• ì‹œë„)"""
        try:
            # ë°©ë²• 1: joblibë¡œ ë¡œë“œ (XGBoost ëª¨ë¸ì— ì¼ë°˜ì )
            import joblib
            loaded_data = joblib.load(self.ml_model_path)
            
            # ë¡œë“œëœ ë°ì´í„°ê°€ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
            if isinstance(loaded_data, dict):
                print("âš ï¸ ëª¨ë¸ì´ ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸ ê°ì²´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.")
                
                # ë”•ì…”ë„ˆë¦¬ì—ì„œ ëª¨ë¸ ê°ì²´ ì°¾ê¸°
                if 'model' in loaded_data:
                    self.ml_model = loaded_data['model']
                elif 'estimator' in loaded_data:
                    self.ml_model = loaded_data['estimator']
                elif 'clf' in loaded_data:
                    self.ml_model = loaded_data['clf']
                elif 'xgb_model' in loaded_data:
                    self.ml_model = loaded_data['xgb_model']
                else:
                    # ë”•ì…”ë„ˆë¦¬ì˜ ì²« ë²ˆì§¸ ê°’ì´ ëª¨ë¸ì¸ì§€ í™•ì¸
                    for key, value in loaded_data.items():
                        if hasattr(value, 'predict') and hasattr(value, 'predict_proba'):
                            self.ml_model = value
                            print(f"ğŸ“‚ ë”•ì…”ë„ˆë¦¬ì—ì„œ ëª¨ë¸ ê°ì²´ ë°œê²¬: {key}")
                            break
                    else:
                        raise ValueError("ë”•ì…”ë„ˆë¦¬ì—ì„œ ìœ íš¨í•œ ëª¨ë¸ ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            else:
                # ì§ì ‘ ëª¨ë¸ ê°ì²´ì¸ ê²½ìš°
                self.ml_model = loaded_data
            
            print(f"ğŸ“‚ ML ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {type(self.ml_model)}")
            
            # ëª¨ë¸ì´ predictì™€ predict_proba ë©”ì„œë“œë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸
            if not (hasattr(self.ml_model, 'predict') and hasattr(self.ml_model, 'predict_proba')):
                raise ValueError(f"ë¡œë“œëœ ê°ì²´ì— predict ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤: {type(self.ml_model)}")
            
            print(f"âœ… ëª¨ë¸ ê²€ì¦ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ëª¨ë“  ë¡œë“œ ë°©ë²• ì‹¤íŒ¨: {e}")
            print("ğŸ”„ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤")
            self.ml_model = None  # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ì „í™˜

    
    def simulate_30_frame_sequence(self, num_sequences=5000):
        """
        30í”„ë ˆì„ ì‹œí€€ìŠ¤ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜ ìƒì„±
        
        Args:
            num_sequences: ìƒì„±í•  ì‹œí€€ìŠ¤ ìˆ˜
            
        Returns:
            sequences: 30í”„ë ˆì„ ì‹œí€€ìŠ¤ ë¦¬ìŠ¤íŠ¸
            labels: ê° ì‹œí€€ìŠ¤ì˜ ë¼ë²¨ (0: ë¹„ì§‘ì¤‘, 1: ì§‘ì¤‘)
        """
        print(f"ğŸ”„ {num_sequences}ê°œ 30í”„ë ˆì„ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± ì¤‘...")
        
        sequences = []
        labels = []
        
        # ê°€ìƒ ì¹´ë©”ë¼ ì„¤ì •
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print("âš ï¸ ì›¹ìº ì´ ì—†ì–´ì„œ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¡œ ëŒ€ì²´")
            return self._generate_simulated_data(num_sequences)
        
        frame_count = 0
        sequence_features = []
        
        with tqdm(total=num_sequences, desc="ì‹œí€€ìŠ¤ ìƒì„±") as pbar:
            while len(sequences) < num_sequences:
                ret, frame = cap.read()
                if not ret:
                    print("âš ï¸ ì›¹ìº  ì½ê¸° ì‹¤íŒ¨, ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¡œ ëŒ€ì²´")
                    cap.release()
                    return self._generate_simulated_data(num_sequences)
                
                frame_count += 1
                
                try:
                    # ì–¼êµ´ ê²€ì¶œ ë° íŠ¹ì§• ì¶”ì¶œ
                    face_box = self.face_detector.detect_face(frame)
                    features, attention_features = self.feature_extractor.extract_features(frame, face_box)
                    
                    # ML ëª¨ë¸ë¡œ ì˜ˆì¸¡ (3í´ë˜ìŠ¤) - ìˆ˜ì •ëœ ë¶€ë¶„
                    if self.ml_model is not None:
                        try:
                            # XGBoostëŠ” predict()ì™€ predict_proba()ë¥¼ ë”°ë¡œ í˜¸ì¶œ
                            ml_pred = self.ml_model.predict(features.reshape(1, -1))
                            ml_probs = self.ml_model.predict_proba(features.reshape(1, -1))
                            
                            # predictëŠ” ë°°ì—´ì„ ë°˜í™˜í•˜ë¯€ë¡œ ì²« ë²ˆì§¸ ìš”ì†Œ ì¶”ì¶œ
                            ml_pred = ml_pred[0] if isinstance(ml_pred, (list, np.ndarray)) else ml_pred
                            ml_probs = ml_probs[0] if len(ml_probs.shape) > 1 else ml_probs
                            
                        except Exception as e:
                            print(f"âš ï¸ ML ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                            # ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´
                            ml_pred = 1  
                            ml_probs = np.array([0.4, 0.3, 0.3])
                    else:
                        # ML ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’
                        ml_pred = 1
                        ml_probs = np.array([0.4, 0.3, 0.3])
                    
                    # ë…¼ë¬¸ ê¸°ë°˜ ë³´ì • ì ìš© (ê¸°ì¡´ inference ë¡œì§)
                    binary_result, confidence = self._apply_inference_correction(
                        ml_pred, ml_probs, attention_features
                    )
                    
                    # íŠ¹ì§• + ë¼ë²¨ ì €ì¥
                    frame_data = {
                        'features': features,
                        'attention_features': attention_features,
                        'face_detected': face_box is not None,
                        'label': binary_result
                    }
                    sequence_features.append(frame_data)
                    
                    # 30í”„ë ˆì„ì´ ëª¨ì´ë©´ ì‹œí€€ìŠ¤ ì™„ì„±
                    if len(sequence_features) == 30:
                        # ì‹œí€€ìŠ¤ì˜ ìµœì¢… ë¼ë²¨ì€ ë§ˆì§€ë§‰ 10í”„ë ˆì„ì˜ ë‹¤ìˆ˜ê²°
                        recent_labels = [f['label'] for f in sequence_features[-10:]]
                        final_label = 1 if sum(recent_labels) >= 5 else 0
                        
                        sequences.append(sequence_features.copy())
                        labels.append(final_label)
                        
                        # ë²„í¼ ì´ˆê¸°í™” (ë‹¤ìŒ ì‹œí€€ìŠ¤ ì¤€ë¹„)
                        sequence_features = []
                        pbar.update(1)
                        
                except Exception as e:
                    print(f"âš ï¸ í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ í”„ë ˆì„ ê±´ë„ˆë›°ê¸°
                    continue
                    
                # ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ë©´ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ì „í™˜
                if frame_count > num_sequences * 50:  # ë„ˆë¬´ ë§ì€ í”„ë ˆì„ ì²˜ë¦¬ ì‹œ
                    print("â° ì›¹ìº  ì²˜ë¦¬ ì‹œê°„ ì´ˆê³¼, ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¡œ ëŒ€ì²´")
                    cap.release()
                    return self._generate_simulated_data(num_sequences)
        
        cap.release()
        
        print(f"âœ… ì´ {len(sequences)}ê°œ ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ")
        focus_ratio = sum(labels) / len(labels) * 100 if labels else 0
        print(f"ğŸ“Š ì§‘ì¤‘ ë¹„ìœ¨: {focus_ratio:.1f}%")
        
        return sequences, labels

    
    def _apply_inference_correction(self, ml_pred, ml_probs, attention_features):
        """ê¸°ì¡´ inference.pyì˜ ë³´ì • ë¡œì§ ì ìš©"""
        # ê¸°ì¡´ focus_sensitive_prediction ë¡œì§ ì¬í˜„
        focus_indicators = 0
        
        # ML ëª¨ë¸ ì˜ˆì¸¡ ê°€ì¤‘ì¹˜
        if ml_pred == 2:  # ì§‘ì¤‘
            focus_indicators += 4
        elif ml_pred == 0:  # ë¹„ì§‘ì¤‘
            focus_indicators -= 4
        elif ml_probs[2] > 0.25:
            focus_indicators += 2
        
        # ëª¨ë¸ í™•ì‹ ë„ ë³´ì •
        if ml_probs[0] > 0.8:  # ë¹„ì§‘ì¤‘ í™•ë¥  80% ì´ìƒ
            focus_indicators -= 3
        elif ml_probs[2] > 0.8:  # ì§‘ì¤‘ í™•ë¥  80% ì´ìƒ
            focus_indicators += 3
        
        # ë…¼ë¬¸ ê¸°ë°˜ ì§€í‘œë“¤
        if attention_features['central_focus'] > 0.7:
            focus_indicators += 2
        if attention_features['gaze_fixation'] > 0.7:
            focus_indicators += 2
        if attention_features['attention_score'] > 0.5:
            focus_indicators += 2
        if attention_features['head_stability'] > 0.4:
            focus_indicators += 1
        if attention_features['face_orientation'] > 0.3:
            focus_indicators += 1
        
        # ìµœì¢… íŒì •
        if focus_indicators >= 3:
            binary_result = 1
            confidence = min(0.95, 0.6 + attention_features['attention_score'] * 0.2 + ml_probs[2] * 0.15)
        else:
            binary_result = 0
            confidence = max(0.5, (ml_probs[0] + ml_probs[1]) / 2)
        
        return binary_result, confidence
    
    def _generate_simulated_data(self, num_sequences):
        """ì›¹ìº ì´ ì—†ì„ ë•Œ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±"""
        print("ğŸ”§ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„± ì¤‘...")
        
        sequences = []
        labels = []
        
        np.random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ë°ì´í„°
        
        for i in tqdm(range(num_sequences), desc="ì‹œë®¬ë ˆì´ì…˜ ì‹œí€€ìŠ¤ ìƒì„±"):
            sequence_features = []
            
            # ì‹œí€€ìŠ¤ íƒ€ì… ê²°ì • (ì§‘ì¤‘ vs ë¹„ì§‘ì¤‘)
            is_focused_sequence = np.random.choice([0, 1], p=[0.4, 0.6])  # 60% ì§‘ì¤‘ ë°ì´í„°
            
            for frame_idx in range(30):
                # ê¸°ë³¸ íŠ¹ì§• ë²¡í„° (26ì°¨ì›)
                if is_focused_sequence:
                    # ì§‘ì¤‘ ì‹œí€€ìŠ¤: ì•ˆì •ì ì¸ íŠ¹ì§•
                    features = self._generate_focused_features()
                    attention_features = {
                        'central_focus': np.random.uniform(0.6, 1.0),
                        'gaze_fixation': np.random.uniform(0.7, 1.0),
                        'head_stability': np.random.uniform(0.6, 0.9),
                        'face_orientation': np.random.uniform(0.5, 1.0),
                        'attention_score': np.random.uniform(0.6, 0.9)
                    }
                else:
                    # ë¹„ì§‘ì¤‘ ì‹œí€€ìŠ¤: ë¶ˆì•ˆì •í•œ íŠ¹ì§•
                    features = self._generate_unfocused_features()
                    attention_features = {
                        'central_focus': np.random.uniform(0.0, 0.4),
                        'gaze_fixation': np.random.uniform(0.0, 0.5),
                        'head_stability': np.random.uniform(0.2, 0.6),
                        'face_orientation': np.random.uniform(0.0, 0.5),
                        'attention_score': np.random.uniform(0.1, 0.5)
                    }
                
                frame_data = {
                    'features': features,
                    'attention_features': attention_features,
                    'face_detected': True,
                    'label': is_focused_sequence
                }
                sequence_features.append(frame_data)
            
            sequences.append(sequence_features)
            labels.append(is_focused_sequence)
        
        return sequences, labels
    
    def _generate_focused_features(self):
        """ì§‘ì¤‘ ìƒíƒœ íŠ¹ì§• ìƒì„±"""
        features = np.zeros(26, dtype=np.float32)
        
        # ì•ˆì •ì ì¸ ë¨¸ë¦¬ í¬ì¦ˆ
        features[0:3] = np.random.normal([0.0, 0.0, 0.0], [0.5, 0.5, 0.5])
        
        # ì¤‘ì•™ ì‹œì„ 
        features[4] = np.random.normal(640, 50)  # í™”ë©´ ì¤‘ì•™ X
        features[5] = np.random.normal(360, 30)  # í™”ë©´ ì¤‘ì•™ Y
        
        # ë‚®ì€ ë³€ë™ì„±
        features[13:15] = np.random.uniform([0.3, 0.3], [1.0, 1.0])
        
        # ë†’ì€ ì•ˆì •ì„±
        features[15] = np.random.uniform(0.8, 0.95)
        features[16] = np.random.uniform(0.8, 0.9)
        
        # ë‚®ì€ ë–¨ë¦¼
        features[17] = np.random.uniform(3.0, 10.0)
        features[18] = np.random.uniform(0.01, 0.05)
        
        # ê¸´ ê³ ì • ì‘ì‹œ
        features[19] = np.random.uniform(10, 25)
        
        # ë‚˜ë¨¸ì§€ íŠ¹ì§•ë“¤
        features[3] = np.random.uniform(60, 80)  # ê±°ë¦¬
        features[6:10] = [620, 350, 660, 350]  # ëˆˆ ìœ„ì¹˜
        features[10:12] = [0.3, 0.3]  # EAR
        features[12] = np.random.uniform(0, 2)  # ë¨¸ë¦¬ ê¸°ìš¸ê¸°
        features[20] = np.random.uniform(5, 10)  # ê³ ì • ì‹œê°„
        features[21] = np.random.uniform(0.6, 1.0)  # ì¤‘ì•™ ì§‘ì¤‘
        features[22] = np.random.uniform(0.6, 0.8)  # ê¹œë¹¡ì„
        
        return features
    
    def _generate_unfocused_features(self):
        """ë¹„ì§‘ì¤‘ ìƒíƒœ íŠ¹ì§• ìƒì„±"""
        features = np.zeros(26, dtype=np.float32)
        
        # ë¶ˆì•ˆì •í•œ ë¨¸ë¦¬ í¬ì¦ˆ
        features[0:3] = np.random.normal([2.0, 2.0, 1.5], [1.0, 1.0, 1.0])
        
        # ë¶„ì‚°ëœ ì‹œì„ 
        features[4] = np.random.normal(640, 150)  # ë” ë„“ì€ ë¶„í¬
        features[5] = np.random.normal(360, 100)
        
        # ë†’ì€ ë³€ë™ì„±
        features[13:15] = np.random.uniform([2.0, 2.0], [5.0, 5.0])
        
        # ë‚®ì€ ì•ˆì •ì„±
        features[15] = np.random.uniform(0.2, 0.5)
        features[16] = np.random.uniform(0.3, 0.6)
        
        # ë†’ì€ ë–¨ë¦¼
        features[17] = np.random.uniform(30.0, 60.0)
        features[18] = np.random.uniform(0.2, 0.5)
        
        # ì§§ì€ ê³ ì • ì‘ì‹œ
        features[19] = np.random.uniform(1, 5)
        
        # ë‚˜ë¨¸ì§€ íŠ¹ì§•ë“¤
        features[3] = np.random.uniform(40, 100)
        features[6:10] = np.random.uniform([500, 300, 700, 400], [800, 500, 900, 600])
        features[10:12] = [0.3, 0.3]
        features[12] = np.random.uniform(0, 8)
        features[20] = np.random.uniform(1, 3)
        features[21] = np.random.uniform(0.0, 0.4)
        features[22] = np.random.uniform(0.3, 0.8)
        
        return features
    
    def save_pytorch_dataset(self, sequences, labels, save_path):
        """PyTorch í•™ìŠµìš© ë°ì´í„°ì…‹ ì €ì¥"""
        print(f"ğŸ’¾ PyTorch ë°ì´í„°ì…‹ ì €ì¥: {save_path}")
        
        # íŠ¹ì§•ë“¤ì„ í…ì„œë¡œ ë³€í™˜
        processed_sequences = []
        processed_labels = []
        
        for seq, label in tqdm(zip(sequences, labels), desc="ë°ì´í„° ë³€í™˜", total=len(sequences)):
            # 30í”„ë ˆì„ì˜ íŠ¹ì§•ì„ ìŠ¤íƒ
            frame_features = []
            for frame_data in seq:
                # 26ì°¨ì› íŠ¹ì§• + 5ì°¨ì› attention íŠ¹ì§• = 31ì°¨ì›
                combined_features = np.concatenate([
                    frame_data['features'],
                    [
                        frame_data['attention_features']['central_focus'],
                        frame_data['attention_features']['gaze_fixation'],
                        frame_data['attention_features']['head_stability'],
                        frame_data['attention_features']['face_orientation'],
                        frame_data['attention_features']['attention_score']
                    ]
                ])
                frame_features.append(combined_features)
            
            # [30, 31] í˜•íƒœì˜ ì‹œí€€ìŠ¤
            sequence_tensor = torch.FloatTensor(frame_features)
            processed_sequences.append(sequence_tensor)
            processed_labels.append(label)
        
        # ì „ì²´ ë°ì´í„°ì…‹ ì €ì¥
        dataset = {
            'sequences': processed_sequences,
            'labels': processed_labels,
            'feature_dim': 31,
            'sequence_length': 30,
            'num_classes': 2
        }
        
        torch.save(dataset, save_path)
        print(f"âœ… ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {len(sequences)}ê°œ ì‹œí€€ìŠ¤")


def main():
    """ML â†’ PyTorch ë³€í™˜ ì‹¤í–‰"""
    print("ğŸ”„ ML ëª¨ë¸ì„ PyTorchë¡œ ë³€í™˜í•©ë‹ˆë‹¤")
    
    # ê¸°ì¡´ ML ëª¨ë¸ ê²½ë¡œ
    ml_model_path = input("ML ëª¨ë¸ ê²½ë¡œ (Enter=ê¸°ë³¸ê°’): ").strip() or \
                   "./xgboost_3class_concentration_classifier.pkl"
    
    if not os.path.exists(ml_model_path):
        print("âŒ ML ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return
    
    # ë³€í™˜ê¸° ìƒì„±
    converter = MLToPyTorchConverter(ml_model_path)
    
    # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
    print("\n1ï¸âƒ£ 30í”„ë ˆì„ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±")
    num_sequences = int(input("ìƒì„±í•  ì‹œí€€ìŠ¤ ìˆ˜ (Enter=5000): ").strip() or "5000")
    sequences, labels = converter.simulate_30_frame_sequence(num_sequences)
    
    # PyTorch ë°ì´í„°ì…‹ ì €ì¥
    print("\n2ï¸âƒ£ PyTorch ë°ì´í„°ì…‹ ì €ì¥")
    os.makedirs("./data", exist_ok=True)
    save_path = "./data/concentration_sequences.pt"
    converter.save_pytorch_dataset(sequences, labels, save_path)
    
    print(f"\nâœ… ë³€í™˜ ì™„ë£Œ!")
    print(f"ğŸ“ ì €ì¥ëœ ë°ì´í„°ì…‹: {save_path}")
    print(f"ğŸ“Š ì´ ì‹œí€€ìŠ¤: {len(sequences)}ê°œ")
    print(f"ğŸ“Š ì§‘ì¤‘ ë¹„ìœ¨: {sum(labels)/len(labels)*100:.1f}%")
    print(f"\në‹¤ìŒ ë‹¨ê³„: python train.py ë¡œ PyTorch ëª¨ë¸ í•™ìŠµ")

if __name__ == "__main__":
    main()
