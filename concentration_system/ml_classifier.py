import numpy as np
import pandas as pd
import xgboost as xgb
from xgboost import XGBClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN
from imblearn.combine import SMOTETomek
import joblib
from typing import Tuple, Dict, List
import warnings
warnings.filterwarnings('ignore')

class ConcentrationClassifier:
    """XGBoost ê¸°ë°˜ ì§‘ì¤‘ë„ ë¶„ë¥˜ê¸° (ë¶ˆê· í˜• ë°ì´í„° ë³´ì™„)"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.model = None
        self.model_type = None
        
        # í´ë˜ìŠ¤ ì •ë³´
        self.class_names = {0: 'ë¹„ì§‘ì¤‘', 1: 'ì£¼ì˜ì‚°ë§Œ', 2: 'ì§‘ì¤‘'}
        self.class_colors = {0: 'ë¹¨ê°•', 1: 'ë…¸ë‘', 2: 'ì´ˆë¡'}
        
        # ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬ ë°©ë²•ë“¤
        self.resampling_methods = {
            'smote': SMOTE(random_state=42, k_neighbors=3),
            'borderline': BorderlineSMOTE(random_state=42, k_neighbors=3),
            'adasyn': ADASYN(random_state=42, n_neighbors=3),
            'smote_tomek': SMOTETomek(random_state=42)
        }
        
    def prepare_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """íŠ¹ì§• ì¤€ë¹„"""
        meta_columns = ['metaid', 'condition', 'posture', 'inst', 'label_3class']
        feature_columns = [col for col in df.columns if col not in meta_columns]
        
        # NaN ê°’ ì²˜ë¦¬
        df_clean = df[feature_columns].fillna(0)
        
        X = df_clean.values
        y = df['label_3class'].values
        
        print(f"ì„ íƒëœ íŠ¹ì§• ìˆ˜: {len(feature_columns)}")
        return X, y, feature_columns
    
    def analyze_class_distribution(self, y: np.ndarray):
        """í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„"""
        unique, counts = np.unique(y, return_counts=True)
        total = len(y)
        
        print("\n=== í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„ ===")
        for cls, count in zip(unique, counts):
            percentage = count / total * 100
            print(f"{self.class_names[cls]}: {count}ê°œ ({percentage:.1f}%)")
        
        # ë¶ˆê· í˜• ë¹„ìœ¨ ê³„ì‚°
        max_count = max(counts)
        min_count = min(counts)
        imbalance_ratio = max_count / min_count
        print(f"ë¶ˆê· í˜• ë¹„ìœ¨: {imbalance_ratio:.2f}:1 {'(ì‹¬ê°í•œ ë¶ˆê· í˜•)' if imbalance_ratio > 3 else '(ë³´í†µ ë¶ˆê· í˜•)'}")
        
        return imbalance_ratio
    
    def prepare_data_advanced(self, X: np.ndarray, y: np.ndarray, 
                            method: str = 'auto') -> Tuple[np.ndarray, np.ndarray]:
        """ê°„ë‹¨í•œ ë°ì´í„° ì „ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì ˆì•½ + ì˜¤ë¥˜ ë°©ì§€)"""
        
        # ì •ê·œí™”
        X_scaled = self.scaler.fit_transform(X.astype(np.float32))  # float32ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
        
        # ë¶ˆê· í˜• ë¹„ìœ¨ ë¶„ì„
        unique, counts = np.unique(y, return_counts=True)
        total = len(y)
        
        print("\n=== í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„ ===")
        for cls, count in zip(unique, counts):
            percentage = count / total * 100
            print(f"{self.class_names[cls]}: {count}ê°œ ({percentage:.1f}%)")
        
        # ë¶ˆê· í˜• ë¹„ìœ¨ ê³„ì‚°
        max_count = max(counts)
        min_count = min(counts)
        imbalance_ratio = max_count / min_count
        print(f"ë¶ˆê· í˜• ë¹„ìœ¨: {imbalance_ratio:.2f}:1")
        
        # ê°„ë‹¨í•œ ë¦¬ìƒ˜í”Œë§ ì ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
        print(f"\nğŸ’¾ ë©”ëª¨ë¦¬ ì ˆì•½í˜• ë¦¬ìƒ˜í”Œë§ ì ìš© ì¤‘...")
        
        try:
            # ë°ì´í„° í¬ê¸° í™•ì¸
            data_size_mb = X_scaled.nbytes / (1024 * 1024)
            print(f"í˜„ì¬ ë°ì´í„° í¬ê¸°: {data_size_mb:.1f} MB")
            
            if data_size_mb > 100:  # 100MB ì´ˆê³¼ì‹œ ë¦¬ìƒ˜í”Œë§ ì œí•œ
                print("âš ï¸ ë°ì´í„°ê°€ ë„ˆë¬´ í¼, í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ë§Œ ì ìš©")
                return X_scaled, y
            
            # ê°„ë‹¨í•œ SMOTE (k_neighbors ìµœì†Œí™”)
            smote_simple = SMOTE(
                random_state=42, 
                k_neighbors=min(2, min(counts) - 1),  # ìµœì†Œ í´ë˜ìŠ¤ ê°œìˆ˜ì— ë§ì¶¤
                sampling_strategy='auto'  # ìë™ ê· í˜•
            )
            
            print("ğŸ”„ SMOTE ë¦¬ìƒ˜í”Œë§ ì¤‘...")
            X_balanced, y_balanced = smote_simple.fit_resample(X_scaled, y)
            
            # ê²°ê³¼ í™•ì¸
            balanced_size_mb = X_balanced.nbytes / (1024 * 1024)
            print(f"ë¦¬ìƒ˜í”Œë§ í›„ í¬ê¸°: {balanced_size_mb:.1f} MB")
            
            print(f"âœ… ë¦¬ìƒ˜í”Œë§ ì™„ë£Œ:")
            unique_balanced, counts_balanced = np.unique(y_balanced, return_counts=True)
            for cls, count in zip(unique_balanced, counts_balanced):
                print(f"  {self.class_names[cls]}: {count}ê°œ")
            
            # ë©”ëª¨ë¦¬ ì²´í¬
            if balanced_size_mb > 500:  # 500MB ì´ˆê³¼ì‹œ ìƒ˜í”Œ ì¤„ì´ê¸°
                print("âš ï¸ ë¦¬ìƒ˜í”Œë§ ë°ì´í„°ê°€ ë„ˆë¬´ í¼, ìƒ˜í”Œë§ ì¤„ì„")
                
                # ê° í´ë˜ìŠ¤ë‹¹ ìµœëŒ€ 1000ê°œë¡œ ì œí•œ
                max_samples_per_class = 1000
                indices_to_keep = []
                
                for cls in unique_balanced:
                    cls_indices = np.where(y_balanced == cls)[0]
                    if len(cls_indices) > max_samples_per_class:
                        selected_indices = np.random.choice(
                            cls_indices, 
                            size=max_samples_per_class, 
                            replace=False
                        )
                        indices_to_keep.extend(selected_indices)
                    else:
                        indices_to_keep.extend(cls_indices)
                
                X_balanced = X_balanced[indices_to_keep]
                y_balanced = y_balanced[indices_to_keep]
                
                print(f"ğŸ“‰ ìƒ˜í”Œë§ í›„ ìµœì¢… í¬ê¸°: {len(X_balanced)}ê°œ")
            
            return X_balanced, y_balanced
            
        except Exception as e:
            print(f"âŒ ë¦¬ìƒ˜í”Œë§ ì‹¤íŒ¨: {str(e)}")
            print("ğŸ”„ ì›ë³¸ ë°ì´í„°ë¡œ ì§„í–‰...")
            
            # ë¦¬ìƒ˜í”Œë§ ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°ì´í„° ì‚¬ìš©
            return X_scaled, y
        
        except MemoryError:
            print("âŒ ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ë¦¬ìƒ˜í”Œë§ ì‹¤íŒ¨")
            print("ğŸ”„ ì›ë³¸ ë°ì´í„°ë¡œ ì§„í–‰...")
            return X_scaled, y

    
    def train_xgboost_simple(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """ê°„ë‹¨í•œ XGBoost í•™ìŠµ (ì˜¤ë¥˜ ë°©ì§€)"""
        
        try:
            # ê°„ë‹¨í•œ XGBoost ì„¤ì •
            self.model = XGBClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42,
                n_jobs=1,  # ë³‘ë ¬ ì²˜ë¦¬ ë”
                verbosity=0  # ë¡œê·¸ ìµœì†Œí™”
            )
            
            # ëª¨ë¸ í•™ìŠµ
            self.model.fit(X, y)
            self.model_type = 'XGBoost'
            
            # êµì°¨ê²€ì¦
            cv_scores = cross_val_score(self.model, X, y, cv=3)  # 5 â†’ 3ìœ¼ë¡œ ì¤„ì„
            
            return {
                'model_type': 'XGBoost',
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std()
            }
            
        except Exception as e:
            print(f"âŒ XGBoost ì˜¤ë¥˜: {e}")
            print("ğŸ”„ RandomForestë¡œ ëŒ€ì²´ í•™ìŠµ...")
            
            # ëŒ€ì²´: RandomForest ì‚¬ìš©
            from sklearn.ensemble import RandomForestClassifier
            self.model = RandomForestClassifier(
                n_estimators=200,
                max_depth=20,
                random_state=42
            )
            self.model.fit(X, y)
            self.model_type = 'RandomForest'
            
            cv_scores = cross_val_score(self.model, X, y, cv=3)
            return {
                'model_type': 'RandomForest (ëŒ€ì²´)',
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std()
            }

    
    def get_feature_importance(self, feature_columns: List[str]) -> Dict:
        """XGBoost íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„"""
        if hasattr(self.model, 'feature_importances_'):
            importances = self.model.feature_importances_
            
            # íŠ¹ì§•ë³„ ì¤‘ìš”ë„ ì •ë ¬
            feature_importance = list(zip(feature_columns, importances))
            feature_importance.sort(key=lambda x: x[1], reverse=True)
            
            print(f"\nğŸ¯ ìƒìœ„ 15ê°œ ì¤‘ìš” íŠ¹ì§• (XGBoost):")
            for i, (feature, importance) in enumerate(feature_importance[:15]):
                print(f"  {i+1:2d}. {feature:20s}: {importance:.4f}")
            
            return dict(feature_importance)
        
        return {}
    
    def evaluate_advanced(self, X_test: np.ndarray, y_test: np.ndarray, feature_columns: List[str]) -> Dict:
        """ê³ ê¸‰ ëª¨ë¸ í‰ê°€ (ë¶ˆê· í˜• ë°ì´í„° ê³ ë ¤)"""
        X_test_scaled = self.scaler.transform(X_test)
        
        # ì˜ˆì¸¡
        y_pred = self.model.predict(X_test_scaled)
        y_pred_proba = self.model.predict_proba(X_test_scaled)
        
        # ë‹¤ì–‘í•œ ë©”íŠ¸ë¦­ ê³„ì‚°
        accuracy = accuracy_score(y_test, y_pred)
        f1_macro = f1_score(y_test, y_pred, average='macro')
        f1_weighted = f1_score(y_test, y_pred, average='weighted')
        
        # ë¶„ë¥˜ ë¦¬í¬íŠ¸
        report = classification_report(
            y_test, y_pred, 
            target_names=[self.class_names[i] for i in range(3)],
            output_dict=True
        )
        
        # í˜¼ë™ í–‰ë ¬
        cm = confusion_matrix(y_test, y_pred)
        
        # íŠ¹ì§• ì¤‘ìš”ë„
        feature_importance = self.get_feature_importance(feature_columns)
        
        print(f"\n=== ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===")
        print(f"ì •í™•ë„ (Accuracy): {accuracy:.4f}")
        print(f"F1 Score (Macro): {f1_macro:.4f}")
        print(f"F1 Score (Weighted): {f1_weighted:.4f}")
        
        return {
            'accuracy': accuracy,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'classification_report': report,
            'confusion_matrix': cm,
            'predictions': y_pred,
            'probabilities': y_pred_proba,
            'feature_importance': feature_importance
        }
    
    def predict(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """ì˜ˆì¸¡ ìˆ˜í–‰"""
        X_scaled = self.scaler.transform(X)
        predictions = self.model.predict(X_scaled)
        probabilities = self.model.predict_proba(X_scaled)
        return predictions, probabilities
    
    def save_model(self, filepath: str, feature_columns: List[str]):
        """ëª¨ë¸ ì €ì¥"""
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'model_type': self.model_type,
            'class_names': self.class_names,
            'feature_columns': feature_columns
        }
        joblib.dump(model_data, filepath)
        print(f"XGBoost ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")
    
    def load_model(self, filepath: str):
        """ëª¨ë¸ ë¡œë“œ"""
        model_data = joblib.load(filepath)
        self.model = model_data['model']
        self.scaler = model_data['scaler']
        self.model_type = model_data['model_type']
        self.class_names = model_data['class_names']
        self.feature_columns = model_data.get('feature_columns', [])
        print(f"XGBoost ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {filepath}")