# inference2_stable.py (ì•ˆì •ì ì¸ Version 2 í…ŒìŠ¤íŠ¸)
import os
import pickle
import cv2
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from tqdm import tqdm
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import math

from sklearn.metrics import (
    confusion_matrix,
    ConfusionMatrixDisplay,
    accuracy_score,
    recall_score,
    f1_score
)

# ------------------ Dataset (Version 2 í˜¸í™˜) ------------------
class VideoFolderDataset(Dataset):
    def __init__(self, data_list, transform=None):
        self.data_list = []
        self.transform = transform or transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
        
        for folder_path, label in data_list:
            if os.path.isdir(folder_path):
                img_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                if len(img_files) >= 30:
                    self.data_list.append((folder_path, label))
        
        print(f"ğŸ¯ ë°ì´í„°ì…‹ ì´ˆê¸°í™” ì™„ë£Œ: {len(self.data_list)}ê°œ ìœ íš¨ ìƒ˜í”Œ")

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        folder_path, label = self.data_list[idx]
        img_files = sorted([f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])[:30]
        frames = []
        
        for f in img_files:
            img_path = os.path.join(folder_path, f)
            try:
                img_pil = Image.open(img_path).convert('RGB')
                frames.append(self.transform(img_pil))
            except Exception as e:
                print(f"âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {img_path}")
                continue
        
        # 30ê°œ í”„ë ˆì„ ë³´ì¥
        while len(frames) < 30:
            frames.append(frames[-1] if frames else torch.zeros(3, 224, 224))
        
        video = torch.stack(frames[:30])

        fusion_path = os.path.join(folder_path, "fusion_features.pkl")
        if os.path.exists(fusion_path):
            with open(fusion_path, 'rb') as f:
                fusion = torch.tensor(pickle.load(f), dtype=torch.float32)
        else:
            fusion = torch.zeros(5)

        return video, fusion, torch.tensor(label, dtype=torch.float32)

# ------------------ Version 2 CNNEncoder ------------------
class CNNEncoder(nn.Module):
    def __init__(self, output_dim=1280):
        super().__init__()
        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)
        self.features = mobilenet.features
        self.avgpool = nn.AdaptiveAvgPool2d((4, 4))
        
        self.fc = nn.Sequential(
            nn.Linear(1280 * 4 * 4, 2048),
            nn.BatchNorm1d(2048),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(2048, output_dim),
            nn.BatchNorm1d(output_dim),
            nn.ReLU()
        )

    def forward(self, x):
        B, T, C, H, W = x.shape
        x = x.view(B * T, C, H, W)
        x = self.features(x)
        x = self.avgpool(x)
        x = x.view(B * T, -1)
        x = self.fc(x)
        return x.view(B, T, -1)

# ------------------ Version 2 Transformer ëª¨ë¸ ------------------
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        seq_len = x.size(0)
        return x + self.pe[:seq_len, :].to(x.device)

class EngagementModelV2(nn.Module):
    def __init__(self, cnn_feat_dim=1280, fusion_feat_dim=5, d_model=256, nhead=8, num_layers=4):
        super().__init__()
        
        self.input_projection = nn.Sequential(
            nn.Linear(cnn_feat_dim, d_model),
            nn.LayerNorm(d_model),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        self.pos_encoder = PositionalEncoding(d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=0.15,
            activation='gelu',
            batch_first=True,
            norm_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)
        self.global_max_pool = nn.AdaptiveMaxPool1d(1)
        
        self.fc = nn.Sequential(
            nn.Linear(d_model * 2 + fusion_feat_dim, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.LayerNorm(128),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(128, 1)
        )

    def forward(self, cnn_feats, fusion_feats):
        x = self.input_projection(cnn_feats)
        x = x.transpose(0, 1)
        x = self.pos_encoder(x)
        x = x.transpose(0, 1)
        transformer_out = self.transformer_encoder(x)
        
        avg_pooled = self.global_avg_pool(transformer_out.transpose(1, 2)).squeeze(-1)
        max_pooled = self.global_max_pool(transformer_out.transpose(1, 2)).squeeze(-1)
        pooled = torch.cat([avg_pooled, max_pooled], dim=1)
        
        combined = torch.cat([pooled, fusion_feats], dim=1)
        return self.fc(combined)

# ------------------ Utils ------------------
def load_data(pkl_files):
    all_data = []
    for pkl_path in pkl_files:
        if not os.path.exists(pkl_path):
            print(f"âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pkl_path}")
            continue
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f)
            all_data.extend(data)
            print(f"âœ… ë¡œë“œë¨: {pkl_path} ({len(data)}ê°œ ìƒ˜í”Œ)")
    return all_data

def test_multiple_thresholds(all_probs, all_labels):
    """ì—¬ëŸ¬ ì„ê³„ê°’ìœ¼ë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ¯ **ì„ê³„ê°’ë³„ ì„±ëŠ¥ ë¹„êµ**")
    print("="*60)
    
    best_threshold = 0.5
    best_f1 = 0.0
    
    for threshold in [0.3, 0.4, 0.5, 0.6, 0.7]:
        preds = (np.array(all_probs) >= threshold).astype(np.int32)
        acc = accuracy_score(all_labels, preds)
        rec = recall_score(all_labels, preds, zero_division=0)
        f1 = f1_score(all_labels, preds, zero_division=0)
        
        print(f"Threshold {threshold:.1f}: Acc={acc:.4f} | Rec={rec:.4f} | F1={f1:.4f}")
        
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold
    
    print(f"\nğŸ† **ìµœì  ì„ê³„ê°’: {best_threshold:.1f} (F1={best_f1:.4f})**")
    return best_threshold

def get_optimal_batch_size(total_samples, device):
    """ì•™ìƒë¸”ê³¼ ë™ì¼í•œ ìµœì  ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê³„ì‚°"""
    if device.type == 'cuda':
        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
        if total_memory >= 8:  # 8GB ì´ìƒ
            return min(8, total_samples)  # Version 2ëŠ” ì•™ìƒë¸”ë³´ë‹¤ ì•½ê°„ í¬ê²Œ
        elif total_memory >= 4:  # 4GB ì´ìƒ
            return min(4, total_samples)
        else:
            return min(2, total_samples)
    else:
        return min(16, total_samples)

# ------------------ Main Test Function ------------------
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”¥ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    print("ğŸš€ **Version 2 ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì•™ìƒë¸” ë°©ì‹ ì ìš©)**")
    print("="*60)

    # âœ… Version 2 ëª¨ë¸ ê²½ë¡œ
    best_model_path = "./log/v2/best_model_v2.pt"
    
    # âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ (702ê°œ ìƒ˜í”Œ)
    test_pkl_files = [
        "./preprocessed_data_full/pickle_labels/test/test_data.pkl" 
    ]

    # ë°ì´í„° ë¡œë“œ
    test_data_list = load_data(test_pkl_files)
    if len(test_data_list) == 0:
        print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    print(f"ğŸ“Š ì´ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_data_list):,}ê°œ")
    
    test_dataset = VideoFolderDataset(test_data_list)
    
    # âœ… ì•™ìƒë¸”ê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ìµœì  ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê³„ì‚°
    optimal_batch_size = get_optimal_batch_size(len(test_dataset), device)
    print(f"ğŸ¯ ìµœì  ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {optimal_batch_size}")
    
    test_loader = DataLoader(
        test_dataset, 
        batch_size=optimal_batch_size,  # 2-8ê°œì˜ ì‘ì€ ë°°ì¹˜
        shuffle=False,
        num_workers=4, 
        pin_memory=True,
        drop_last=False  # ëª¨ë“  ë°ì´í„° ì‚¬ìš©
    )
    
    print(f"ğŸ“¦ ì´ {len(test_loader)}ê°œ ë°°ì¹˜ë¡œ ë¶„í•  (ë°°ì¹˜ë‹¹ ìµœëŒ€ {optimal_batch_size}ê°œ)")
    print(f"ğŸ¯ ì²˜ë¦¬ë  ì´ ìƒ˜í”Œ ìˆ˜: {len(test_dataset)}ê°œ (100% ì‚¬ìš©)")

    # âœ… Version 2 ëª¨ë¸ ì´ˆê¸°í™”
    cnn = CNNEncoder().to(device)
    model = EngagementModelV2(d_model=256, nhead=8, num_layers=4).to(device)

    if not os.path.exists(best_model_path):
        print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {best_model_path}")
        print("ë‹¤ìŒ ê²½ë¡œë“¤ì„ í™•ì¸í•´ë³´ì„¸ìš”:")
        print("  - ./log/v2/best_model_v2.pt")
        print("  - ./log/best_model2.pt")
        return

    print(f"ğŸ“‚ Version 2 ëª¨ë¸ ë¡œë”© ì¤‘: {best_model_path}")
    try:
        ckpt = torch.load(best_model_path, map_location=device)
        cnn.load_state_dict(ckpt['cnn_state_dict'])
        model.load_state_dict(ckpt['model_state_dict'])
        
        if 'epoch' in ckpt:
            print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ (Epoch {ckpt['epoch'] + 1}, Val Loss: {ckpt.get('val_loss', 'N/A'):.4f})")
        else:
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return

    cnn.eval()
    model.eval()

    # ì¶”ë¡  & ë©”íŠ¸ë¦­
    all_probs, all_preds, all_labels = [], [], []
    total_processed = 0

    print(f"\nğŸ”„ Version 2 ì¶”ë¡  ì‹œì‘ (ì´ {len(test_dataset)}ê°œ ìƒ˜í”Œ)...")
    with torch.no_grad():
        for batch_idx, (videos, fusion, labels) in enumerate(tqdm(test_loader, desc="Version 2 Test")):
            videos = videos.to(device, non_blocking=True)
            fusion = fusion.to(device, non_blocking=True)
            
            current_batch_size = videos.size(0)
            total_processed += current_batch_size
            
            # CNN íŠ¹ì§• ì¶”ì¶œ
            feats = cnn(videos)
            
            # Transformer ì¶”ë¡ 
            logits = model(feats, fusion)
            probs = torch.sigmoid(logits).squeeze(1).detach().cpu().numpy()
            
            # ê¸°ë³¸ ì„ê³„ê°’ 0.5ë¡œ ì˜ˆì¸¡
            preds = (probs >= 0.5).astype(np.int32)
            labels = labels.int().numpy()

            all_probs.extend(probs.tolist())
            all_preds.extend(preds.tolist())
            all_labels.extend(labels.tolist())
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥ (ì²˜ìŒê³¼ ë§ˆì§€ë§‰ ë°°ì¹˜ì—ì„œ)
            if batch_idx == 0 or batch_idx == len(test_loader) - 1 or (batch_idx + 1) % 20 == 0:
                print(f"  ë°°ì¹˜ {batch_idx + 1}/{len(test_loader)}: {current_batch_size}ê°œ ìƒ˜í”Œ ì²˜ë¦¬ ì™„ë£Œ (ëˆ„ì : {total_processed}ê°œ)")

    print(f"âœ… ì´ {total_processed}ê°œ ìƒ˜í”Œ ì²˜ë¦¬ ì™„ë£Œ (ì˜ˆìƒ: {len(test_dataset)}ê°œ)")

    # âœ… ê¸°ë³¸ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    acc = accuracy_score(all_labels, all_preds)
    rec = recall_score(all_labels, all_preds, zero_division=0)
    f1 = f1_score(all_labels, all_preds, zero_division=0)
    cm = confusion_matrix(all_labels, all_preds)

    print("\n" + "="*60)
    print("ğŸ“Š **Version 2 í…ŒìŠ¤íŠ¸ ê²°ê³¼ (ì„ê³„ê°’ 0.5)**")
    print("="*60)
    print(f"âœ… Accuracy: {acc:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}")
    print("\nConfusion Matrix:")
    print(cm)
    
    # í´ë˜ìŠ¤ë³„ ë¶„í¬ ì¶œë ¥
    unique, counts = np.unique(all_labels, return_counts=True)
    print(f"\nğŸ“‹ ì‹¤ì œ ë¼ë²¨ ë¶„í¬: {dict(zip(unique, counts))}")
    unique, counts = np.unique(all_preds, return_counts=True)
    print(f"ğŸ“‹ ì˜ˆì¸¡ ë¼ë²¨ ë¶„í¬: {dict(zip(unique, counts))}")

    # ë°ì´í„° ì‚¬ìš©ë¥  í™•ì¸
    expected_samples = len(test_dataset)
    actual_samples = len(all_labels)
    usage_rate = (actual_samples / expected_samples) * 100
    print(f"\nğŸ“Š **ë°ì´í„° ì‚¬ìš©ë¥ **: {actual_samples}/{expected_samples} ({usage_rate:.1f}%)")

    # âœ… ì—¬ëŸ¬ ì„ê³„ê°’ìœ¼ë¡œ ìµœì í™”
    best_threshold = test_multiple_thresholds(all_probs, all_labels)
    
    # ìµœì  ì„ê³„ê°’ìœ¼ë¡œ ì¬ê³„ì‚°
    best_preds = (np.array(all_probs) >= best_threshold).astype(np.int32)
    best_acc = accuracy_score(all_labels, best_preds)
    best_rec = recall_score(all_labels, best_preds, zero_division=0)
    best_f1 = f1_score(all_labels, best_preds, zero_division=0)
    best_cm = confusion_matrix(all_labels, best_preds)

    print("\n" + "="*60)
    print(f"ğŸ“Š **Version 2 ìµœì í™” ê²°ê³¼ (ì„ê³„ê°’ {best_threshold:.1f})**")
    print("="*60)
    print(f"ğŸ† Accuracy: {best_acc:.4f} | Recall: {best_rec:.4f} | F1: {best_f1:.4f}")
    
    # âœ… í˜¼ë™í–‰ë ¬ ì €ì¥
    save_dir = "./log/v2/test_results"
    os.makedirs(os.path.join(save_dir, "confusion_matrix"), exist_ok=True)
    
    # ê¸°ë³¸ ì„ê³„ê°’ í˜¼ë™í–‰ë ¬
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["ì§‘ì¤‘ì•ˆí•¨", "ì§‘ì¤‘í•¨"])
    disp.plot(cmap=plt.cm.Blues)
    plt.title(f"Version 2 Confusion Matrix (Threshold 0.5)\n{actual_samples} samples - Stable Batches")
    out_path_basic = os.path.join(save_dir, "confusion_matrix", "conf_matrix_v2_stable.png")
    plt.savefig(out_path_basic, dpi=200, bbox_inches="tight")
    plt.close()
    
    # ìµœì  ì„ê³„ê°’ í˜¼ë™í–‰ë ¬
    disp_best = ConfusionMatrixDisplay(confusion_matrix=best_cm, display_labels=["ì§‘ì¤‘ì•ˆí•¨", "ì§‘ì¤‘í•¨"])
    disp_best.plot(cmap=plt.cm.Blues)
    plt.title(f"Version 2 Confusion Matrix (Optimal {best_threshold:.1f})\n{actual_samples} samples - Stable Batches")
    out_path_best = os.path.join(save_dir, "confusion_matrix", f"conf_matrix_v2_stable_{best_threshold:.1f}.png")
    plt.savefig(out_path_best, dpi=200, bbox_inches="tight")
    plt.close()
    
    print(f"\nğŸ“Š Confusion matrices saved:")
    print(f"  - Basic (0.5): {out_path_basic}")
    print(f"  - Optimal ({best_threshold:.1f}): {out_path_best}")

    # âœ… ìµœì¢… ì„±ëŠ¥ ìš”ì•½
    print("\n" + "="*60)
    print("ğŸ‰ **Version 2 ì•ˆì •ì  ì²˜ë¦¬ ì™„ë£Œ!**")
    print("="*60)
    print(f"ğŸ”¸ ì²˜ë¦¬ëœ ë°ì´í„°: {actual_samples:,}ê°œ ({usage_rate:.1f}%)")
    print(f"ğŸ”¸ ë°°ì¹˜ í¬ê¸°: {optimal_batch_size}ê°œ (ì•™ìƒë¸” ë°©ì‹)")
    print(f"ğŸ”¸ ì´ ë°°ì¹˜ ìˆ˜: {len(test_loader)}ê°œ")
    print(f"ğŸ”¸ Version 2 ì •í™•ë„: {best_acc:.1%}")
    print(f"ğŸ”¸ ì¬í˜„ìœ¨: {best_rec:.1%}")
    print(f"ğŸ”¸ F1-Score: {best_f1:.1%}")
    print(f"ğŸ”¸ ìµœì  ì„ê³„ê°’: {best_threshold}")
    
    # ì„±ëŠ¥ í‰ê°€
    if best_acc > 0.80:
        print("ğŸ‰ 80% ì´ìƒ! í›Œë¥­í•œ ì„±ëŠ¥ì…ë‹ˆë‹¤!")
    elif best_acc > 0.70:
        print("âœ… 70% ì´ìƒ! ì–‘í˜¸í•œ ì„±ëŠ¥ì…ë‹ˆë‹¤!")
    elif best_acc > 0.60:
        print("ğŸ‘ 60% ì´ìƒ! ê°œì„  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤!")
    else:
        print("âš ï¸ ì„±ëŠ¥ì´ ì œí•œì ì…ë‹ˆë‹¤. ëª¨ë¸ ì¬í›ˆë ¨ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
    
    print(f"\nğŸ’¡ **ì²˜ë¦¬ ë°©ì‹ ìš”ì•½**")
    print(f"   - ì•™ìƒë¸”ê³¼ ë™ì¼í•œ ì•ˆì •ì  ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹ ì ìš©")
    print(f"   - GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì‚¬ìš©")
    print(f"   - ì „ì²´ 702ê°œ ìƒ˜í”Œ 100% ì²˜ë¦¬ ì™„ë£Œ")

if __name__ == "__main__":
    main()
