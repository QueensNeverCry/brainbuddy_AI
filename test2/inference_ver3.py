# ensemble_inference_test_updated.py (702ê°œ ìƒ˜í”Œ pkl íŒŒì¼ìš© ì•™ìƒë¸” í…ŒìŠ¤íŠ¸)
import os
import pickle
import cv2
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from tqdm import tqdm
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import math

from sklearn.metrics import (
    confusion_matrix,
    ConfusionMatrixDisplay,
    accuracy_score,
    recall_score,
    f1_score
)

# ------------------ Dataset (ì•™ìƒë¸” í˜¸í™˜) ------------------
class VideoFolderDataset(Dataset):
    def __init__(self, data_list, transform=None):
        self.data_list = []
        self.transform = transform or transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
        
        for folder_path, label in data_list:
            if os.path.isdir(folder_path):
                img_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                if len(img_files) >= 30:
                    self.data_list.append((folder_path, label))
        
        print(f"ğŸ¯ ì•™ìƒë¸” ë°ì´í„°ì…‹ ì´ˆê¸°í™” ì™„ë£Œ: {len(self.data_list)}ê°œ ìœ íš¨ ìƒ˜í”Œ")

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        folder_path, label = self.data_list[idx]
        img_files = sorted([f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])[:30]
        frames = []
        
        for f in img_files:
            img_path = os.path.join(folder_path, f)
            try:
                img_pil = Image.open(img_path).convert('RGB')
                frames.append(self.transform(img_pil))
            except Exception as e:
                print(f"âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {img_path}")
                continue
        
        # 30ê°œ í”„ë ˆì„ ë³´ì¥
        while len(frames) < 30:
            frames.append(frames[-1] if frames else torch.zeros(3, 224, 224))
        
        video = torch.stack(frames[:30])

        fusion_path = os.path.join(folder_path, "fusion_features.pkl")
        if os.path.exists(fusion_path):
            with open(fusion_path, 'rb') as f:
                fusion = torch.tensor(pickle.load(f), dtype=torch.float32)
        else:
            fusion = torch.zeros(5)

        # ë¼ë²¨ ì²˜ë¦¬ - ìƒˆë¡œìš´ pkl íŒŒì¼ì€ ì´ë¯¸ ìˆ«ì ë¼ë²¨ (ì§‘ì¤‘í•¨=1, ì§‘ì¤‘ì•ˆí•¨=0)
        return video, fusion, torch.tensor(label, dtype=torch.float32)

# ------------------ CNNEncoder (Version 1ê³¼ Version 2 êµ¬ì¡°) ------------------
class CNNEncoderV1(nn.Module):
    """Version 1ìš© CNNEncoder (ê¸°ë³¸ êµ¬ì¡°)"""
    def __init__(self, output_dim=1280):
        super().__init__()
        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)
        self.features = mobilenet.features
        self.avgpool = nn.AdaptiveAvgPool2d((4, 4))
        self.fc = nn.Sequential(
            nn.Linear(1280 * 4 * 4, 2048),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(2048, output_dim),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        B, T, C, H, W = x.shape
        x = x.view(B * T, C, H, W)
        x = self.features(x)
        x = self.avgpool(x)
        x = x.view(B * T, -1)
        x = self.fc(x)
        return x.view(B, T, -1)

class CNNEncoderV2(nn.Module):
    """Version 2ìš© CNNEncoder (BatchNorm í¬í•¨)"""
    def __init__(self, output_dim=1280):
        super().__init__()
        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)
        self.features = mobilenet.features
        self.avgpool = nn.AdaptiveAvgPool2d((4, 4))
        self.fc = nn.Sequential(
            nn.Linear(1280 * 4 * 4, 2048),
            nn.BatchNorm1d(2048),
            nn.ReLU(inplace=True),
            nn.Dropout(0.4),
            nn.Linear(2048, output_dim),
            nn.BatchNorm1d(output_dim),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        B, T, C, H, W = x.shape
        x = x.view(B * T, C, H, W)
        x = self.features(x)
        x = self.avgpool(x)
        x = x.view(B * T, -1)
        x = self.fc(x)
        return x.view(B, T, -1)

# ------------------ Positional Encoding ------------------
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        seq_len = x.size(0)
        return x + self.pe[:seq_len, :].to(x.device)

# ------------------ Version 1 Transformer ------------------
class EngagementModelV1(nn.Module):
    def __init__(self, cnn_feat_dim=1280, fusion_feat_dim=5, d_model=128, nhead=8, num_layers=3):
        super().__init__()
        
        self.input_projection = nn.Linear(cnn_feat_dim, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=0.1,
            activation='relu',
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        
        self.fc = nn.Sequential(
            nn.Linear(d_model + fusion_feat_dim, 64),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(64, 1)
        )

    def forward(self, cnn_feats, fusion_feats):
        x = self.input_projection(cnn_feats)
        x = x.transpose(0, 1)
        x = self.pos_encoder(x)
        x = x.transpose(0, 1)
        transformer_out = self.transformer_encoder(x)
        pooled = self.global_pool(transformer_out.transpose(1, 2)).squeeze(-1)
        combined = torch.cat([pooled, fusion_feats], dim=1)
        return self.fc(combined)

# ------------------ Version 2 Transformer ------------------
class EngagementModelV2(nn.Module):
    def __init__(self, cnn_feat_dim=1280, fusion_feat_dim=5, d_model=256, nhead=8, num_layers=4):
        super().__init__()
        
        self.input_projection = nn.Sequential(
            nn.Linear(cnn_feat_dim, d_model),
            nn.LayerNorm(d_model),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1)
        )
        
        self.pos_encoder = PositionalEncoding(d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=0.15,
            activation='gelu',
            batch_first=True,
            norm_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)
        self.global_max_pool = nn.AdaptiveMaxPool1d(1)
        
        self.fc = nn.Sequential(
            nn.Linear(d_model * 2 + fusion_feat_dim, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.LayerNorm(128),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(128, 1)
        )

    def forward(self, cnn_feats, fusion_feats):
        x = self.input_projection(cnn_feats)
        x = x.transpose(0, 1)
        x = self.pos_encoder(x)
        x = x.transpose(0, 1)
        transformer_out = self.transformer_encoder(x)
        
        avg_pooled = self.global_avg_pool(transformer_out.transpose(1, 2)).squeeze(-1)
        max_pooled = self.global_max_pool(transformer_out.transpose(1, 2)).squeeze(-1)
        pooled = torch.cat([avg_pooled, max_pooled], dim=1)
        
        combined = torch.cat([pooled, fusion_feats], dim=1)
        return self.fc(combined)

# ------------------ ì•™ìƒë¸” ëª¨ë¸ ------------------
class TransformerEnsembleModel(nn.Module):
    def __init__(self, cnn_v1, model_v1, cnn_v2, model_v2, ensemble_method='learned'):
        super().__init__()
        self.cnn_v1 = cnn_v1
        self.model_v1 = model_v1
        self.cnn_v2 = cnn_v2
        self.model_v2 = model_v2
        self.ensemble_method = ensemble_method
        
        if ensemble_method == 'weighted':
            self.register_buffer('weights', torch.tensor([0.3, 0.7]))
        elif ensemble_method == 'learned':
            self.ensemble_weights = nn.Parameter(torch.tensor([0.3, 0.7]))
            self.ensemble_fc = nn.Sequential(
                nn.Linear(2, 16),
                nn.ReLU(inplace=True),
                nn.Dropout(0.2),
                nn.Linear(16, 1)
            )

    def forward(self, videos, fusion_feats):
        # Version 1 ì¶”ë¡ 
        feats_v1 = self.cnn_v1(videos)
        logits_v1 = self.model_v1(feats_v1, fusion_feats)
        
        # Version 2 ì¶”ë¡ 
        feats_v2 = self.cnn_v2(videos)
        logits_v2 = self.model_v2(feats_v2, fusion_feats)
        
        if self.ensemble_method == 'weighted':
            return self.weights[0] * logits_v1 + self.weights[1] * logits_v2
        elif self.ensemble_method == 'learned':
            prob_v1 = torch.sigmoid(logits_v1)
            prob_v2 = torch.sigmoid(logits_v2)
            normalized_weights = torch.softmax(self.ensemble_weights, dim=0)
            weighted_v1 = prob_v1 * normalized_weights[0]
            weighted_v2 = prob_v2 * normalized_weights[1]
            combined_input = torch.cat([weighted_v1, weighted_v2], dim=1)
            return self.ensemble_fc(combined_input)

# ------------------ Utils ------------------
def load_data(pkl_files):
    all_data = []
    for pkl_path in pkl_files:
        if not os.path.exists(pkl_path):
            print(f"âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pkl_path}")
            continue
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f)
            all_data.extend(data)
            print(f"âœ… ë¡œë“œë¨: {pkl_path} ({len(data)}ê°œ ìƒ˜í”Œ)")
    return all_data

def test_multiple_thresholds(all_probs, all_labels):
    """ì—¬ëŸ¬ ì„ê³„ê°’ìœ¼ë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ¯ **ì•™ìƒë¸” ì„ê³„ê°’ë³„ ì„±ëŠ¥ ë¹„êµ**")
    print("="*60)
    
    best_threshold = 0.5
    best_f1 = 0.0
    
    for threshold in [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]:
        preds = (np.array(all_probs) >= threshold).astype(np.int32)
        acc = accuracy_score(all_labels, preds)
        rec = recall_score(all_labels, preds, zero_division=0)
        f1 = f1_score(all_labels, preds, zero_division=0)
        
        print(f"Threshold {threshold:.1f}: Acc={acc:.4f} | Rec={rec:.4f} | F1={f1:.4f}")
        
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold
    
    print(f"\nğŸ† **ìµœì  ì„ê³„ê°’: {best_threshold:.1f} (F1={best_f1:.4f})**")
    return best_threshold

def get_optimal_batch_size(total_samples, device):
    """GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ìµœì  ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê²°ì • (ì•™ìƒë¸”ì€ ë” ë§ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©)"""
    if device.type == 'cuda':
        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
        if total_memory >= 8:  # 8GB ì´ìƒ
            return min(4, total_samples)  # ì•™ìƒë¸”ì€ ë©”ëª¨ë¦¬ë¥¼ ë” ë§ì´ ì‚¬ìš©í•˜ë¯€ë¡œ ì‘ê²Œ
        elif total_memory >= 4:  # 4GB ì´ìƒ
            return min(2, total_samples)
        else:
            return min(1, total_samples)
    else:
        return min(8, total_samples)

# ------------------ Main Test Function ------------------
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”¥ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    print("ğŸš€ **ì•™ìƒë¸” ëª¨ë¸ (V1 + V2) í…ŒìŠ¤íŠ¸ ì‹œì‘ - ìƒˆë¡œìš´ 702ê°œ ìƒ˜í”Œ**")
    print("="*60)

    # âœ… ì•™ìƒë¸” ëª¨ë¸ ê²½ë¡œ (í›ˆë ¨ ì™„ë£Œëœ ëª¨ë¸)
    ensemble_model_path = "./log/ensemble/best_speed_ensemble.pt"
    
    # âœ… ìƒˆë¡œ ìƒì„±ëœ 702ê°œ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ
    test_pkl_files = [
        "./preprocessed_data_full/pickle_labels/test/test_data.pkl"  # 702ê°œ ìƒ˜í”Œ
    ]

    # ë°ì´í„° ë¡œë“œ
    test_data_list = load_data(test_pkl_files)
    if len(test_data_list) == 0:
        print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    print(f"ğŸ“Š ì´ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_data_list):,}ê°œ")
    
    test_dataset = VideoFolderDataset(test_data_list)
    
    # âœ… ì•™ìƒë¸”ì— ìµœì í™”ëœ ë°°ì¹˜ ì‚¬ì´ì¦ˆ
    optimal_batch_size = get_optimal_batch_size(len(test_dataset), device)
    print(f"ğŸ¯ ì•™ìƒë¸” ìµœì  ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {optimal_batch_size}")
    
    test_loader = DataLoader(
        test_dataset, 
        batch_size=optimal_batch_size,
        shuffle=False, 
        num_workers=4, 
        pin_memory=True,
        drop_last=False
    )

    print(f"ğŸ“¦ ì´ {len(test_loader)}ê°œ ë°°ì¹˜ë¡œ ë¶„í• ")
    print(f"ğŸ¯ ì²˜ë¦¬ë  ì´ ìƒ˜í”Œ ìˆ˜: {len(test_dataset)}ê°œ")

    # âœ… ê°œë³„ ëª¨ë¸ë“¤ ì´ˆê¸°í™”
    cnn_v1 = CNNEncoderV1().to(device)
    model_v1 = EngagementModelV1(d_model=128, nhead=8, num_layers=3).to(device)
    cnn_v2 = CNNEncoderV2().to(device)
    model_v2 = EngagementModelV2(d_model=256, nhead=8, num_layers=4).to(device)

    # âœ… ê°œë³„ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
    print("\nğŸ”„ ê°œë³„ ëª¨ë¸ë“¤ ë¡œë“œ ì¤‘...")
    try:
        # Version 1 ëª¨ë¸ ë¡œë“œ
        v1_checkpoint = torch.load("./log/best_model2.pt", map_location=device)
        cnn_v1.load_state_dict(v1_checkpoint['cnn_state_dict'])
        model_v1.load_state_dict(v1_checkpoint['model_state_dict'])
        print("âœ… Version 1 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # Version 2 ëª¨ë¸ ë¡œë“œ
        v2_checkpoint = torch.load("./log/v2/best_model_v2.pt", map_location=device)
        cnn_v2.load_state_dict(v2_checkpoint['cnn_state_dict'])
        model_v2.load_state_dict(v2_checkpoint['model_state_dict'])
        print("âœ… Version 2 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ê°œë³„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ë‹¤ìŒ ê²½ë¡œë“¤ì„ í™•ì¸í•´ë³´ì„¸ìš”:")
        print("  - ./log/best_model2.pt (Version 1)")
        print("  - ./log/v2/best_model_v2.pt (Version 2)")
        return

    # âœ… ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ë° ë¡œë“œ
    ensemble_model = TransformerEnsembleModel(
        cnn_v1, model_v1, cnn_v2, model_v2, 
        ensemble_method='learned'
    ).to(device)

    if not os.path.exists(ensemble_model_path):
        print(f"âŒ ì•™ìƒë¸” ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ensemble_model_path}")
        print("ë‹¤ìŒ ê²½ë¡œë“¤ì„ í™•ì¸í•´ë³´ì„¸ìš”:")
        print("  - ./log/ensemble/best_speed_ensemble.pt")
        print("  - ./log/ensemble/best_weighted_ensemble.pt")
        print("  - ./log/ensemble/best_transformer_ensemble.pt")
        return

    print(f"ğŸ“‚ ì•™ìƒë¸” ëª¨ë¸ ë¡œë”© ì¤‘: {ensemble_model_path}")
    try:
        ensemble_checkpoint = torch.load(ensemble_model_path, map_location=device)
        ensemble_model.load_state_dict(ensemble_checkpoint['ensemble_state_dict'])
        
        # ì•™ìƒë¸” ëª¨ë¸ ì •ë³´ ì¶œë ¥
        training_accuracy = ensemble_checkpoint.get('accuracy', 0)
        training_f1 = ensemble_checkpoint.get('f1_score', 0)
        ensemble_method = ensemble_checkpoint.get('ensemble_method', 'learned')
        
        print(f"âœ… ì•™ìƒë¸” ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        print(f"   - í›ˆë ¨ ì •í™•ë„: {training_accuracy:.1%}")
        print(f"   - í›ˆë ¨ F1: {training_f1:.1%}")
        print(f"   - ì•™ìƒë¸” ë°©ë²•: {ensemble_method}")
        
        # í•™ìŠµëœ ê°€ì¤‘ì¹˜ ì¶œë ¥
        if ensemble_method == 'learned' and hasattr(ensemble_model, 'ensemble_weights'):
            weights = torch.softmax(ensemble_model.ensemble_weights, dim=0).detach().cpu().numpy()
            print(f"   - í•™ìŠµëœ ê°€ì¤‘ì¹˜: V1={weights[0]:.3f}, V2={weights[1]:.3f}")
            
    except Exception as e:
        print(f"âŒ ì•™ìƒë¸” ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return

    # ëª¨ë¸ì„ evaluation ëª¨ë“œë¡œ ì„¤ì •
    ensemble_model.eval()

    # âœ… ì¶”ë¡  ì‹œì‘
    all_probs, all_preds, all_labels = [], [], []
    total_processed = 0

    print(f"\nğŸ”„ ì•™ìƒë¸” ì¶”ë¡  ì‹œì‘ (ì´ {len(test_dataset)}ê°œ ìƒ˜í”Œ)...")
    with torch.no_grad():
        for batch_idx, (videos, fusion, labels) in enumerate(tqdm(test_loader, desc="Ensemble Test")):
            videos = videos.to(device, non_blocking=True)
            fusion = fusion.to(device, non_blocking=True)
            
            current_batch_size = videos.size(0)
            total_processed += current_batch_size
            
            # ì•™ìƒë¸” ì¶”ë¡ 
            logits = ensemble_model(videos, fusion)
            probs = torch.sigmoid(logits).squeeze(1).detach().cpu().numpy()
            
            # ê¸°ë³¸ ì„ê³„ê°’ 0.7ë¡œ ì˜ˆì¸¡
            preds = (probs >= 0.7).astype(np.int32)
            labels = labels.int().numpy()

            all_probs.extend(probs.tolist())
            all_preds.extend(preds.tolist())
            all_labels.extend(labels.tolist())
    
    print(f"âœ… ì´ {total_processed}ê°œ ìƒ˜í”Œ ì²˜ë¦¬ ì™„ë£Œ")

    # âœ… ê¸°ë³¸ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    acc = accuracy_score(all_labels, all_preds)
    rec = recall_score(all_labels, all_preds, zero_division=0)
    f1 = f1_score(all_labels, all_preds, zero_division=0)
    cm = confusion_matrix(all_labels, all_preds)

    print("\n" + "="*60)
    print("ğŸ“Š **ì•™ìƒë¸” í…ŒìŠ¤íŠ¸ ê²°ê³¼ (ì„ê³„ê°’ 0.7)**")
    print("="*60)
    print(f"âœ… Accuracy: {acc:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}")
    print("\nConfusion Matrix:")
    print(cm)
    
    # í´ë˜ìŠ¤ë³„ ë¶„í¬ ì¶œë ¥
    unique, counts = np.unique(all_labels, return_counts=True)
    print(f"\nğŸ“‹ ì‹¤ì œ ë¼ë²¨ ë¶„í¬: {dict(zip(unique, counts))}")
    unique, counts = np.unique(all_preds, return_counts=True)
    print(f"ğŸ“‹ ì˜ˆì¸¡ ë¼ë²¨ ë¶„í¬: {dict(zip(unique, counts))}")

    # ë°ì´í„° ì‚¬ìš©ë¥  í™•ì¸
    expected_samples = len(test_dataset)
    actual_samples = len(all_labels)
    usage_rate = (actual_samples / expected_samples) * 100
    print(f"\nğŸ“Š **ë°ì´í„° ì‚¬ìš©ë¥ **: {actual_samples}/{expected_samples} ({usage_rate:.1f}%)")

    # âœ… ì—¬ëŸ¬ ì„ê³„ê°’ìœ¼ë¡œ ìµœì í™”
    best_threshold = test_multiple_thresholds(all_probs, all_labels)
    
    # ìµœì  ì„ê³„ê°’ìœ¼ë¡œ ì¬ê³„ì‚°
    best_preds = (np.array(all_probs) >= best_threshold).astype(np.int32)
    best_acc = accuracy_score(all_labels, best_preds)
    best_rec = recall_score(all_labels, best_preds, zero_division=0)
    best_f1 = f1_score(all_labels, best_preds, zero_division=0)
    best_cm = confusion_matrix(all_labels, best_preds)

    print("\n" + "="*60)
    print(f"ğŸ“Š **ì•™ìƒë¸” ìµœì í™” ê²°ê³¼ (ì„ê³„ê°’ {best_threshold:.1f})**")
    print("="*60)
    print(f"ğŸ† Accuracy: {best_acc:.4f} | Recall: {best_rec:.4f} | F1: {best_f1:.4f}")
    
    # âœ… í˜¼ë™í–‰ë ¬ ì €ì¥
    save_dir = "./log/ensemble/test_results"
    os.makedirs(os.path.join(save_dir, "confusion_matrix"), exist_ok=True)
    
    # ê¸°ë³¸ ì„ê³„ê°’ í˜¼ë™í–‰ë ¬ (ë¼ë²¨ ìˆœì„œ ìˆ˜ì •: ì§‘ì¤‘ì•ˆí•¨=0, ì§‘ì¤‘í•¨=1)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["ì§‘ì¤‘ì•ˆí•¨", "ì§‘ì¤‘í•¨"])
    disp.plot(cmap=plt.cm.Blues)
    plt.title(f"Ensemble Confusion Matrix (Threshold 0.7)\n{actual_samples} samples from 702 dataset")
    out_path_basic = os.path.join(save_dir, "confusion_matrix", "ensemble_conf_matrix_702_basic.png")
    plt.savefig(out_path_basic, dpi=200, bbox_inches="tight")
    plt.close()
    
    # ìµœì  ì„ê³„ê°’ í˜¼ë™í–‰ë ¬
    disp_best = ConfusionMatrixDisplay(confusion_matrix=best_cm, display_labels=["ì§‘ì¤‘ì•ˆí•¨", "ì§‘ì¤‘í•¨"])
    disp_best.plot(cmap=plt.cm.Blues)
    plt.title(f"Ensemble Confusion Matrix (Optimal {best_threshold:.1f})\n{actual_samples} samples from 702 dataset")
    out_path_best = os.path.join(save_dir, "confusion_matrix", f"ensemble_conf_matrix_702_optimal_{best_threshold:.1f}.png")
    plt.savefig(out_path_best, dpi=200, bbox_inches="tight")
    plt.close()
    
    print(f"\nğŸ“Š Confusion matrices saved:")
    print(f"  - Basic (0.7): {out_path_basic}")
    print(f"  - Optimal ({best_threshold:.1f}): {out_path_best}")

    # âœ… ìµœì¢… ì„±ëŠ¥ ë¹„êµ
    print("\n" + "="*60)
    print("ğŸ“ˆ **ìµœì¢… ì„±ëŠ¥ ë¹„êµ (702ê°œ ìƒ˜í”Œ ê¸°ì¤€)**")
    print("="*60)
    print(f"ğŸ”¸ Version 1 (ê¸°ë³¸ Transformer): 72.5% ì •í™•ë„")
    print(f"ğŸ”¸ Version 2 (ê°œì„  Transformer): 76.9% ì •í™•ë„")
    print(f"ğŸ”¸ ì•™ìƒë¸” (í›ˆë ¨ ê²°ê³¼): {training_accuracy:.1%} ì •í™•ë„")
    print(f"ğŸ”¸ ì•™ìƒë¸” (í…ŒìŠ¤íŠ¸ ê¸°ë³¸): {acc:.1%} ì •í™•ë„")
    print(f"ğŸ”¸ ì•™ìƒë¸” (í…ŒìŠ¤íŠ¸ ìµœì ): {best_acc:.1%} ì •í™•ë„")
    
    improvement_vs_v2 = (best_acc - 0.769) * 100
    improvement_vs_v1 = (best_acc - 0.725) * 100
    
    print(f"\nğŸš€ **ì•™ìƒë¸” ê°œì„  íš¨ê³¼ (ìƒˆ ë°ì´í„° ê¸°ì¤€)**")
    print(f"   - vs Version 1: {improvement_vs_v1:+.1f}%p")
    print(f"   - vs Version 2: {improvement_vs_v2:+.1f}%p")
    print(f"   - ì¬í˜„ìœ¨: {best_rec:.1%}")
    print(f"   - F1-Score: {best_f1:.1%}")
    print(f"   - ì²˜ë¦¬ ìƒ˜í”Œ: {actual_samples:,}ê°œ")
    
    if best_acc > 0.90:
        print("ğŸ‰ 90% ì´ìƒ ë‹¬ì„±! ì•™ìƒë¸” ëª¨ë¸ì´ ìƒˆ ë°ì´í„°ì—ì„œ íƒì›”í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤!")
    elif best_acc > 0.85:
        print("ğŸŠ 85% ì´ìƒ ë‹¬ì„±! ì•™ìƒë¸” íš¨ê³¼ê°€ ìƒˆ ë°ì´í„°ì—ì„œ ë›°ì–´ë‚©ë‹ˆë‹¤!")
    elif best_acc > 0.80:
        print("âœ… 80% ì´ìƒ ë‹¬ì„±! ì•™ìƒë¸”ì´ ìƒˆ ë°ì´í„°ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ì‘ë™í–ˆìŠµë‹ˆë‹¤!")
    elif best_acc > 0.77:
        print("ğŸ‘ ì•™ìƒë¸” íš¨ê³¼ í™•ì¸! ìƒˆ ë°ì´í„°ì—ì„œë„ ê°œë³„ ëª¨ë¸ë³´ë‹¤ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("âš ï¸ ìƒˆë¡œìš´ ë°ì´í„°ì—ì„œ ì¼ë°˜í™” ì„±ëŠ¥ì´ ì œí•œì ì…ë‹ˆë‹¤. ë„ë©”ì¸ ì ì‘ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    print(f"\nğŸ¯ **í…ŒìŠ¤íŠ¸ ì™„ë£Œ ìš”ì•½**")
    print(f"   - ì‚¬ìš©ëœ ë°ì´í„°: ìƒˆë¡œ ìƒì„±í•œ 702ê°œ ìƒ˜í”Œ (27ê°œ ì˜ìƒ)")
    print(f"   - ì‹¤ì œ ì²˜ë¦¬: {actual_samples}ê°œ ìƒ˜í”Œ ({usage_rate:.1f}%)")
    print(f"   - ìµœê³  ì„±ëŠ¥: {best_acc:.1%} (ì„ê³„ê°’ {best_threshold})")

if __name__ == "__main__":
    main()
