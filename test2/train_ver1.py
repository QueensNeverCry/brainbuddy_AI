import os
import pickle
import cv2
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from tqdm import tqdm
from PIL import Image
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, recall_score, f1_score
import math  # Positional EncodingÏùÑ ÏúÑÌïú math Ï∂îÍ∞Ä
from torch.cuda.amp import autocast, GradScaler #Mixed Precision

# ------------------ Dataset (Í∏∞Ï°¥Í≥º ÎèôÏùº) ------------------
class VideoFolderDataset(Dataset):
    def __init__(self, data_list, transform=None):
        self.data_list = []
        self.transform = transform or transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])

        for folder_path, label in data_list:
            if os.path.isdir(folder_path):
                img_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                if len(img_files) >= 30:
                    self.data_list.append((folder_path, label))

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        folder_path, label = self.data_list[idx]
        img_files = sorted([f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])[:30]
        frames = []

        for f in img_files:
            img_path = os.path.join(folder_path, f)
            
            try:
                img_pil = Image.open(img_path).convert('RGB')
                frames.append(self.transform(img_pil))
            except Exception as e:
                print(f"‚ö†Ô∏è Ïù¥ÎØ∏ÏßÄ Î°úÎìú Ïã§Ìå®: {img_path}")
                continue
        
        
        while len(frames) < 30:
            frames.append(frames[-1] if frames else torch.zeros(3, 224, 224))
        
        video = torch.stack(frames[:30])  # Ï†ïÌôïÌûà 30Í∞úÎßå

        fusion_path = os.path.join(folder_path, "fusion_features.pkl")
        if os.path.exists(fusion_path):
            with open(fusion_path, 'rb') as f:
                fusion = torch.tensor(pickle.load(f), dtype=torch.float32)
        else:
            fusion = torch.zeros(5)

        return video, fusion, torch.tensor(label, dtype=torch.float32)

# ------------------ Model (CNNÏùÄ Í∏∞Ï°¥Í≥º ÎèôÏùº) ------------------
class CNNEncoder(nn.Module):
    def __init__(self, output_dim=1280):
        super().__init__()
        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)
        self.features = mobilenet.features
        self.avgpool = nn.AdaptiveAvgPool2d((4, 4))
        self.fc = nn.Sequential(
            nn.Linear(1280 * 4 * 4, 2048),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(2048, output_dim),
            nn.ReLU()
        )

    def forward(self, x):
        B, T, C, H, W = x.shape
        x = x.view(B * T, C, H, W)
        x = self.features(x)
        x = self.avgpool(x)
        x = x.view(B * T, -1)
        x = self.fc(x)
        return x.view(B, T, -1)

# ------------------ ÏÉàÎ°úÏö¥ Transformer Í∏∞Î∞ò Î™®Îç∏ ------------------
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        seq_len = x.size(0)
        return x + self.pe[:seq_len, :].to(x.device)

class EngagementModel(nn.Module):
    def __init__(self, cnn_feat_dim=1280, fusion_feat_dim=5, d_model=128, nhead=8, num_layers=3):
        super().__init__()
        
        # ÏûÖÎ†• ÌîÑÎ°úÏ†ùÏÖò: CNN ÌäπÏßïÏùÑ Transformer Ï∞®ÏõêÏúºÎ°ú Î≥ÄÌôò
        self.input_projection = nn.Linear(cnn_feat_dim, d_model)
        
        # Positional Encoding
        self.pos_encoder = PositionalEncoding(d_model)
        
        # Transformer Encoder Layer
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=0.1,
            activation='relu',
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # ÏãúÌÄÄÏä§ ÏßëÏïΩÏùÑ ÏúÑÌïú Global Average Pooling
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        
        # ÏµúÏ¢Ö Î∂ÑÎ•òÍ∏∞
        self.fc = nn.Sequential(
            nn.Linear(d_model + fusion_feat_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 1)
        )

    def forward(self, cnn_feats, fusion_feats):
        # ÏûÖÎ†• ÌîÑÎ°úÏ†ùÏÖò
        x = self.input_projection(cnn_feats)  # (B, T, d_model)
        
        # Positional Encoding Ï∂îÍ∞Ä (ÏãúÌÄÄÏä§ ÏàúÏÑú Ï†ïÎ≥¥)
        x = x.transpose(0, 1)  # (T, B, d_model)
        x = self.pos_encoder(x)
        x = x.transpose(0, 1)  # (B, T, d_model)
        
        # Transformer Encoder
        transformer_out = self.transformer_encoder(x)  # (B, T, d_model)
        
        # Global Average PoolingÏúºÎ°ú ÏãúÌÄÄÏä§ ÏßëÏïΩ
        pooled = self.global_pool(transformer_out.transpose(1, 2)).squeeze(-1)  # (B, d_model)
        
        # Fusion features Í≤∞Ìï©
        combined = torch.cat([pooled, fusion_feats], dim=1)  # (B, d_model + 5)
        
        # ÏµúÏ¢Ö Ï∂úÎ†•
        return self.fc(combined)

# ------------------ Training Functions (Í∏∞Ï°¥Í≥º ÎèôÏùº) ------------------
def train(model_cnn, model_top, loader, criterion, optimizer, device, scaler, accumulation_steps=4):
    model_cnn.train()
    model_top.train()
    total_loss = 0
    optimizer.zero_grad()

    for i, (videos, fusion, labels) in enumerate(tqdm(loader, desc="Train")):
        # üî• non_blockingÏúºÎ°ú GPU Ï†ÑÏÜ° ÏµúÏ†ÅÌôî
        videos = videos.to(device, non_blocking=True)
        fusion = fusion.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True).unsqueeze(1)

        # üî• Mixed Precision Ï†ÅÏö© - autocastÎ°ú Í∞êÏã∏Í∏∞
        with autocast():
            features = model_cnn(videos)
            output = model_top(features, fusion)
            loss = criterion(output, labels)

        # üî• Í∏∞Ï°¥ loss.backward()Î•º scalerÎ°ú Î≥ÄÍ≤Ω
        scaler.scale(loss).backward()
        total_loss += loss.item()

        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(loader):
            # üî• Í∑∏ÎûòÎîîÏñ∏Ìä∏ ÌÅ¥Î¶¨ÌïëÎèÑ scalerÏôÄ Ìï®Íªò ÏÇ¨Ïö©
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(
                list(model_cnn.parameters()) + list(model_top.parameters()), 
                max_norm=1.0
            )
            # üî• optimizer.step()ÏùÑ scalerÎ°ú Î≥ÄÍ≤Ω
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()

    return total_loss / len(loader)

def validate(model_cnn, model_top, loader, criterion, device):
    model_cnn.eval()
    model_top.eval()
    total_loss = 0

    with torch.no_grad():
        for videos, fusion, labels in tqdm(loader, desc="Validation"):
            # üî• non_blocking ÏµúÏ†ÅÌôî
            videos = videos.to(device, non_blocking=True)
            fusion = fusion.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True).unsqueeze(1)
            
            # üî• Mixed Precision Ï†ÅÏö©
            with autocast():
                features = model_cnn(videos)
                outputs = model_top(features, fusion)
                loss = criterion(outputs, labels)
            total_loss += loss.item()

    return total_loss / len(loader)

def load_data(pkl_files):
    all_data = []
    for pkl_path in pkl_files:
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f)
            all_data.extend(data)
    
    # üî• Î°úÎî© ÌõÑ Ï¶âÏãú ÏÖîÌîåÎßÅ
    import random
    random.shuffle(all_data)
    return all_data


def check_batch_distribution(loader, num_batches=5):
    """Î∞∞ÏπòÎ≥Ñ ÌÅ¥ÎûòÏä§ Î∂ÑÌè¨ ÌôïÏù∏"""
    print("=" * 50)
    print("Î∞∞ÏπòÎ≥Ñ ÌÅ¥ÎûòÏä§ Î∂ÑÌè¨ ÌôïÏù∏")
    print("=" * 50)
    
    for i, (videos, fusion, labels) in enumerate(loader):
        if i >= num_batches:
            break
        
        class_0_count = (labels == 0).sum().item()
        class_1_count = (labels == 1).sum().item()
        total = len(labels)
        
        print(f"Batch {i+1}: Class 0: {class_0_count}/{total} ({class_0_count/total:.1%}) | Class 1: {class_1_count}/{total} ({class_1_count/total:.1%})")
    
    print("=" * 50)

def evaluate_and_save_confusion_matrix(model_cnn, model_top, loader, device, epoch):
    model_cnn.eval()
    model_top.eval()

    all_preds = []
    all_labels = []

    with torch.no_grad():
        for videos, fusion, labels in loader:
            videos, fusion = videos.to(device), fusion.to(device)
            features = model_cnn(videos)
            outputs = model_top(features, fusion)
            preds = (torch.sigmoid(outputs) > 0.3).int().cpu().numpy()
            labels = labels.int().numpy()
            all_preds.extend(preds.flatten())
            all_labels.extend(labels.flatten())

    cm = confusion_matrix(all_labels, all_preds)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])
    disp.plot(cmap=plt.cm.Blues)
    plt.title(f"Confusion Matrix - Epoch {epoch+1}")
    plt.savefig(f"./log/confusion_matrix/train1/conf_matrix_epoch_{epoch+1}.png")
    plt.close()
    print(f"üìä Confusion matrix saved: conf_matrix_epoch_{epoch+1}.png")

def evaluate_metrics(model_cnn, model_top, loader, device):
    model_cnn.eval()
    model_top.eval()

    all_preds = []
    all_labels = []

    with torch.no_grad():
        for videos, fusion, labels in loader:
            videos, fusion = videos.to(device), fusion.to(device)
            features = model_cnn(videos)
            outputs = model_top(features, fusion)
            preds = (torch.sigmoid(outputs) > 0.3).int().cpu().numpy()
            labels = labels.int().numpy()
            all_preds.extend(preds.flatten())
            all_labels.extend(labels.flatten())

    recall = recall_score(all_labels, all_preds, zero_division=0)
    f1 = f1_score(all_labels, all_preds, zero_division=0)
    return recall, f1

# ------------------ Main Function (Í∏∞Ï°¥Í≥º ÎèôÏùº, Î™®Îç∏ Ï¥àÍ∏∞Ìôî Î∂ÄÎ∂ÑÎßå Î≥ÄÍ≤Ω) ------------------
def main():
    torch.backends.cudnn.benchmark = True  # ÏÑ±Îä• Ìñ•ÏÉÅ
    torch.cuda.empty_cache()  # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üî• ÏÇ¨Ïö© ÎîîÎ∞îÏù¥Ïä§: {device}")
    
    # GPU Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ ÌôïÏù∏
    if torch.cuda.is_available():
        print(f"üìä GPU: {torch.cuda.get_device_name(0)}")
        print(f"üìä GPU Î©îÎ™®Î¶¨: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        
    # ÏàòÏ†ïÎêú Í≤ΩÎ°ú: ÏÇ¨Ïö©ÏûêÍ∞Ä Ï†úÍ≥µÌïú Desktop Í∏∞Î∞ò Í≤ΩÎ°ú ÏÇ¨Ïö©
    base_path = r"C:\Users\user\Desktop\brainbuddy_AI\preprocess2\pickle_labels"
    train_pkl_files = [
        f"{base_path}\\train\\20_01.pkl",
        f"{base_path}\\train\\20_03.pkl"
    ]
    val_pkl_files = [
        f"{base_path}\\valid\\20_01.pkl",
        f"{base_path}\\valid\\20_03.pkl"
    ]

    train_data_list = load_data(train_pkl_files)
    val_data_list = load_data(val_pkl_files)

    train_dataset = VideoFolderDataset(train_data_list)
    val_dataset = VideoFolderDataset(val_data_list)

    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=8, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=8, pin_memory=True)

    print("üîç Training Îç∞Ïù¥ÌÑ∞ Î∞∞Ïπò Î∂ÑÌè¨ ÌôïÏù∏:")
    check_batch_distribution(train_loader, num_batches=3)
    
    print("üîç Validation Îç∞Ïù¥ÌÑ∞ Î∞∞Ïπò Î∂ÑÌè¨ ÌôïÏù∏:")
    check_batch_distribution(val_loader, num_batches=3)

    # Î™®Îç∏ Ï¥àÍ∏∞Ìôî (Transformer Í∏∞Î∞òÏúºÎ°ú Î≥ÄÍ≤Ω)
    cnn = CNNEncoder().to(device)
    model = EngagementModel(d_model=128, nhead=8, num_layers=4).to(device)
    pos_weight = torch.tensor([1.2]).to(device)  # ÏÜåÏàò ÌÅ¥ÎûòÏä§Ïóê Îçî ÎÜíÏùÄ Í∞ÄÏ§ëÏπò
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    optimizer = torch.optim.Adam(list(cnn.parameters()) + list(model.parameters()), lr=1e-5) 
    scaler = GradScaler()  # Mixed PrecisionÏùÑ ÏúÑÌïú Scaler ÏÉùÏÑ±
    
    best_val_loss = float('inf')
    best_model_path = "./log/best_model2.pt"
    checkpoint_path = "./log/last_checkpoint2.pt"
    log_history = []

    start_epoch = 0
    patience = 3
    patience_counter = 0

    """
    if os.path.exists(checkpoint_path):
        print(f"üîÑ Resuming training from {checkpoint_path}")
        ckpt = torch.load(checkpoint_path, map_location=device)
        cnn.load_state_dict(ckpt['cnn_state_dict'])
        model.load_state_dict(ckpt['model_state_dict'])
        optimizer.load_state_dict(ckpt['optimizer_state_dict'])
        start_epoch = ckpt['epoch'] + 1
        best_val_loss = ckpt.get('best_val_loss', float('inf'))
        print(f"Resumed from epoch {start_epoch} (best_val_loss={best_val_loss:.4f})")
    """

    num_epochs = 10
    for epoch in range(start_epoch, num_epochs):
        train_loss = train(cnn, model, train_loader, criterion, optimizer, device, scaler, accumulation_steps=4)
        val_loss = validate(cnn, model, val_loader, criterion, device)

        print(f"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

        recall, f1 = evaluate_metrics(cnn, model, val_loader, device)

        log_history.append({
            "epoch": epoch + 1,
            "train_loss": train_loss,
            "val_loss": val_loss,
            "recall": recall,
            "f1_score": f1
        })

        evaluate_and_save_confusion_matrix(cnn, model, val_loader, device, epoch)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save({
                'cnn_state_dict': cnn.state_dict(),
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'epoch': epoch,
                'val_loss': val_loss
            }, best_model_path)
            print(f"‚úÖ Best model saved at epoch {epoch+1} with val_loss {val_loss:.4f}")
        else:
            patience_counter += 1
            print(f"Early stopping patience {patience_counter}/{patience}")
            if patience_counter >= patience:
                print(f"==== Early stopping Triggered===")
                break

        torch.save({
            'cnn_state_dict': cnn.state_dict(),
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'epoch': epoch,
            'best_val_loss': best_val_loss
        }, checkpoint_path)
        print(f"üíæ Checkpoint saved at epoch {epoch+1}")

    log_df = pd.DataFrame(log_history)
    log_df.to_csv("./log/train_log2.csv", index=False)
    print("üìÑ Training log saved to train_log.csv")

    checkpoint = torch.load(best_model_path, map_location=device)
    cnn.load_state_dict(checkpoint['cnn_state_dict'])
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    print(f"üîÅ Loaded best model from epoch {checkpoint['epoch']+1} (val_loss={checkpoint['val_loss']:.4f})")

if __name__ == '__main__':
    main()
