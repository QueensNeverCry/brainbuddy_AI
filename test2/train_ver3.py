# ver1+ver2 ensemble ì ìš©
import os
import pickle
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from tqdm import tqdm
from PIL import Image
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, recall_score, f1_score, accuracy_score
import math
import numpy as np
from torch.cuda.amp import autocast, GradScaler
import time
import torch.nn.functional as F

# ------------------ ìµœì í™”ëœ Dataset ------------------
class VideoFolderDataset(Dataset):
    def __init__(self, data_list, transform=None, is_training=True):
        self.data_list = []
        self.is_training = is_training
        
        # ìµœì†Œí•œì˜ ë³€í™˜ìœ¼ë¡œ ì†ë„ ìµœëŒ€í™”
        if is_training:
            self.transform = transform or transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.RandomHorizontalFlip(p=0.2),  # í™•ë¥  ë‚®ì¶¤
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
            ])
        else:
            self.transform = transform or transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
            ])

        for folder_path, label in data_list:
            if os.path.isdir(folder_path):
                img_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
                if len(img_files) >= 30:
                    self.data_list.append((folder_path, label))

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        folder_path, label = self.data_list[idx]
        img_files = sorted([f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])[:30]
        frames = []

        for f in img_files:
            img_path = os.path.join(folder_path, f)
            try:
                img_pil = Image.open(img_path).convert('RGB')
                frames.append(self.transform(img_pil))
            except Exception:
                continue
        
        while len(frames) < 30:
            frames.append(frames[-1] if frames else torch.zeros(3, 224, 224))
        
        video = torch.stack(frames[:30])

        fusion_path = os.path.join(folder_path, "fusion_features.pkl")
        if os.path.exists(fusion_path):
            with open(fusion_path, 'rb') as f:
                fusion = torch.tensor(pickle.load(f), dtype=torch.float32)
        else:
            fusion = torch.zeros(5)

        return video, fusion, torch.tensor(label, dtype=torch.float32)

# ------------------ ìµœì í™”ëœ CNNEncoder ------------------
class CNNEncoderV1(nn.Module):
    def __init__(self, output_dim=1280):
        super().__init__()
        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)
        self.features = mobilenet.features
        self.avgpool = nn.AdaptiveAvgPool2d((4, 4))
        self.fc = nn.Sequential(
            nn.Linear(1280 * 4 * 4, 2048),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(2048, output_dim),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        B, T, C, H, W = x.shape
        x = x.view(B * T, C, H, W)
        x = self.features(x)
        x = self.avgpool(x)
        x = x.view(B * T, -1)
        x = self.fc(x)
        return x.view(B, T, -1)

class CNNEncoderV2(nn.Module):
    def __init__(self, output_dim=1280):
        super().__init__()
        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)
        self.features = mobilenet.features
        self.avgpool = nn.AdaptiveAvgPool2d((4, 4))
        self.fc = nn.Sequential(
            nn.Linear(1280 * 4 * 4, 2048),
            nn.BatchNorm1d(2048),
            nn.ReLU(inplace=True),
            nn.Dropout(0.4),
            nn.Linear(2048, output_dim),
            nn.BatchNorm1d(output_dim),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        B, T, C, H, W = x.shape
        x = x.view(B * T, C, H, W)
        x = self.features(x)
        x = self.avgpool(x)
        x = x.view(B * T, -1)
        x = self.fc(x)
        return x.view(B, T, -1)

# ------------------ Positional Encoding ------------------
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        seq_len = x.size(0)
        return x + self.pe[:seq_len, :].to(x.device)

# ------------------ ìµœì í™”ëœ Version 1 Transformer ------------------
class EngagementModelV1(nn.Module):
    def __init__(self, cnn_feat_dim=1280, fusion_feat_dim=5, d_model=128, nhead=8, num_layers=3):
        super().__init__()
        
        self.input_projection = nn.Linear(cnn_feat_dim, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=0.1,
            activation='relu',
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        
        self.fc = nn.Sequential(
            nn.Linear(d_model + fusion_feat_dim, 64),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(64, 1)
        )

    def forward(self, cnn_feats, fusion_feats):
        x = self.input_projection(cnn_feats)
        x = x.transpose(0, 1)
        x = self.pos_encoder(x)
        x = x.transpose(0, 1)
        transformer_out = self.transformer_encoder(x)
        pooled = self.global_pool(transformer_out.transpose(1, 2)).squeeze(-1)
        combined = torch.cat([pooled, fusion_feats], dim=1)
        return self.fc(combined)

# ------------------ ìµœì í™”ëœ Version 2 Transformer ------------------
class EngagementModelV2(nn.Module):
    def __init__(self, cnn_feat_dim=1280, fusion_feat_dim=5, d_model=256, nhead=8, num_layers=4):
        super().__init__()
        
        self.input_projection = nn.Sequential(
            nn.Linear(cnn_feat_dim, d_model),
            nn.LayerNorm(d_model),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1)
        )
        
        self.pos_encoder = PositionalEncoding(d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=0.15,
            activation='gelu',
            batch_first=True,
            norm_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)
        self.global_max_pool = nn.AdaptiveMaxPool1d(1)
        
        self.fc = nn.Sequential(
            nn.Linear(d_model * 2 + fusion_feat_dim, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(512, 128),
            nn.LayerNorm(128),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(128, 1)
        )

    def forward(self, cnn_feats, fusion_feats):
        x = self.input_projection(cnn_feats)
        x = x.transpose(0, 1)
        x = self.pos_encoder(x)
        x = x.transpose(0, 1)
        transformer_out = self.transformer_encoder(x)
        
        avg_pooled = self.global_avg_pool(transformer_out.transpose(1, 2)).squeeze(-1)
        max_pooled = self.global_max_pool(transformer_out.transpose(1, 2)).squeeze(-1)
        pooled = torch.cat([avg_pooled, max_pooled], dim=1)
        
        combined = torch.cat([pooled, fusion_feats], dim=1)
        return self.fc(combined)

# ------------------ ì†ë„ ìµœì í™”ëœ ì•™ìƒë¸” ëª¨ë¸ ------------------
class TransformerEnsembleModel(nn.Module):
    def __init__(self, cnn_v1, model_v1, cnn_v2, model_v2, ensemble_method='weighted'):
        super().__init__()
        self.cnn_v1 = cnn_v1
        self.model_v1 = model_v1
        self.cnn_v2 = cnn_v2
        self.model_v2 = model_v2
        self.ensemble_method = ensemble_method
        
        if ensemble_method == 'weighted':
            self.register_buffer('weights', torch.tensor([0.3, 0.7]))
        elif ensemble_method == 'learned':
            self.ensemble_weights = nn.Parameter(torch.tensor([0.3, 0.7]))
            self.ensemble_fc = nn.Sequential(
                nn.Linear(2, 16),
                nn.ReLU(inplace=True),
                nn.Dropout(0.2),
                nn.Linear(16, 1)
            )

    def forward(self, videos, fusion_feats):
        # Mixed Precisionê³¼ í•¨ê»˜ ë³‘ë ¬ ì²˜ë¦¬
        feats_v1 = self.cnn_v1(videos)
        feats_v2 = self.cnn_v2(videos)
        
        logits_v1 = self.model_v1(feats_v1, fusion_feats)
        logits_v2 = self.model_v2(feats_v2, fusion_feats)
        
        if self.ensemble_method == 'weighted':
            return self.weights[0] * logits_v1 + self.weights[1] * logits_v2
        elif self.ensemble_method == 'learned':
            prob_v1 = torch.sigmoid(logits_v1)
            prob_v2 = torch.sigmoid(logits_v2)
            normalized_weights = torch.softmax(self.ensemble_weights, dim=0)
            weighted_v1 = prob_v1 * normalized_weights[0]
            weighted_v2 = prob_v2 * normalized_weights[1]
            combined_input = torch.cat([weighted_v1, weighted_v2], dim=1)
            return self.ensemble_fc(combined_input)

# ------------------ ê³ ì† í›ˆë ¨ í•¨ìˆ˜ (Mixed Precision + Gradient Accumulation) ------------------
def train_ensemble_speed(ensemble_model, loader, criterion, optimizer, device, scaler, accumulation_steps=4):
    ensemble_model.train()
    # ì‚¬ì „í›ˆë ¨ ëª¨ë¸ë“¤ì€ eval ëª¨ë“œ ìœ ì§€
    ensemble_model.cnn_v1.eval()
    ensemble_model.model_v1.eval()
    ensemble_model.cnn_v2.eval()
    ensemble_model.model_v2.eval()
    
    total_loss = 0
    batch_count = 0
    optimizer.zero_grad()

    for i, (videos, fusion, labels) in enumerate(tqdm(loader, desc="High-Speed Ensemble Train")):
        videos = videos.to(device, non_blocking=True)
        fusion = fusion.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True).unsqueeze(1)

        # Mixed Precision ì ìš©
        with autocast():
            output = ensemble_model(videos, fusion)
            loss = criterion(output, labels) / accumulation_steps  # gradient accumulationìš© ìŠ¤ì¼€ì¼ë§

        scaler.scale(loss).backward()
        
        # Gradient Accumulationìœ¼ë¡œ effective batch size ì¦ê°€
        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(loader):
            scaler.unscale_(optimizer)
            # Gradient clipping (ì„ íƒì‚¬í•­)
            if ensemble_model.ensemble_method == 'learned':
                torch.nn.utils.clip_grad_norm_(
                    list(ensemble_model.ensemble_weights) + list(ensemble_model.ensemble_fc.parameters()), 
                    max_norm=1.0
                )
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
        
        total_loss += loss.item() * accumulation_steps
        batch_count += 1

    return total_loss / batch_count

def validate_ensemble_speed(ensemble_model, loader, criterion, device, max_batches=None):
    ensemble_model.eval()
    total_loss = 0
    batch_count = 0

    with torch.no_grad():
        for i, (videos, fusion, labels) in enumerate(tqdm(loader, desc="High-Speed Validation")):
            if max_batches and i >= max_batches:
                break
                
            videos = videos.to(device, non_blocking=True)
            fusion = fusion.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True).unsqueeze(1)
            
            # Mixed Precision ì ìš©
            with autocast():
                outputs = ensemble_model(videos, fusion)
                loss = criterion(outputs, labels)
            
            total_loss += loss.item()
            batch_count += 1

    return total_loss / batch_count

def load_data(pkl_files):
    all_data = []
    for pkl_path in pkl_files:
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f)
            all_data.extend(data)
    
    import random
    random.shuffle(all_data)
    return all_data

def evaluate_ensemble_speed(ensemble_model, loader, device, threshold=0.7, max_batches=200):
    ensemble_model.eval()
    
    all_preds = []
    all_labels = []
    batch_count = 0
    
    with torch.no_grad():
        for videos, fusion, labels in tqdm(loader, desc="Fast Evaluation"):
            if batch_count >= max_batches:
                break
                
            videos, fusion = videos.to(device, non_blocking=True), fusion.to(device, non_blocking=True)
            
            with autocast():
                outputs = ensemble_model(videos, fusion)
                probs = torch.sigmoid(outputs).cpu().numpy()
            
            # í›„ì²˜ë¦¬ ë³´ì • ë¡œì§ (ì´ë™í‰ê·  ìŠ¤ë¬´ë”©)
            p = probs.flatten()
            k = 3
            kernel = np.ones(k)/k
            p_smooth = np.convolve(p, kernel, mode='same')
            preds = (p_smooth > threshold).astype(int)

            
            all_preds.extend(preds.flatten())
            all_labels.extend(labels.int().numpy().flatten())
            batch_count += 1
    
    if len(all_labels) > 0:
        accuracy = accuracy_score(all_labels, all_preds)
        recall = recall_score(all_labels, all_preds, zero_division=0)
        f1 = f1_score(all_labels, all_preds, zero_division=0)
        return accuracy, recall, f1
    else:
        return 0, 0, 0

# ------------------ ë©”ì¸ í•¨ìˆ˜ (ì†ë„ ìµœì í™”) ------------------
def main():
    # ìµœëŒ€ GPU ìµœì í™” ì„¤ì •
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.cuda.empty_cache()
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    print(" **ì†ë„ ìµœì í™”ëœ ì „ì²´ ë°ì´í„° ì•™ìƒë¸” í›ˆë ¨**")
    print("="*70)
    
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ìµœëŒ€í™”
        torch.cuda.set_per_process_memory_fraction(0.95)
    
    ensemble_dir = "./log/ensemble"
    os.makedirs(ensemble_dir, exist_ok=True)
    
    # ì „ì²´ ë°ì´í„° ì‚¬ìš©
    base_path = r"C:\Users\user\Desktop\brainbuddy_AI\preprocess2\pickle_labels"
    
    train_pkl_files = [
        f"{base_path}\\train\\20_01.pkl",
        f"{base_path}\\train\\20_03.pkl"  # ì „ì²´ í›ˆë ¨ ë°ì´í„° ì‚¬ìš©
    ]
    val_pkl_files = [
        f"{base_path}\\valid\\20_01.pkl",
        f"{base_path}\\valid\\20_03.pkl"   # ì „ì²´ ê²€ì¦ ë°ì´í„° ì‚¬ìš©
    ]

    train_data_list = load_data(train_pkl_files)
    val_data_list = load_data(val_pkl_files)

    # ì „ì²´ ë°ì´í„° ì‚¬ìš© (ìƒ˜í”Œë§ ì—†ìŒ)
    train_dataset = VideoFolderDataset(train_data_list, is_training=True)
    val_dataset = VideoFolderDataset(val_data_list, is_training=False)

    # í° ë°°ì¹˜ í¬ê¸° + ìµœì í™” ì„¤ì •
    batch_size = 8  # GPU ë©”ëª¨ë¦¬ê°€ í—ˆìš©í•˜ëŠ” ìµœëŒ€ í¬ê¸°
    accumulation_steps = 4  # effective batch size = 8 * 4 = 32
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size,
        shuffle=True, 
        num_workers=8,  # ì›Œì»¤ ìˆ˜ ì¦ê°€
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=4,  # ë¯¸ë¦¬ ë¡œë”© ìˆ˜ ì¦ê°€
        drop_last=True  # ë§ˆì§€ë§‰ ë°°ì¹˜ ë“œë¡­ìœ¼ë¡œ ì†ë„ í–¥ìƒ
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size,
        shuffle=False, 
        num_workers=8,
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=4,
        drop_last=False
    )

    print(f"í›ˆë ¨ ë°ì´í„°: {len(train_data_list):,}ê°œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(val_data_list):,}ê°œ")
    print(f"í›ˆë ¨ ë°°ì¹˜: {len(train_loader):,}ê°œ (ë°°ì¹˜í¬ê¸°: {batch_size}, Accumulation: {accumulation_steps})")
    print(f"Effective Batch Size: {batch_size * accumulation_steps}")

    # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ë“¤ ë¡œë“œ
    print("\n Transformer ëª¨ë¸ë“¤ ë¡œë“œ ì¤‘...")
    
    # Version 1 ëª¨ë¸ ë¡œë“œ
    cnn_v1 = CNNEncoderV1().to(device)
    model_v1 = EngagementModelV1(d_model=128, nhead=8, num_layers=3).to(device)
    
    try:
        v1_checkpoint = torch.load("./log/best_model2.pt", map_location=device)
        cnn_v1.load_state_dict(v1_checkpoint['cnn_state_dict'])
        model_v1.load_state_dict(v1_checkpoint['model_state_dict'])
        print("Version 1 Transformer ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"Version 1 ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # Version 2 ëª¨ë¸ ë¡œë“œ
    cnn_v2 = CNNEncoderV2().to(device)
    model_v2 = EngagementModelV2(d_model=256, nhead=8, num_layers=4).to(device)
    
    try:
        v2_checkpoint = torch.load("./log/v2/best_model_v2.pt", map_location=device)
        cnn_v2.load_state_dict(v2_checkpoint['cnn_state_dict'])
        model_v2.load_state_dict(v2_checkpoint['model_state_dict'])
        print("Version 2 Transformer ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"Version 2 ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ë“¤ì„ ê³ ì •
    for param in cnn_v1.parameters():
        param.requires_grad = False
    for param in model_v1.parameters():
        param.requires_grad = False
    for param in cnn_v2.parameters():
        param.requires_grad = False
    for param in model_v2.parameters():
        param.requires_grad = False
    
    cnn_v1.eval()
    model_v1.eval()
    cnn_v2.eval()
    model_v2.eval()
    
    # í•™ìŠµ ê°€ëŠ¥í•œ ì•™ìƒë¸” ëª¨ë¸
    ensemble_model = TransformerEnsembleModel(
        cnn_v1, model_v1, cnn_v2, model_v2, 
        ensemble_method='learned'  # í•™ìŠµ ê°€ëŠ¥í•œ ì•™ìƒë¸”
    ).to(device)
    
    # Loss & Optimizer
    pos_weight = torch.tensor([1.2]).to(device)
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    
    # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë§Œ ì˜µí‹°ë§ˆì´ì €ì— ì¶”ê°€
    trainable_params = []
    trainable_params.append(ensemble_model.ensemble_weights)
    trainable_params.extend(list(ensemble_model.ensemble_fc.parameters()))
    
    optimizer = torch.optim.AdamW(trainable_params, lr=5e-4, weight_decay=1e-3)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=4, eta_min=1e-6)
    scaler = GradScaler()
    
    # ì €ì¥ ê²½ë¡œ
    best_model_path = f"{ensemble_dir}/best_speed_ensemble.pt"
    checkpoint_path = f"{ensemble_dir}/last_speed_checkpoint.pt"
    log_history = []

    print(f"\nğŸ“ˆ **ì†ë„ ìµœì í™” ì•™ìƒë¸” ì„¤ì •**")
    print(f"   - ì•™ìƒë¸” ë°©ë²•: Learned Weighting")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size} Ã— Accumulation {accumulation_steps} = Effective {batch_size * accumulation_steps}")
    print(f"   - ì—í¬í¬: 4 (ì†ë„ ìš°ì„ )")
    print(f"   - Mixed Precision: í™œì„±í™”")
    print(f"   - ì˜ˆìƒ ì‹œê°„: 1-1.5ì‹œê°„/ì—í¬í¬")
    print("="*70)

    # í›ˆë ¨ ë£¨í”„
    start_epoch = 0
    patience = 2
    patience_counter = 0
    best_val_loss = float('inf')
    num_epochs = 4  # ì†ë„ë¥¼ ìœ„í•´ ì—í¬í¬ ìˆ˜ ì¤„ì„

    for epoch in range(start_epoch, num_epochs):
        current_lr = scheduler.get_last_lr()[0] if scheduler.get_last_lr() else 5e-4
        
        # í˜„ì¬ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì¶œë ¥
        weights = torch.softmax(ensemble_model.ensemble_weights, dim=0).detach().cpu().numpy()
        
        print(f"\n[Epoch {epoch+1}/{num_epochs}] LR: {current_lr:.2e}, Weights: V1={weights[0]:.3f}, V2={weights[1]:.3f}")
        
        # í›ˆë ¨ ì‹œê°„ ì¸¡ì •
        train_start = time.time()
        train_loss = train_ensemble_speed(ensemble_model, train_loader, criterion, optimizer, device, scaler, accumulation_steps)
        train_time = time.time() - train_start
        
        # ê²€ì¦ ì‹œê°„ ì¸¡ì • (ì „ì²´ ê²€ì¦ ë°ì´í„° ì‚¬ìš©)
        val_start = time.time()
        val_loss = validate_ensemble_speed(ensemble_model, val_loader, criterion, device)
        val_time = time.time() - val_start
        
        print(f"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} ({train_time/60:.1f}ë¶„)")
        print(f"[Epoch {epoch+1}] Val Loss: {val_loss:.4f} ({val_time/60:.1f}ë¶„)")

        # ë¹ ë¥¸ ì„±ëŠ¥ í‰ê°€ (200 ë°°ì¹˜ë§Œ ìƒ˜í”Œë§)
        eval_start = time.time()
        accuracy, recall, f1 = evaluate_ensemble_speed(ensemble_model, val_loader, device, threshold=0.7, max_batches=200)
        eval_time = time.time() - eval_start
        
        print(f"[Epoch {epoch+1}] Metrics: Acc={accuracy:.4f}, Rec={recall:.4f}, F1={f1:.4f} (â±ï¸{eval_time:.1f}ì´ˆ)")

        total_epoch_time = train_time + val_time + eval_time
        print(f"[Epoch {epoch+1}] ì´ ì†Œìš” ì‹œê°„: {total_epoch_time/60:.1f}ë¶„")

        log_history.append({
            "epoch": epoch + 1,
            "train_loss": train_loss,
            "val_loss": val_loss,
            "accuracy": accuracy,
            "recall": recall,
            "f1_score": f1,
            "learning_rate": current_lr,
            "v1_weight": weights[0],
            "v2_weight": weights[1],
            "train_time_min": train_time/60,
            "val_time_min": val_time/60,
            "total_time_min": total_epoch_time/60
        })

        # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save({
                'ensemble_state_dict': ensemble_model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'epoch': epoch,
                'val_loss': val_loss,
                'accuracy': accuracy,
                'recall': recall,
                'f1_score': f1,
                'ensemble_method': 'learned',
                'ensemble_weights': ensemble_model.ensemble_weights.detach().cpu(),
                'training_time': total_epoch_time/60,
                'model_info': {
                    'v1_type': 'basic_transformer',
                    'v2_type': 'improved_transformer',
                    'v1_accuracy': 0.725,
                    'v2_accuracy': 0.769,
                    'ensemble_accuracy': accuracy,
                    'speed_optimized': True
                }
            }, best_model_path)
            print(f"Best model saved (Val Loss: {val_loss:.4f}, Acc: {accuracy:.4f})")
            patience_counter = 0
        else:
            patience_counter += 1
            print(f"Early stopping patience {patience_counter}/{patience}")
            if patience_counter >= patience:
                print(f"==== Early stopping Triggered ====")
                break

        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        torch.save({
            'ensemble_state_dict': ensemble_model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'epoch': epoch,
            'best_val_loss': best_val_loss
        }, checkpoint_path)
        
        scheduler.step()

    # ë¡œê·¸ ì €ì¥
    log_df = pd.DataFrame(log_history)
    log_df.to_csv(f"{ensemble_dir}/speed_ensemble_log.csv", index=False)
    print(f"\n Training log saved to {ensemble_dir}/speed_ensemble_log.csv")

    # ìµœì¢… ê²°ê³¼
    checkpoint = torch.load(best_model_path, map_location=device)
    final_accuracy = checkpoint['accuracy']
    final_recall = checkpoint['recall']
    final_f1 = checkpoint['f1_score']
    final_weights = checkpoint['ensemble_weights']
    
    print("\n" + "="*70)
    print(" **ì†ë„ ìµœì í™” ì•™ìƒë¸” ëª¨ë¸ ì™„ë£Œ!**")
    print("="*70)
    print(f"Version 1 (ê¸°ë³¸): 72.5% ì •í™•ë„")
    print(f"Version 2 (ê°œì„ ): 76.9% ì •í™•ë„")
    print(f"ì•™ìƒë¸” (ì „ì²´ ë°ì´í„°): {final_accuracy:.1%} ì •í™•ë„")
    print(f"ì„±ëŠ¥ í–¥ìƒ: +{(final_accuracy - 0.769) * 100:.1f}%p (vs Version 2)")
    print(f"ì¬í˜„ìœ¨: {final_recall:.1%}")
    print(f"F1-Score: {final_f1:.1%}")
    
    if final_weights is not None:
        weights = torch.softmax(final_weights, dim=0).numpy()
        print(f"í•™ìŠµëœ ìµœì¢… ê°€ì¤‘ì¹˜: V1={weights[0]:.3f}, V2={weights[1]:.3f}")
    
    avg_time_per_epoch = log_df['total_time_min'].mean()
    print(f"í‰ê·  ì—í¬í¬ ì‹œê°„: {avg_time_per_epoch:.1f}ë¶„")
    print(f"ëª¨ë¸ ì €ì¥: {best_model_path}")
    
    if final_accuracy > 0.785:
        print("ì†ë„ ìµœì í™” + ì„±ëŠ¥ í–¥ìƒ ì„±ê³µ!")
    elif final_accuracy > 0.77:
        print("ì†ë„ì™€ ì„±ëŠ¥ì˜ ê· í˜•ì¡íŒ ê°œì„ !")
    else:
        print("ì†ë„ëŠ” ê°œì„ ë˜ì—ˆìœ¼ë‚˜ ì„±ëŠ¥ í–¥ìƒì€ ì œí•œì ")

if __name__ == '__main__':
    main()
